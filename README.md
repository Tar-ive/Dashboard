# NSF Researcher Matching API

A comprehensive FastAPI-based system for matching researchers to NSF solicitations and assembling optimal research teams using advanced machine learning algorithms.

## ğŸš€ Features

- **PDF Upload & Analysis**: Upload and analyze NSF solicitation PDFs with intelligent text extraction
- **Researcher Matching**: Advanced hybrid search algorithm for matching researchers to solicitations
- **Dream Team Assembly**: Optimize research team composition using affinity matrices and selection strategies
- **Real-time Processing**: Asynchronous processing with status tracking
- **RESTful API**: Complete REST API with comprehensive documentation
- **Docker Support**: Containerized deployment with Docker Compose
- **Interactive Notebooks**: Jupyter notebooks for data analysis and exploration

## ğŸ“ Project Structure

```
Dashboard/
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ LICENSE                     # Project license
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ brahman.ipynb          # Research analysis notebook
â”‚   â”œâ”€â”€ dashboard_CADS.ipynb   # Dashboard prototype
â”‚   â””â”€â”€ krishna.ipynb          # Data exploration notebook
â””â”€â”€ backend/                   # FastAPI backend service
    â”œâ”€â”€ .env.example           # Environment variables template
    â”œâ”€â”€ .gitignore             # Git ignore rules
    â”œâ”€â”€ app/                   # Main application code
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ main.py           # FastAPI application entry point
    â”‚   â”œâ”€â”€ api/              # API route handlers
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ solicitations.py  # Solicitation endpoints
    â”‚   â”‚   â”œâ”€â”€ matching.py      # Matching endpoints
    â”‚   â”‚   â””â”€â”€ teams.py        # Dream team endpoints
    â”‚   â”œâ”€â”€ config.py          # Configuration management
    â”‚   â”œâ”€â”€ core/              # Core functionality
    â”‚   â”œâ”€â”€ dependencies.py    # Dependency injection
    â”‚ |   â”œâ”€â”€ models/          # Data models
    â”‚   â”œâ”€â”€ processors/        # Data processing modules
    â”‚   â”œâ”€â”€ services/          # Business logic services
    â”‚   â”œâ”€â”€ storage/           # Data storage handling
    â”‚   â””â”€â”€ utils/             # Utility functions
    â”œâ”€â”€ data/                  # Data storage
    â”‚   â”œâ”€â”€ uploads/           # Uploaded PDF files
    â”‚   â””â”€â”€ outputs/           # Generated outputs
    â”œâ”€â”€ docker-compose.yml     # Docker composition
    â”œâ”€â”€ Dockerfile             # Docker build instructions
    â”œâ”€â”€ requirements.txt       # Python dependencies
    â”œâ”€â”€ scripts/               # Utility scripts
    â””â”€â”€ tests/                 # Test files
```

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- Python 3.8+
- pip
- Docker & Docker Compose (optional)

### Local Development Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/Tar-ive/Dashboard.git
   cd Dashboard
   ```

2. **Set up the backend**
   ```bash
   cd backend
   pip install -r requirements.txt
   ```

3. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env file with your configuration
   ```

4. **Run the application**
   ```bash
   uvicorn app.main:app --reload
   ```

5. **Access the API**
   - API: http://localhost:8000
   - Documentation: http://localhost:8000/docs
   - Alternative docs: http://localhost:8000/redoc

### Docker Deployment

1. **Build and run with Docker Compose**
   ```bash
   cd backend
   docker-compose up -d
   ```

2. **Access the containerized API**
   - API: http://localhost:8000
   - Health check: http://localhost:8000/health

## ğŸ“š API Documentation

### Core Endpoints

#### Health Check
- `GET /` - Basic health check
- `GET /health` - Detailed health status

#### Solicitations API
- `GET /solicitations/` - List all solicitations
- `POST /solicitations/upload` - Upload PDF solicitation
- `POST /solicitations/{solicitation_id}/analyze` - Analyze solicitation
- `GET /solicitations/{solicitation_id}` - Get solicitation details

#### Matching API
- `GET /api/v1/matching/` - Get matching service info
- `POST /api/v1/matching/run` - Run matching algorithm
- `GET /api/v1/matching/{session_id}/status` - Get matching status
- `GET /api/v1/matching/{session_id}/results` - Get matching results
- `DELETE /api/v1/matching/{session_id}` - Delete matching session

#### Dream Teams API
- `GET /api/v1/teams/` - Get teams service info
- `POST /api/v1/teams/assemble` - Assemble dream team
- `GET /api/v1/teams/{team_id}` - Get team details
- `DELETE /api/v1/teams/{team_id}` - Delete team session
- `GET /api/v1/teams/{team_id}/matrix` - Export affinity matrix
- `POST /api/v1/teams/{team_id}/optimize` - Optimize team composition
- `POST /api/v1/teams/{team_id}/compare` - Compare selection strategies

### Request/Response Examples

#### Upload Solicitation
```bash
curl -X POST "http://localhost:8000/solicitations/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@solicitation.pdf"
```

#### Run Matching
```bash
curl -X POST "http://localhost:8000/api/v1/matching/run" \
  -H "Content-Type: application/json" \
  -d '{
    "solicitation_id": "123",
    "algorithm": "hybrid",
    "max_results": 10
  }'
```

#### Assemble Dream Team
```bash
curl -X POST "http://localhost:8000/api/v1/teams/assemble" \
  -H "Content-Type: application/json" \
  -d '{
    "solicitation_id": "123",
    "team_size": 5,
    "strategy": "optimal"
  }'
```

## ğŸ§ª Data Models

### Core Models
- **SolicitationResponse**: Uploaded solicitation metadata
- **SolicitationAnalysis**: Analyzed solicitation data
- **MatchingRequest**: Matching algorithm parameters
- **MatchingResults**: Researcher matching results
- **DreamTeamRequest**: Team assembly parameters
- **DreamTeamReport**: Assembled team details
- **ResearcherMatch**: Individual researcher match data
- **AffinityMatrixExport**: Team affinity matrix data

## ğŸ”§ Services Architecture

### Core Services
1. **PDF Processing Utilities**: Utility functions for PDF text extraction and analysis
2. **MatchingService**: Implements hybrid search algorithms
3. **DreamTeamService**: Optimizes team composition
4. **AnalysisService**: Performs solicitation analysis
5. **StorageService**: Manages data persistence

### Configuration System
The application uses a simplified configuration system that:
- Loads settings from environment variables using `os.getenv()`
- Supports `.env` files for local development
- Automatically creates required directories on startup
- Provides sensible defaults for all configuration options

### Processing Pipeline
1. **PDF Upload** â†’ Utility-based Text Extraction â†’ Content Analysis
2. **Solicitation Analysis** â†’ Title/Abstract Detection â†’ Section Mapping
3. **Researcher Matching** â†’ Similarity Scoring â†’ Ranking
4. **Team Assembly** â†’ Affinity Calculation â†’ Optimization

### PDF Processing Architecture
The system now uses a streamlined utility-based approach for PDF processing:

- **`extract_pdf_text()`**: Core utility function for PDF text extraction using PyMuPDF
- **Intelligent Text Analysis**: Automatic title and abstract detection
- **Section Recognition**: Identifies key NSF solicitation sections (Program Description, Award Information, Eligibility, etc.)
- **Performance Optimized**: Direct utility functions for faster processing without service layer overhead

## ğŸ“Š Notebooks

### Available Notebooks
1. **brahman.ipynb**: Research analysis and algorithm development
2. **dashboard_CADS.ipynb**: Dashboard prototype and visualization
3. **krishna.ipynb**: Data exploration and preprocessing

### Running Notebooks
```bash
cd notebooks
jupyter notebook
```

## ğŸ§ª Testing

The project uses a simplified, pragmatic testing approach that starts minimal and grows with complexity as needed.

### Current Test Infrastructure

The testing framework currently provides:
- **Basic Test Client**: FastAPI test client for API endpoint testing
- **Temporary File System**: Isolated temporary directories for file-based tests
- **Sample Test Data**: Basic PDF content fixtures for upload testing
- **Pytest Configuration**: Comprehensive test configuration with coverage reporting

### Run Tests
```bash
cd backend
python -m pytest tests/
```

### Test Coverage
```bash
python -m pytest --cov=app tests/
```

### Test Coverage Report
```bash
# Generate HTML coverage report
python -m pytest --cov=app --cov-report=html tests/

# View coverage report
open htmlcov/index.html
```

### Test Categories
The project supports multiple test categories through pytest markers:
- `unit`: Unit tests for individual components
- `integration`: Integration tests for component interactions  
- `e2e`: End-to-end tests for complete workflows
- `performance`: Performance and load tests
- `golden`: Ground-truth validation tests
- `slow`: Tests that take longer to run
- `external`: Tests that require external services

### Running Specific Test Categories
```bash
# Run only unit tests
python -m pytest -m unit

# Run integration tests
python -m pytest -m integration

# Skip slow tests
python -m pytest -m "not slow"
```

### Test Development Philosophy

The testing approach follows these principles:
1. **Start Simple**: Begin with basic fixtures and grow complexity as needed
2. **Pragmatic Coverage**: Focus on critical paths and business logic
3. **Fast Feedback**: Prioritize fast-running tests for development workflow
4. **Realistic Data**: Use meaningful test data that reflects real-world scenarios
5. **Isolated Tests**: Each test runs independently with proper cleanup

### Current Test Infrastructure Details

The simplified test configuration (`backend/tests/conftest.py`) provides:

```python
@pytest.fixture
def test_client():
    """Create a test client."""
    return TestClient(app)

@pytest.fixture
def temp_dir():
    """Create temporary directory for test files."""
    temp_path = tempfile.mkdtemp()
    yield temp_path
    shutil.rmtree(temp_path, ignore_errors=True)

@pytest.fixture
def sample_pdf_content():
    """Sample PDF content for testing."""
    return b"%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n/Pages 2 0 R\n>>\nendobj\n"
```

### Writing Tests

Example test structure:

```python
def test_api_endpoint(test_client):
    """Test API endpoint with basic client."""
    response = test_client.get("/health")
    assert response.status_code == 200

def test_file_processing(temp_dir, sample_pdf_content):
    """Test file processing with temporary directory."""
    pdf_path = Path(temp_dir) / "test.pdf"
    pdf_path.write_bytes(sample_pdf_content)
    
    # Test file processing logic
    assert pdf_path.exists()
```

### Future Testing Enhancements

As the project grows, the testing infrastructure will expand to include:
- Advanced fixture management for complex test scenarios
- Mock services for external dependencies
- Performance benchmarking and load testing
- Ground-truth validation against historical data
- Automated test data generation

### Testing Best Practices

1. **Keep Tests Simple**: Start with basic assertions and grow complexity as needed
2. **Use Descriptive Names**: Test names should clearly describe what is being tested
3. **Isolate Tests**: Each test should be independent and not rely on other tests
4. **Test Edge Cases**: Include tests for boundary conditions and error scenarios
5. **Maintain Fast Feedback**: Prioritize fast-running tests for development workflow

## ğŸ“ˆ Performance & Scalability

### Optimization Features
- **Asynchronous Processing**: Non-blocking API operations
- **Caching**: Intelligent result caching
- **Batch Processing**: Efficient bulk operations
- **Resource Management**: Optimized memory usage

### Monitoring
- Health check endpoints for monitoring
- Detailed status tracking for long-running operations
- Performance metrics collection

## ğŸš€ Deployment

### Production Deployment

1. **Environment Setup**
   ```bash
   export ENVIRONMENT=production
   export DATABASE_URL=your_production_db
   export SECRET_KEY=your_secret_key
   ```

2. **Docker Production Build**
   ```bash
   docker build -t nsf-matcher:latest .
   docker run -p 8000:8000 nsf-matcher:latest
   ```

3. **Kubernetes Deployment** (Optional)
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nsf-matcher
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nsf-matcher
  template:
    metadata:
      labels:
        app: nsf-matcher
    spec:
      containers:
      - name: nsf-matcher
        image: nsf-matcher:latest
        ports:
        - containerPort: 8000
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guidelines
- Write comprehensive tests
- Update documentation
- Use semantic commit messages

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support, please:
1. Check the API documentation
2. Review existing issues
3. Create a new issue with detailed description

## ğŸ™ Acknowledgments

- NSF for solicitation data standards
- FastAPI team for the excellent framework
- Contributors and researchers who made this possible

## ğŸ“Š API Status

- **Version**: 1.0.0
- **Status**: Active Development
- **Last Updated**: July 2025
- **Endpoints**: 17 active endpoints
- **Features**: PDF Processing, Matching, Team Assembly

**Built with â¤ï¸ for the research community**