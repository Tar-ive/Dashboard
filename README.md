# Intelligent Grant Matching Engine

An advanced FastAPI-based system designed to deconstruct NSF solicitation PDFs into structured data and assemble optimal research teams. The architecture leverages **Redis and RQ (Redis Queue)** for robust, scalable background job processing.

This project is built with three core principles: **Simplicity First**, **Test-Driven Development (TDD)**, and **Incremental Builds**.

---

## ğŸš€ Project Status & Roadmap

### Current Status: **Active Development**
- **Version**: 1.0.0-beta
- **Architecture**: Fully designed and documented
- **Implementation**: Foundation components in progress
- **Target MVP**: Q4 2025

### ğŸ¯ MVP Milestone (Coming Soon)

Our Minimum Viable Product will deliver the complete **Solicitation Deconstruction** workflow:

**MVP Features:**
- âœ… **PDF Upload API**: FastAPI endpoint for NSF solicitation uploads
- âœ… **Asynchronous Processing**: Redis + RQ background job processing
- ğŸ”„ **Smart PDF Extraction**: PyMuPDF-based text extraction with section recognition
- ğŸ”„ **LLM-Powered Analysis**: Groq/Anthropic integration for metadata extraction
- ğŸ”„ **Structured Output**: Complete `StructuredSolicitation` JSON objects
- âœ… **Job Status Tracking**: Real-time progress monitoring
- ğŸ”„ **Comprehensive Testing**: 95%+ test coverage with TDD approach

**What You'll Be Able to Do:**
1. Upload any NSF solicitation PDF via `/api/deconstruct`
2. Receive immediate `job_id` for tracking
3. Monitor processing status via `/api/jobs/{job_id}`
4. Receive structured JSON with extracted metadata, eligibility rules, and required skills

### ğŸ—“ï¸ Development Timeline

**Phase 1: Foundation** *(Current - August 2025)*
- âœ… FastAPI + RQ + Redis infrastructure
- âœ… Core data models and job management
- ğŸ”„ Basic endpoint implementations
- ğŸ”„ Foundation testing framework

**Phase 2: MVP - Solicitation Deconstruction** *(September 2025)*
- ğŸ”„ TDD implementation of PDF processing
- ğŸ”„ LLM integration for metadata extraction
- ğŸ”„ Section-based text chunking
- ğŸ”„ Comprehensive error handling

**Phase 3: Dream Team Assembly** *(Q4 2025)*
- Dream team optimization algorithms
- Researcher affinity scoring
- Gap analysis and strategic recommendations
- End-to-end workflow completion

---

## ğŸ›ï¸ Architecture

The system uses a decoupled architecture where the FastAPI web server acts as a lightweight interface for enqueuing jobs and querying their status. All heavy processing is handled asynchronously by RQ workers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚    â”‚     Redis       â”‚    â”‚   RQ Worker     â”‚
â”‚  (Web Server)   â”‚    â”‚  (Broker &      â”‚    â”‚  (Background    â”‚
â”‚                 â”‚    â”‚ State Manager)  â”‚    â”‚   Processor)    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚â€¢ POST /deconstruct  â”‚â—„â”€â”€â–ºâ”‚ â€¢ Job queue     â”‚â—„â”€â”€â–ºâ”‚â€¢ deconstruct_   â”‚
â”‚â€¢ POST /assemble-teamâ”‚    â”‚ â€¢ Job status    â”‚    â”‚  solicitation_  â”‚
â”‚â€¢ GET /jobs/{id}     â”‚    â”‚ â€¢ Results store â”‚    â”‚  task           â”‚
â”‚â€¢ Returns job_id     â”‚    â”‚                 â”‚    â”‚â€¢ assemble_      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  dream_team_    â”‚
                                              â”‚  task           â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Core Features

### ğŸ¯ Milestone 1: Solicitation Deconstruction *(MVP)*
- **PDF Intelligence**: Advanced PDF parsing with section recognition
- **LLM-Powered Extraction**: Structured metadata extraction using Groq/Anthropic
- **Comprehensive Analysis**: Extract funding details, eligibility rules, required skills
- **Async Processing**: Non-blocking background job processing
- **Real-time Tracking**: Monitor job progress and status

### ğŸš€ Milestone 2: Dream Team Assembly *(Future)*
- **Smart Team Building**: Optimal research team composition algorithms
- **Affinity Scoring**: Advanced researcher-skill matching
- **Gap Analysis**: Strategic recommendations using LLM analysis
- **Constraint Optimization**: Team size, role, and eligibility constraint handling

### ğŸ”§ Platform Features
- **Dockerized Deployment**: Complete containerization with docker-compose
- **RESTful API**: Comprehensive REST API with interactive documentation
- **TDD Development**: 95%+ test coverage with comprehensive test suite
- **Real-time Monitoring**: Health checks and job status endpoints

---

## ğŸ“ Project Structure

The backend has been refactored for simplicity and maintainability, eliminating unnecessary directories and circular dependencies.

```
backend/
â”œâ”€â”€ app/                      # Main application source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api/                  # FastAPI routers (endpoints)
â”‚   â”‚   â”œâ”€â”€ deconstruct.py    # ğŸ”„ PDF deconstruction endpoints
â”‚   â”‚   â”œâ”€â”€ jobs.py           # âœ… Job status tracking
â”‚   â”‚   â””â”€â”€ teams.py          # ğŸ”œ Dream team assembly (future)
â”‚   â”œâ”€â”€ jobs/                 # Background task management
â”‚   â”‚   â”œâ”€â”€ job_manager.py    # âœ… Job queue management
â”‚   â”‚   â”œâ”€â”€ redis_connection.py # âœ… Redis client setup
â”‚   â”‚   â””â”€â”€ worker_config.py  # âœ… RQ worker configuration
â”‚   â”œâ”€â”€ models/               # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ solicitation.py   # ğŸ”„ StructuredSolicitation model
â”‚   â”‚   â”œâ”€â”€ jobs.py          # âœ… Job tracking models
â”‚   â”‚   â””â”€â”€ teams.py         # ğŸ”œ Dream team models (future)
â”‚   â”œâ”€â”€ services/             # Core business logic
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py  # ğŸ”„ PDF extraction service
â”‚   â”‚   â”œâ”€â”€ llm_service.py    # ğŸ”„ LLM integration service
â”‚   â”‚   â””â”€â”€ team_builder.py   # ğŸ”œ Team assembly service (future)
â”‚   â”œâ”€â”€ config.py             # âœ… Simplified configuration
â”‚   â”œâ”€â”€ main.py               # âœ… FastAPI application entry point
â”‚   â””â”€â”€ utils.py              # ğŸ”„ Utility functions
â”œâ”€â”€ data/                     # Data storage
â”‚   â”œâ”€â”€ uploads/              # PDF file uploads
â”‚   â””â”€â”€ models/               # ğŸ”œ Researcher data (future)
â”œâ”€â”€ tests/                    # Comprehensive test suite
â”‚   â”œâ”€â”€ unit/                 # Unit tests
â”‚   â”œâ”€â”€ integration/          # Integration tests
â”‚   â””â”€â”€ e2e/                  # End-to-end tests
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ Dockerfile                # Docker build instructions
â”œâ”€â”€ docker-compose.yml        # Local development services
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ worker.py                 # ğŸ”„ RQ worker script
```

**Legend:** âœ… Complete | ğŸ”„ In Progress | ğŸ”œ Planned

---

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- Redis (can be run via Docker)

### Quick Start

1. **Clone the Repository**
   ```bash
   git clone https://github.com/Tar-ive/Dashboard.git
   cd Dashboard/backend
   ```

2. **Environment Setup**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

   Required environment variables:
   ```ini
   # LLM API Keys (choose one or both)
   ANTHROPIC_API_KEY=your_anthropic_key_here
   GROQ_API_KEY=your_groq_key_here
   
   # Redis Configuration
   REDIS_HOST=localhost
   REDIS_PORT=6379
   
   # Application Settings
   DEBUG=true
   ```

3. **Start Services**
   ```bash
   # Start FastAPI + Redis with Docker Compose
   docker-compose up -d --build
   
   # In a separate terminal, start the worker
   python worker.py
   ```

4. **Access the Application**
   - **API**: http://localhost:8000
   - **Interactive Docs**: http://localhost:8000/docs
   - **Health Check**: http://localhost:8000/health

---

## ğŸš€ Usage Example

### Step 1: Upload NSF Solicitation

```bash
curl -X POST "http://localhost:8000/api/deconstruct" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_nsf_solicitation.pdf"
```

**Response:**
```json
{
  "job_id": "a1b2c3d4-e5f6-7890-a1b2-c3d4e5f67890",
  "status": "queued",
  "message": "PDF uploaded successfully. Processing started."
}
```

### Step 2: Monitor Processing

```bash
curl -X GET "http://localhost:8000/api/jobs/a1b2c3d4-e5f6-7890-a1b2-c3d4e5f67890"
```

**Response (when completed):**
```json
{
  "job_id": "a1b2c3d4-e5f6-7890-a1b2-c3d4e5f67890",
  "status": "completed",
  "progress": 100,
  "result": {
    "solicitation_id": "NSF-25-12345",
    "award_title": "Mathematical Foundations of Artificial Intelligence",
    "funding_ceiling": 500000.0,
    "project_duration_months": 36,
    "required_scientific_skills": [
      "machine learning",
      "mathematical optimization",
      "statistical analysis"
    ],
    "pi_eligibility_rules": [
      "Must hold PhD in relevant field",
      "Must be affiliated with eligible institution"
    ],
    "processing_time_seconds": 127.5
  },
  "created_at": "2025-07-18T10:00:00Z",
  "completed_at": "2025-07-18T10:02:07Z"
}
```

---

## ğŸ§ª Testing

The project follows a comprehensive TDD approach with multiple test categories.

### Running Tests

```bash
# Run all tests
make test

# Run with coverage report
make test-coverage

# Run specific test categories
pytest -m unit          # Unit tests only
pytest -m integration   # Integration tests only
pytest -m e2e           # End-to-end tests only
```

### Test Categories

- **Unit Tests**: Test individual functions in isolation
- **Integration Tests**: Test component interactions
- **E2E Tests**: Test complete API-to-worker workflows
- **Performance Tests**: Validate response times and throughput

---

## ğŸ“Š Data Models

### Milestone 1: Solicitation Models

```python
class StructuredSolicitation(BaseModel):
    solicitation_id: str
    award_title: str
    funding_ceiling: Optional[float]
    project_duration_months: Optional[int]
    submission_deadline: Optional[datetime]
    pi_eligibility_rules: List[str]
    required_scientific_skills: List[str]
    full_text: str
    processing_time_seconds: float
    created_at: datetime
```

### Future: Dream Team Models

```python
class DreamTeamReport(BaseModel):
    solicitation_id: str
    team_members: List[ProposedTeamMember]
    team_coverage_score: float
    gap_analysis: Dict[str, Any]
    strategic_recommendations: List[str]
```

---

## ğŸ”§ Development

### TDD Workflow

**Red-Green-Refactor Cycle:**
1. **Red**: Write failing tests first
2. **Green**: Implement minimal code to pass tests
3. **Refactor**: Clean and optimize implementation

### Code Quality Standards

- **Test Coverage**: Maintain 95%+ coverage
- **Type Hints**: Full type annotation coverage
- **Documentation**: Comprehensive docstrings
- **Error Handling**: Robust error handling and logging

---

## ğŸš€ What's Next

### Immediate Next Steps (July 2025)

1. **Complete PDF Processing Pipeline**
   - Finalize PyMuPDF integration
   - Implement section recognition algorithms
   - Add robust error handling for malformed PDFs

2. **LLM Integration Completion**
   - Complete Groq/Anthropic API integration
   - Implement retry logic and rate limiting
   - Add response validation and error recovery

3. **MVP Testing & Validation**
   - Complete end-to-end test suite
   - Performance testing and optimization
   - User acceptance testing with sample solicitations

### Future Enhancements (August 2025)

1. **Dream Team Assembly**
   - Researcher database integration
   - Affinity scoring algorithms
   - Team optimization strategies

2. **Advanced Features**
   - Multi-solicitation comparison
   - Historical success analysis
   - Real-time collaboration features

3. **Production Readiness**
   - Kubernetes deployment configurations
   - Monitoring and alerting systems
   - API rate limiting and authentication

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Follow TDD principles (write tests first!)
4. Ensure all tests pass (`make test`)
5. Submit a pull request

---

## ğŸ“ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## ğŸ“ Support

- **Documentation**: http://localhost:8000/docs (when running locally)
- **Issues**: [GitHub Issues](https://github.com/Tar-ive/Dashboard/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Tar-ive/Dashboard/discussions)

---

**Built with â¤ï¸ for the research community**

*Transforming how researchers discover and collaborate on NSF opportunities*