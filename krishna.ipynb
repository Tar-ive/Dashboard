{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tar-ive/Dashboard/blob/main/krishna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection + Pre Processing\n"
      ],
      "metadata": {
        "id": "BuH_XxbHivQ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWRwcetM9uTY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d0rEgHk95uo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/texas_state_nsf_grants.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6jLHAqP92Sz"
      },
      "outputs": [],
      "source": [
        "# prompt: make a single list of all the names from the  principal_investigator\tco_principal_investigators columns, the co_principle_investigators column have names separated by ; delimitor.\n",
        "\n",
        "import pandas as pd\n",
        " # Replace 'your_file.csv' with the actual file name\n",
        "\n",
        "all_names = []\n",
        "for index, row in df.iterrows():\n",
        "  all_names.append(row['principal_investigator'])\n",
        "  co_pis = row['co_principal_investigators']\n",
        "  if pd.notnull(co_pis):\n",
        "    all_names.extend([name.strip() for name in co_pis.split(';')])\n",
        "\n",
        "all_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS-hQeCe9_61"
      },
      "outputs": [],
      "source": [
        "# prompt: make a  df  of  only these 2 columns.\n",
        "\n",
        "relevant_df = df[['principal_investigator', 'co_principal_investigators']]\n",
        "relevant_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSVHNP0R-ALL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from urllib.parse import quote\n",
        "\n",
        "# List of researchers\n",
        "researchers = [\n",
        "    \"Alice Olmstead\", \"Apan Qasem\", \"Araceli Martinez Ortiz\", \"Barbara Hewitt\",\n",
        "    \"Byoung-Hee You\", \"Carolyn Chang\", \"Chiu Au\", \"Christopher Rhodes\",\n",
        "    \"Chul-Ho Lee\", \"Cindy Royal\", \"Clara Novoa\", \"Craig Hanks\",\n",
        "    \"Cynthia Luxford\", \"Damian Valles Molina\", \"Danny Wescott\", \"David Gibbs\",\n",
        "    \"Denise Gobert\", \"Diane Dolozel\", \"Dincer Konur\", \"Dominick Fazarro\",\n",
        "    \"Eduardo Perez\", \"Edward Yu\", \"Edwin Chow\", \"Edwin Piner\",\n",
        "    \"Eleanor Close\", \"Emanuel Alanis\", \"Emily Zhu\", \"Erica Nason\",\n",
        "    \"Eunsang Cho\", \"Feng Wang\", \"Francis Mendez\", \"Gabriel Lopez\",\n",
        "    \"Gregory Lakomski\", \"Heather Galloway\", \"Holly Lewis\", \"Holly Veselka\",\n",
        "    \"Hsing-Huang Tseng\", \"Hyunhwan Kim\", \"In-Hyouk Song\", \"Ivan Ojeda-Ruiz\",\n",
        "    \"Jana Minifie\", \"Jelena Tesic\", \"Jennifer Irvin\", \"Jie Zhu\",\n",
        "    \"Jitendra Tate\", \"Karen Lewis\", \"Keshav Bhandari\", \"Kimberly Talley\",\n",
        "    \"Larry Price\", \"Li Feng\", \"Lucia Summers\", \"Maria Resendiz\",\n",
        "    \"Mario Cristian Gaedicke-Hornung\", \"Mike Ferrara\", \"Mina Guirguis\",\n",
        "    \"Monica Hughes\", \"Mylene Farias\", \"Nadim Adi\", \"Nicole Taylor\",\n",
        "    \"Nikoleta Theodoropoulou\", \"Rasim Musal\", \"Ravindranath Droopad\",\n",
        "    \"Sarah Fritts\", \"Satyajit Dutta\", \"Sean Bauld\", \"Semih Aslan\",\n",
        "    \"Shannon Williams\", \"Stefan Zauscher\", \"Steven Whitten\", \"Subasish Das\",\n",
        "    \"Tahir Ekin\", \"Tania Betancourt\", \"Thomas Keller\", \"Togay Ozbakkaloglu\",\n",
        "    \"Tongdan Jin\", \"Toni Watt\", \"Ty Schepis\", \"Vangelis Metsis\",\n",
        "    \"Vishan Shen\", \"Wenquan Dong\", \"Wilhelmus Geerts\", \"William Brittain\",\n",
        "    \"Xiangping Liu\", \"Xiaoxi Shen\", \"Yaroslava Yingling\", \"Yihong 'Maggie' Chen\",\n",
        "    \"Yihong Yuan\", \"Yong Yang\", \"Young Ju Lee\", \"Ziliang Zong\"\n",
        "]\n",
        "\n",
        "# Function to search OpenAlex for a researcher\n",
        "def search_openalex(name):\n",
        "    \"\"\"Search OpenAlex API for researcher and return results\"\"\"\n",
        "    try:\n",
        "        # URL encode the name for the API call\n",
        "        encoded_name = quote(name)\n",
        "        url = f\"https://api.openalex.org/authors?search={encoded_name}\"\n",
        "\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        return data.get('results', [])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching for {name}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to check if researcher is affiliated with Texas State\n",
        "def check_texas_state_affiliation(author_data):\n",
        "    \"\"\"Check if author has Texas State affiliation\"\"\"\n",
        "    affiliations = author_data.get('affiliations', [])\n",
        "\n",
        "    for affiliation in affiliations:\n",
        "        institution = affiliation.get('institution', {})\n",
        "        display_name = institution.get('display_name', '').lower()\n",
        "\n",
        "        # Check for Texas State University variations\n",
        "        if any(term in display_name for term in ['texas state', 'texas state university']):\n",
        "            return True, institution.get('display_name', 'Texas State University')\n",
        "\n",
        "    return False, None\n",
        "\n",
        "# Main processing\n",
        "results = []\n",
        "\n",
        "print(\"Starting OpenAlex lookup for 90 researchers...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, name in enumerate(researchers, 1):\n",
        "    print(f\"Processing {i}/90: {name}\")\n",
        "\n",
        "    # Search OpenAlex\n",
        "    search_results = search_openalex(name)\n",
        "\n",
        "    if not search_results:\n",
        "        results.append({\n",
        "            'name': name,\n",
        "            'openalex_id': 'NOT_FOUND',\n",
        "            'texas_state_affiliated': False,\n",
        "            'institution_name': '',\n",
        "            'display_name': '',\n",
        "            'works_count': 0,\n",
        "            'cited_by_count': 0\n",
        "        })\n",
        "        print(f\"  → No results found\")\n",
        "        continue\n",
        "\n",
        "    # Check each result for Texas State affiliation\n",
        "    found_texas_state = False\n",
        "\n",
        "    for author in search_results:\n",
        "        is_texas_state, institution_name = check_texas_state_affiliation(author)\n",
        "\n",
        "        if is_texas_state:\n",
        "            results.append({\n",
        "                'name': name,\n",
        "                'openalex_id': author.get('id', ''),\n",
        "                'texas_state_affiliated': True,\n",
        "                'institution_name': institution_name,\n",
        "                'display_name': author.get('display_name', ''),\n",
        "                'works_count': author.get('works_count', 0),\n",
        "                'cited_by_count': author.get('cited_by_count', 0)\n",
        "            })\n",
        "            print(f\"  ✓ Found at Texas State: {author.get('display_name', name)}\")\n",
        "            found_texas_state = True\n",
        "            break\n",
        "\n",
        "    if not found_texas_state:\n",
        "        # Take the first result even if not at Texas State\n",
        "        first_result = search_results[0]\n",
        "        results.append({\n",
        "            'name': name,\n",
        "            'openalex_id': first_result.get('id', ''),\n",
        "            'texas_state_affiliated': False,\n",
        "            'institution_name': '',\n",
        "            'display_name': first_result.get('display_name', ''),\n",
        "            'works_count': first_result.get('works_count', 0),\n",
        "            'cited_by_count': first_result.get('cited_by_count', 0)\n",
        "        })\n",
        "        print(f\"  ⚠ Found but not at Texas State: {first_result.get('display_name', name)}\")\n",
        "\n",
        "    # Small delay to be respectful to the API\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Summary statistics\n",
        "total_found = len(df[df['openalex_id'] != 'NOT_FOUND'])\n",
        "texas_state_count = len(df[df['texas_state_affiliated'] == True])\n",
        "not_found_count = len(df[df['openalex_id'] == 'NOT_FOUND'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"SUMMARY:\")\n",
        "print(f\"Total researchers: {len(researchers)}\")\n",
        "print(f\"Found in OpenAlex: {total_found}\")\n",
        "print(f\"Confirmed at Texas State: {texas_state_count}\")\n",
        "print(f\"Not found: {not_found_count}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = \"texas_state_researchers_openalex.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"\\nResults saved to: {csv_filename}\")\n",
        "\n",
        "# Display first few results\n",
        "print(f\"\\nFirst 5 results:\")\n",
        "print(df.head().to_string(index=False))\n",
        "\n",
        "# Show researchers not found\n",
        "if not_found_count > 0:\n",
        "    print(f\"\\nResearchers not found in OpenAlex:\")\n",
        "    not_found = df[df['openalex_id'] == 'NOT_FOUND']['name'].tolist()\n",
        "    for name in not_found:\n",
        "        print(f\"  - {name}\")\n",
        "\n",
        "# Show researchers found but not at Texas State\n",
        "non_texas_state = df[(df['texas_state_affiliated'] == False) & (df['openalex_id'] != 'NOT_FOUND')]\n",
        "if len(non_texas_state) > 0:\n",
        "    print(f\"\\nResearchers found but not affiliated with Texas State:\")\n",
        "    for _, row in non_texas_state.iterrows():\n",
        "        print(f\"  - {row['name']}: {row['display_name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnmyNbT4CFwH"
      },
      "source": [
        "hardcoding these now for testing purposes but any relevent data scientist could patch this up with real data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VghWpsv4DYhL"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGhncMlQEYDs"
      },
      "outputs": [],
      "source": [
        "# prompt: drop columns _meta', '_captcha', '_login', '_error',\n",
        "\n",
        "# Assuming df is already created and populated from the previous code\n",
        "# List of columns to drop\n",
        "columns_to_drop = ['_meta', '_captcha', '_login', '_error']\n",
        "\n",
        "# Drop the specified columns if they exist in the DataFrame\n",
        "# Using errors='ignore' will prevent an error if a column is not found\n",
        "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Display the columns to verify\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-pjws5LEgfC"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cDvnvLXDZXC"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('/content/texas_state_researchers_openalex.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biyvOcDTDovx"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0JVK4feIUSN"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/texas_state_nsf_grants.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2IA2Rv-Dpz4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "\n",
        "# Researchers to remove (not found in OpenAlex or not at Texas State)\n",
        "researchers_to_remove = [\n",
        "    # Not found in OpenAlex\n",
        "    \"Danny Wescott\", \"Diane Dolozel\", \"Emanuel Alanis\", \"Holly Veselka\",\n",
        "    \"Mario Cristian Gaedicke-Hornung\", \"Sean Bauld\", \"Vishan Shen\",\n",
        "    # Found but not affiliated with Texas State\n",
        "    \"Chiu Au\", \"Edward Yu\", \"Feng Wang\", \"Gabriel Lopez\", \"Holly Lewis\",\n",
        "    \"Jie Zhu\", \"Li Feng\", \"Mike Ferrara\", \"Nadim Adi\", \"Stefan Zauscher\",\n",
        "    \"Yaroslava Yingling\"\n",
        "]\n",
        "\n",
        "# Original list of researchers (90 total)\n",
        "original_researchers = [\n",
        "    \"Alice Olmstead\", \"Apan Qasem\", \"Araceli Martinez Ortiz\", \"Barbara Hewitt\",\n",
        "    \"Byoung-Hee You\", \"Carolyn Chang\", \"Chiu Au\", \"Christopher Rhodes\",\n",
        "    \"Chul-Ho Lee\", \"Cindy Royal\", \"Clara Novoa\", \"Craig Hanks\",\n",
        "    \"Cynthia Luxford\", \"Damian Valles Molina\", \"Danny Wescott\", \"David Gibbs\",\n",
        "    \"Denise Gobert\", \"Diane Dolozel\", \"Dincer Konur\", \"Dominick Fazarro\",\n",
        "    \"Eduardo Perez\", \"Edward Yu\", \"Edwin Chow\", \"Edwin Piner\",\n",
        "    \"Eleanor Close\", \"Emanuel Alanis\", \"Emily Zhu\", \"Erica Nason\",\n",
        "    \"Eunsang Cho\", \"Feng Wang\", \"Francis Mendez\", \"Gabriel Lopez\",\n",
        "    \"Gregory Lakomski\", \"Heather Galloway\", \"Holly Lewis\", \"Holly Veselka\",\n",
        "    \"Hsing-Huang Tseng\", \"Hyunhwan Kim\", \"In-Hyouk Song\", \"Ivan Ojeda-Ruiz\",\n",
        "    \"Jana Minifie\", \"Jelena Tesic\", \"Jennifer Irvin\", \"Jie Zhu\",\n",
        "    \"Jitendra Tate\", \"Karen Lewis\", \"Keshav Bhandari\", \"Kimberly Talley\",\n",
        "    \"Larry Price\", \"Li Feng\", \"Lucia Summers\", \"Maria Resendiz\",\n",
        "    \"Mario Cristian Gaedicke-Hornung\", \"Mike Ferrara\", \"Mina Guirguis\",\n",
        "    \"Monica Hughes\", \"Mylene Farias\", \"Nadim Adi\", \"Nicole Taylor\",\n",
        "    \"Nikoleta Theodoropoulou\", \"Rasim Musal\", \"Ravindranath Droopad\",\n",
        "    \"Sarah Fritts\", \"Satyajit Dutta\", \"Sean Bauld\", \"Semih Aslan\",\n",
        "    \"Shannon Williams\", \"Stefan Zauscher\", \"Steven Whitten\", \"Subasish Das\",\n",
        "    \"Tahir Ekin\", \"Tania Betancourt\", \"Thomas Keller\", \"Togay Ozbakkaloglu\",\n",
        "    \"Tongdan Jin\", \"Toni Watt\", \"Ty Schepis\", \"Vangelis Metsis\",\n",
        "    \"Vishan Shen\", \"Wenquan Dong\", \"Wilhelmus Geerts\", \"William Brittain\",\n",
        "    \"Xiangping Liu\", \"Xiaoxi Shen\", \"Yaroslava Yingling\", \"Yihong 'Maggie' Chen\",\n",
        "    \"Yihong Yuan\", \"Yong Yang\", \"Young Ju Lee\", \"Ziliang Zong\"\n",
        "]\n",
        "\n",
        "# Filter out researchers to remove\n",
        "valid_researchers = [name for name in original_researchers if name not in researchers_to_remove]\n",
        "\n",
        "print(f\"Original researchers: {len(original_researchers)}\")\n",
        "print(f\"Researchers removed: {len(researchers_to_remove)}\")\n",
        "print(f\"Valid researchers remaining: {len(valid_researchers)}\")\n",
        "\n",
        "# Function to parse dates in format \"June 1, 2012\"\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse date string in format 'Month Day, Year'\"\"\"\n",
        "    if pd.isna(date_str) or date_str == '':\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.strptime(date_str.strip(), \"%B %d, %Y\")\n",
        "    except:\n",
        "        try:\n",
        "            # Try alternative format without comma\n",
        "            return datetime.strptime(date_str.strip(), \"%B %d %Y\")\n",
        "        except:\n",
        "            print(f\"Could not parse date: {date_str}\")\n",
        "            return None\n",
        "\n",
        "# Function to normalize names for matching\n",
        "def normalize_name(name):\n",
        "    \"\"\"Normalize name for matching\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return \"\"\n",
        "    # Remove extra whitespace, convert to lowercase\n",
        "    name = re.sub(r'\\s+', ' ', str(name).strip().lower())\n",
        "    # Remove common prefixes/suffixes\n",
        "    name = re.sub(r'\\b(dr\\.?|prof\\.?|professor)\\b', '', name).strip()\n",
        "    return name\n",
        "\n",
        "# Function to check if names match (flexible matching)\n",
        "def names_match(name1, name2):\n",
        "    \"\"\"Check if two names likely refer to the same person\"\"\"\n",
        "    norm1 = normalize_name(name1)\n",
        "    norm2 = normalize_name(name2)\n",
        "\n",
        "    if norm1 == norm2:\n",
        "        return True\n",
        "\n",
        "    # Split into parts\n",
        "    parts1 = norm1.split()\n",
        "    parts2 = norm2.split()\n",
        "\n",
        "    if len(parts1) < 2 or len(parts2) < 2:\n",
        "        return False\n",
        "\n",
        "    # Check if first and last names match\n",
        "    first1, last1 = parts1[0], parts1[-1]\n",
        "    first2, last2 = parts2[0], parts2[-1]\n",
        "\n",
        "    return first1 == first2 and last1 == last2\n",
        "\n",
        "# Load your dataframes (replace with your actual loading code)\n",
        "# df = pd.read_csv('grants_data.csv')  # Your grants dataframe\n",
        "# df1 = pd.read_csv('texas_state_researchers_openalex.csv')  # Your OpenAlex dataframe\n",
        "\n",
        "# For demonstration, I'll create the structure you need:\n",
        "def analyze_grants_data(df, df1, valid_researchers):\n",
        "    \"\"\"\n",
        "    Analyze grants data to count PI and Co-PI roles in last 7 years\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with grants data\n",
        "    df1: DataFrame with researcher OpenAlex data\n",
        "    valid_researchers: List of valid researcher names\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter df1 to only include valid researchers\n",
        "    df1_filtered = df1[df1['name'].isin(valid_researchers)].copy()\n",
        "\n",
        "    # Parse dates in grants dataframe\n",
        "    df['start_date_parsed'] = df['start_date'].apply(parse_date)\n",
        "    df['expiration_date_parsed'] = df['expiration_date'].apply(parse_date)\n",
        "\n",
        "    # Calculate cutoff date (7 years ago from today)\n",
        "    cutoff_date = datetime.now() - timedelta(days=7*365)\n",
        "\n",
        "    # Filter grants to last 7 years (using start date)\n",
        "    recent_grants = df[\n",
        "        (df['start_date_parsed'].notna()) &\n",
        "        (df['start_date_parsed'] >= cutoff_date)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"Total grants in dataset: {len(df)}\")\n",
        "    print(f\"Grants from last 7 years: {len(recent_grants)}\")\n",
        "\n",
        "    # Initialize results dictionary\n",
        "    results = []\n",
        "\n",
        "    for _, researcher in df1_filtered.iterrows():\n",
        "        name = researcher['name']\n",
        "        openalex_id = researcher['openalex_id']\n",
        "        works_count = researcher['works_count']\n",
        "        cited_by_count = researcher['cited_by_count']\n",
        "\n",
        "        pi_count = 0\n",
        "        copi_count = 0\n",
        "        affiliated_awards = []\n",
        "\n",
        "        # Count PI roles\n",
        "        pi_matches = recent_grants[\n",
        "            recent_grants['principal_investigator'].apply(\n",
        "                lambda x: names_match(x, name) if pd.notna(x) else False\n",
        "            )\n",
        "        ]\n",
        "        pi_count = len(pi_matches)\n",
        "        affiliated_awards.extend(pi_matches['award_id'].tolist())\n",
        "\n",
        "        # Count Co-PI roles\n",
        "        for _, grant in recent_grants.iterrows():\n",
        "            co_pis = grant['co_principal_investigators']\n",
        "            if pd.notna(co_pis):\n",
        "                # Split by semicolon and check each co-PI\n",
        "                co_pi_list = [copi.strip() for copi in str(co_pis).split(';')]\n",
        "                for co_pi in co_pi_list:\n",
        "                    if names_match(co_pi, name):\n",
        "                        copi_count += 1\n",
        "                        affiliated_awards.append(grant['award_id'])\n",
        "                        break  # Only count once per grant\n",
        "\n",
        "        # Remove duplicate award IDs\n",
        "        affiliated_awards = list(set([aid for aid in affiliated_awards if pd.notna(aid)]))\n",
        "\n",
        "        results.append({\n",
        "            'name': name,\n",
        "            'openalex_id': openalex_id,\n",
        "            'citations': cited_by_count,\n",
        "            'works_count': works_count,\n",
        "            'pi_7_years': pi_count,\n",
        "            'copi_7_years': copi_count,\n",
        "            'affiliated_award_ids': '; '.join(map(str, affiliated_awards)) if affiliated_awards else ''\n",
        "        })\n",
        "\n",
        "        if pi_count > 0 or copi_count > 0:\n",
        "            print(f\"{name}: PI={pi_count}, Co-PI={copi_count}, Awards={len(affiliated_awards)}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Create the analysis function call\n",
        "# Replace 'df' and 'df1' with your actual dataframe variables\n",
        "# final_df = analyze_grants_data(df, df1, valid_researchers)\n",
        "\n",
        "# Save results\n",
        "# final_df.to_csv('texas_state_researchers_final_analysis.csv', index=False)\n",
        "\n",
        "# Display summary\n",
        "# print(f\"\\nFinal dataset shape: {final_df.shape}\")\n",
        "# print(f\"\\nResearchers with PI roles in last 7 years: {len(final_df[final_df['pi_7_years'] > 0])}\")\n",
        "# print(f\"Researchers with Co-PI roles in last 7 years: {len(final_df[final_df['copi_7_years'] > 0])}\")\n",
        "# print(f\"Researchers with any grant role in last 7 years: {len(final_df[(final_df['pi_7_years'] > 0) | (final_df['copi_7_years'] > 0)])}\")\n",
        "\n",
        "print(\"Code structure ready. Replace df and df1 with your actual dataframes and uncomment the function calls.\")\n",
        "\n",
        "# Example of how to use (uncomment and modify):\n",
        "\n",
        "\n",
        "# Run analysis\n",
        "final_df = analyze_grants_data(df, df1, valid_researchers)\n",
        "\n",
        "# Save results\n",
        "final_df.to_csv('texas_state_researchers_final_analysis.csv', index=False)\n",
        "\n",
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjbcM24wF2Ys"
      },
      "outputs": [],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pffm1Zd2F9QE"
      },
      "outputs": [],
      "source": [
        "final_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6LpGglvPiE7"
      },
      "outputs": [],
      "source": [
        "final_df = pd.read_csv('/content/texas_state_researchers_final_analysis.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H30hse9IeDc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_author_works(openalex_id: str, per_page: int = 200) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch all works for a given author from OpenAlex API\n",
        "    \"\"\"\n",
        "    all_works = []\n",
        "    page = 1\n",
        "\n",
        "    # Clean the OpenAlex ID if it contains the full URL\n",
        "    if openalex_id.startswith('https://openalex.org/'):\n",
        "        author_id = openalex_id\n",
        "    else:\n",
        "        author_id = f\"https://openalex.org/{openalex_id}\" if not openalex_id.startswith('A') else f\"https://openalex.org/{openalex_id}\"\n",
        "\n",
        "    while True:\n",
        "        url = f\"https://api.openalex.org/works\"\n",
        "        params = {\n",
        "            'filter': f'author.id:{author_id}',\n",
        "            'per-page': per_page,\n",
        "            'page': page,\n",
        "            'select': 'id,title,publication_year,concepts,keywords,topics,related_works,type,cited_by_count'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            works = data.get('results', [])\n",
        "            if not works:\n",
        "                break\n",
        "\n",
        "            all_works.extend(works)\n",
        "\n",
        "            # Check if we've got all pages\n",
        "            if len(works) < per_page:\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "\n",
        "            # Rate limiting - OpenAlex allows 100 requests per second\n",
        "            time.sleep(0.01)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching works for {author_id}: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_works\n",
        "\n",
        "def extract_topics_from_related_works(related_work_ids: List[str], max_related: int = 10) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch topics from related works (limited to avoid too many API calls)\n",
        "    \"\"\"\n",
        "    related_topics = []\n",
        "\n",
        "    # Limit the number of related works to fetch to avoid excessive API calls\n",
        "    limited_ids = related_work_ids[:max_related]\n",
        "\n",
        "    for work_id in limited_ids:\n",
        "        try:\n",
        "            url = f\"https://api.openalex.org/works/{work_id}\"\n",
        "            params = {'select': 'topics,concepts'}\n",
        "\n",
        "            response = requests.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            # Extract topics and concepts from related work\n",
        "            topics = data.get('topics', [])\n",
        "            concepts = data.get('concepts', [])\n",
        "\n",
        "            for topic in topics:\n",
        "                related_topics.append({\n",
        "                    'name': topic.get('display_name', ''),\n",
        "                    'score': topic.get('score', 0),\n",
        "                    'type': 'related_topic',\n",
        "                    'source_work': work_id\n",
        "                })\n",
        "\n",
        "            for concept in concepts:\n",
        "                if concept.get('score', 0) > 0.3:  # Only high-confidence concepts\n",
        "                    related_topics.append({\n",
        "                        'name': concept.get('display_name', ''),\n",
        "                        'score': concept.get('score', 0),\n",
        "                        'type': 'related_concept',\n",
        "                        'source_work': work_id\n",
        "                    })\n",
        "\n",
        "            time.sleep(0.01)  # Rate limiting\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching related work {work_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return related_topics\n",
        "\n",
        "def create_curated_topic_list(work: Dict) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Create a curated list combining topics, keywords, concepts, and related topics\n",
        "    \"\"\"\n",
        "    curated_topics = []\n",
        "\n",
        "    # Extract topics\n",
        "    topics = work.get('topics', [])\n",
        "    for topic in topics:\n",
        "        curated_topics.append({\n",
        "            'name': topic.get('display_name', ''),\n",
        "            'score': topic.get('score', 0),\n",
        "            'type': 'topic',\n",
        "            'subfield': topic.get('subfield', {}).get('display_name', ''),\n",
        "            'field': topic.get('field', {}).get('display_name', ''),\n",
        "            'domain': topic.get('domain', {}).get('display_name', '')\n",
        "        })\n",
        "\n",
        "    # Extract keywords\n",
        "    keywords = work.get('keywords', [])\n",
        "    for keyword in keywords:\n",
        "        curated_topics.append({\n",
        "            'name': keyword.get('display_name', ''),\n",
        "            'score': keyword.get('score', 0),\n",
        "            'type': 'keyword',\n",
        "            'subfield': '',\n",
        "            'field': '',\n",
        "            'domain': ''\n",
        "        })\n",
        "\n",
        "    # Extract concepts (only those with score > 0.3)\n",
        "    concepts = work.get('concepts', [])\n",
        "    for concept in concepts:\n",
        "        if concept.get('score', 0) > 0.3:\n",
        "            curated_topics.append({\n",
        "                'name': concept.get('display_name', ''),\n",
        "                'score': concept.get('score', 0),\n",
        "                'type': 'concept',\n",
        "                'level': concept.get('level', 0),\n",
        "                'subfield': '',\n",
        "                'field': '',\n",
        "                'domain': ''\n",
        "            })\n",
        "\n",
        "    # Extract topics from related works (limited to avoid too many API calls)\n",
        "    related_work_ids = work.get('related_works', [])\n",
        "    if related_work_ids:\n",
        "        related_topics = extract_topics_from_related_works(related_work_ids, max_related=5)\n",
        "        curated_topics.extend(related_topics)\n",
        "\n",
        "    # Remove duplicates and sort by score\n",
        "    unique_topics = {}\n",
        "    for topic in curated_topics:\n",
        "        name = topic['name'].lower().strip()\n",
        "        if name and name not in unique_topics:\n",
        "            unique_topics[name] = topic\n",
        "        elif name in unique_topics and topic['score'] > unique_topics[name]['score']:\n",
        "            unique_topics[name] = topic\n",
        "\n",
        "    # Sort by score (descending)\n",
        "    sorted_topics = sorted(unique_topics.values(), key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return sorted_topics\n",
        "\n",
        "def process_all_researchers(final_df: pd.DataFrame, checkpoint_dir: str = \"/content/drive/MyDrive/datastore/v2_data\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process all researchers to get their works and curated topics with checkpoint functionality\n",
        "    \"\"\"\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Check for existing checkpoints\n",
        "    checkpoint_file = os.path.join(checkpoint_dir, \"researcher_works_checkpoint.csv\")\n",
        "    progress_file = os.path.join(checkpoint_dir, \"progress_checkpoint.json\")\n",
        "\n",
        "    results = []\n",
        "    start_idx = 0\n",
        "\n",
        "    # Load existing progress if checkpoint exists\n",
        "    if os.path.exists(checkpoint_file) and os.path.exists(progress_file):\n",
        "        print(\"Found existing checkpoint, loading...\")\n",
        "        try:\n",
        "            existing_df = pd.read_csv(checkpoint_file)\n",
        "            if len(existing_df) > 0:\n",
        "                # Convert string representation back to list of dicts\n",
        "                existing_df['curated_topics'] = existing_df['curated_topics'].apply(\n",
        "                    lambda x: eval(x) if isinstance(x, str) else x\n",
        "                )\n",
        "                results = existing_df.to_dict('records')\n",
        "\n",
        "                with open(progress_file, 'r') as f:\n",
        "                    progress = json.load(f)\n",
        "                    start_idx = progress.get('last_completed_idx', 0) + 1\n",
        "\n",
        "                print(f\"Resuming from researcher {start_idx + 1}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting fresh...\")\n",
        "            start_idx = 0\n",
        "            results = []\n",
        "\n",
        "    total_researchers = len(final_df)\n",
        "\n",
        "    for idx in range(start_idx, total_researchers):\n",
        "        row = final_df.iloc[idx]\n",
        "        print(f\"Processing researcher {idx + 1}/{total_researchers}: {row['name']}\")\n",
        "\n",
        "        try:\n",
        "            # Fetch all works for this researcher\n",
        "            works = fetch_author_works(row['openalex_id'])\n",
        "\n",
        "            print(f\"  Found {len(works)} works\")\n",
        "\n",
        "            # Process each work\n",
        "            for work in works:\n",
        "                # Create curated topic list for this work\n",
        "                curated_topics = create_curated_topic_list(work)\n",
        "\n",
        "                work_data = {\n",
        "                    'researcher_name': row['name'],\n",
        "                    'researcher_openalex_id': row['openalex_id'],\n",
        "                    'work_id': work.get('id', ''),\n",
        "                    'work_title': work.get('title', ''),\n",
        "                    'publication_year': work.get('publication_year', None),\n",
        "                    'work_type': work.get('type', ''),\n",
        "                    'cited_by_count': work.get('cited_by_count', 0),\n",
        "                    'curated_topics': curated_topics,\n",
        "                    'num_topics': len([t for t in curated_topics if t['type'] == 'topic']),\n",
        "                    'num_keywords': len([t for t in curated_topics if t['type'] == 'keyword']),\n",
        "                    'num_concepts': len([t for t in curated_topics if t['type'] == 'concept']),\n",
        "                    'num_related_topics': len([t for t in curated_topics if t['type'].startswith('related')])\n",
        "                }\n",
        "\n",
        "                results.append(work_data)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing researcher {row['name']}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Save checkpoint every 5 researchers\n",
        "        if (idx + 1) % 5 == 0:\n",
        "            save_checkpoint(results, idx, checkpoint_dir)\n",
        "            print(f\"Checkpoint saved after processing {idx + 1} researchers\")\n",
        "\n",
        "        # Add delay between researchers to be respectful to the API\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Save final checkpoint\n",
        "    save_checkpoint(results, total_researchers - 1, checkpoint_dir, is_final=True)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def save_checkpoint(results: List[Dict], last_idx: int, checkpoint_dir: str, is_final: bool = False):\n",
        "    \"\"\"\n",
        "    Save checkpoint data to files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert to DataFrame for saving\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        if len(df) > 0:\n",
        "            # Convert curated_topics to string for CSV storage\n",
        "            df_to_save = df.copy()\n",
        "            df_to_save['curated_topics'] = df_to_save['curated_topics'].astype(str)\n",
        "\n",
        "            # Save checkpoint file\n",
        "            checkpoint_file = os.path.join(checkpoint_dir, \"researcher_works_checkpoint.csv\")\n",
        "            df_to_save.to_csv(checkpoint_file, index=False)\n",
        "\n",
        "            # Save progress file\n",
        "            progress_file = os.path.join(checkpoint_dir, \"progress_checkpoint.json\")\n",
        "            progress_data = {\n",
        "                'last_completed_idx': last_idx,\n",
        "                'total_works_processed': len(df),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'is_final': is_final\n",
        "            }\n",
        "\n",
        "            with open(progress_file, 'w') as f:\n",
        "                json.dump(progress_data, f, indent=2)\n",
        "\n",
        "            print(f\"Saved {len(df)} works to checkpoint\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving checkpoint: {e}\")\n",
        "\n",
        "def clean_checkpoints(checkpoint_dir: str = \"/content/drive/MyDrive/datastore/v2_data\"):\n",
        "    \"\"\"\n",
        "    Clean up checkpoint files after successful completion\n",
        "    \"\"\"\n",
        "    try:\n",
        "        checkpoint_file = os.path.join(checkpoint_dir, \"researcher_works_checkpoint.csv\")\n",
        "        progress_file = os.path.join(checkpoint_dir, \"progress_checkpoint.json\")\n",
        "\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "        if os.path.exists(progress_file):\n",
        "            os.remove(progress_file)\n",
        "\n",
        "        print(\"Checkpoint files cleaned up\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error cleaning checkpoints: {e}\")\n",
        "\n",
        "def save_final_data(works_df: pd.DataFrame, summary_df: pd.DataFrame,\n",
        "                   save_dir: str = \"/content/drive/MyDrive/datastore/v2_data\"):\n",
        "    \"\"\"\n",
        "    Save final processed data with timestamp\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save works data\n",
        "    works_file = os.path.join(save_dir, f\"researcher_works_with_topics_{timestamp}.csv\")\n",
        "\n",
        "    # Convert curated_topics to string for CSV storage\n",
        "    works_to_save = works_df.copy()\n",
        "    works_to_save['curated_topics'] = works_to_save['curated_topics'].astype(str)\n",
        "    works_to_save.to_csv(works_file, index=False)\n",
        "\n",
        "    # Save summary data\n",
        "    summary_file = os.path.join(save_dir, f\"researcher_topic_summary_{timestamp}.csv\")\n",
        "\n",
        "    # Convert top_topics to string for CSV storage\n",
        "    summary_to_save = summary_df.copy()\n",
        "    summary_to_save['top_topics'] = summary_to_save['top_topics'].astype(str)\n",
        "    summary_to_save.to_csv(summary_file, index=False)\n",
        "\n",
        "    # Also save the latest versions without timestamp\n",
        "    latest_works_file = os.path.join(save_dir, \"researcher_works_with_topics_latest.csv\")\n",
        "    latest_summary_file = os.path.join(save_dir, \"researcher_topic_summary_latest.csv\")\n",
        "\n",
        "    works_to_save.to_csv(latest_works_file, index=False)\n",
        "    summary_to_save.to_csv(latest_summary_file, index=False)\n",
        "\n",
        "    print(f\"Final data saved:\")\n",
        "    print(f\"  Works data: {works_file}\")\n",
        "    print(f\"  Summary data: {summary_file}\")\n",
        "    print(f\"  Latest versions also saved without timestamps\")\n",
        "\n",
        "    return works_file, summary_file\n",
        "\n",
        "def get_topic_summary_by_researcher(works_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a summary of topics by researcher\n",
        "    \"\"\"\n",
        "    summary_data = []\n",
        "\n",
        "    for researcher in works_df['researcher_name'].unique():\n",
        "        researcher_works = works_df[works_df['researcher_name'] == researcher]\n",
        "\n",
        "        # Collect all topics for this researcher\n",
        "        all_topics = []\n",
        "        for _, work in researcher_works.iterrows():\n",
        "            all_topics.extend(work['curated_topics'])\n",
        "\n",
        "        # Count topic frequencies\n",
        "        topic_counts = defaultdict(lambda: {'count': 0, 'total_score': 0, 'types': set()})\n",
        "\n",
        "        for topic in all_topics:\n",
        "            name = topic['name']\n",
        "            topic_counts[name]['count'] += 1\n",
        "            topic_counts[name]['total_score'] += topic['score']\n",
        "            topic_counts[name]['types'].add(topic['type'])\n",
        "\n",
        "        # Get top topics for this researcher\n",
        "        top_topics = sorted(\n",
        "            topic_counts.items(),\n",
        "            key=lambda x: (x[1]['count'], x[1]['total_score']),\n",
        "            reverse=True\n",
        "        )[:20]  # Top 20 topics\n",
        "\n",
        "        summary_data.append({\n",
        "            'researcher_name': researcher,\n",
        "            'researcher_openalex_id': researcher_works.iloc[0]['researcher_openalex_id'],\n",
        "            'total_works': len(researcher_works),\n",
        "            'total_citations': researcher_works['cited_by_count'].sum(),\n",
        "            'top_topics': [\n",
        "                {\n",
        "                    'name': topic[0],\n",
        "                    'frequency': topic[1]['count'],\n",
        "                    'avg_score': topic[1]['total_score'] / topic[1]['count'],\n",
        "                    'types': list(topic[1]['types'])\n",
        "                }\n",
        "                for topic in top_topics\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary_data)\n",
        "\n",
        "# Main execution function\n",
        "def main(final_df: pd.DataFrame, save_dir: str = \"/content/drive/MyDrive/datastore/v2_data\"):\n",
        "    \"\"\"\n",
        "    Main function to process all researchers and their works with checkpoint support\n",
        "    \"\"\"\n",
        "    print(\"Starting to process researchers and their works...\")\n",
        "    print(f\"Processing {len(final_df)} researchers\")\n",
        "    print(f\"Data will be saved to: {save_dir}\")\n",
        "    print(\"Checkpoints will be saved every 5 researchers\")\n",
        "\n",
        "    # Process all researchers and their works (with checkpoints)\n",
        "    works_df = process_all_researchers(final_df, save_dir)\n",
        "\n",
        "    print(f\"\\nProcessed {len(works_df)} total works\")\n",
        "\n",
        "    # Create summary by researcher\n",
        "    print(\"Creating researcher summaries...\")\n",
        "    summary_df = get_topic_summary_by_researcher(works_df)\n",
        "\n",
        "    # Save final data\n",
        "    print(\"Saving final data...\")\n",
        "    works_file, summary_file = save_final_data(works_df, summary_df, save_dir)\n",
        "\n",
        "    # Clean up checkpoint files\n",
        "    clean_checkpoints(save_dir)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Total researchers processed: {summary_df.shape[0]}\")\n",
        "    print(f\"Total works processed: {len(works_df)}\")\n",
        "    print(f\"Files saved to: {save_dir}\")\n",
        "\n",
        "    return works_df, summary_df\n",
        "\n",
        "# Resume from checkpoint function\n",
        "def resume_processing(final_df: pd.DataFrame, save_dir: str = \"/content/drive/MyDrive/datastore/v2_data\"):\n",
        "    \"\"\"\n",
        "    Resume processing from the last checkpoint\n",
        "    \"\"\"\n",
        "    progress_file = os.path.join(save_dir, \"progress_checkpoint.json\")\n",
        "\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            progress = json.load(f)\n",
        "\n",
        "        print(f\"Found checkpoint from {progress['timestamp']}\")\n",
        "        print(f\"Last completed researcher index: {progress['last_completed_idx']}\")\n",
        "        print(f\"Works processed so far: {progress['total_works_processed']}\")\n",
        "\n",
        "        return main(final_df, save_dir)\n",
        "    else:\n",
        "        print(\"No checkpoint found. Starting fresh...\")\n",
        "        return main(final_df, save_dir)\n",
        "\n",
        "# Function to check current progress\n",
        "def check_progress(save_dir: str = \"/content/drive/MyDrive/datastore/v2_data\"):\n",
        "    \"\"\"\n",
        "    Check the current progress if a checkpoint exists\n",
        "    \"\"\"\n",
        "    progress_file = os.path.join(save_dir, \"progress_checkpoint.json\")\n",
        "\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            progress = json.load(f)\n",
        "\n",
        "        print(\"Current Progress:\")\n",
        "        print(f\"  Last completed researcher index: {progress['last_completed_idx']}\")\n",
        "        print(f\"  Total works processed: {progress['total_works_processed']}\")\n",
        "        print(f\"  Last update: {progress['timestamp']}\")\n",
        "        print(f\"  Is final: {progress.get('is_final', False)}\")\n",
        "\n",
        "        return progress\n",
        "    else:\n",
        "        print(\"No checkpoint found.\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Method 1: Start fresh processing\n",
        "# works_df, summary_df = main(final_df)\n",
        "\n",
        "# Method 2: Resume from checkpoint (if exists)\n",
        "works_df, summary_df = resume_processing(final_df)\n",
        "\n",
        "# Method 3: Check current progress\n",
        "# check_progress()\n",
        "\n",
        "# Method 4: Manual checkpoint control\n",
        "# works_df = process_all_researchers(final_df, \"/content/drive/MyDrive/datastore/v2_data\")\n",
        "# summary_df = get_topic_summary_by_researcher(works_df)\n",
        "# save_final_data(works_df, summary_df, \"/content/drive/MyDrive/datastore/v2_data\")\n",
        "\n",
        "# The recommended approach is to use resume_processing() as it will:\n",
        "# 1. Check for existing checkpoints and resume if found\n",
        "# 2. Start fresh if no checkpoints exist\n",
        "# 3. Save checkpoints every 5 researchers automatically\n",
        "# 4. Save final data with timestamps\n",
        "# 5. Clean up checkpoint files when complete\n",
        "\n",
        "# Example of complete workflow:\n",
        "\"\"\"\n",
        "# Check if there's any existing progress\n",
        "check_progress()\n",
        "\n",
        "# Start or resume processing\n",
        "works_df, summary_df = resume_processing(final_df)\n",
        "\n",
        "# Files will be automatically saved to /content/drive/MyDrive/datastore/v2_data/\n",
        "# with both timestamped versions and \"latest\" versions\n",
        "\"\"\"\n",
        "\n",
        "# To view a sample of curated topics for a specific work:\n",
        "def display_sample_topics(works_df: pd.DataFrame, work_index: int = 0):\n",
        "    \"\"\"\n",
        "    Display sample curated topics for a specific work\n",
        "    \"\"\"\n",
        "    if work_index < len(works_df):\n",
        "        work = works_df.iloc[work_index]\n",
        "        print(f\"Work: {work['work_title']}\")\n",
        "        print(f\"Researcher: {work['researcher_name']}\")\n",
        "        print(f\"Year: {work['publication_year']}\")\n",
        "        print(\"\\nCurated Topics:\")\n",
        "\n",
        "        for i, topic in enumerate(work['curated_topics'][:10]):  # Show top 10\n",
        "            print(f\"{i+1}. {topic['name']} (Score: {topic['score']:.3f}, Type: {topic['type']})\")\n",
        "            if topic.get('field'):\n",
        "                print(f\"   Field: {topic['field']} > {topic['subfield']}\")\n",
        "    else:\n",
        "        print(\"Work index out of range\")\n",
        "\n",
        "# Example of how to use:\n",
        "# display_sample_topics(works_df, 0)\n",
        "\n",
        "# Handling Google Colab disconnections:\n",
        "\"\"\"\n",
        "If your Colab session disconnects or crashes:\n",
        "\n",
        "1. Reconnect to your session\n",
        "2. Re-run the import statements and function definitions\n",
        "3. Load your final_df again\n",
        "4. Run: works_df, summary_df = resume_processing(final_df)\n",
        "\n",
        "The system will automatically detect where you left off and continue processing\n",
        "from the last completed checkpoint.\n",
        "\n",
        "Checkpoint files stored in /content/drive/MyDrive/datastore/v2_data/:\n",
        "- researcher_works_checkpoint.csv (temporary checkpoint data)\n",
        "- progress_checkpoint.json (progress tracking)\n",
        "- researcher_works_with_topics_YYYYMMDD_HHMMSS.csv (final timestamped results)\n",
        "- researcher_topic_summary_YYYYMMDD_HHMMSS.csv (final timestamped summary)\n",
        "- researcher_works_with_topics_latest.csv (latest version without timestamp)\n",
        "- researcher_topic_summary_latest.csv (latest summary without timestamp)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rvxFTN2Na7v6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Optional\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def call_openalex_api(endpoint, params=None, email=\"your_email@example.com\"):\n",
        "    \"\"\"Make API calls with proper headers and rate limiting\"\"\"\n",
        "    base_url = f\"https://api.openalex.org/{endpoint}\"\n",
        "    headers = {'User-Agent': f'mailto:{email}'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        time.sleep(0.2)  # Conservative rate limiting\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling {endpoint} API: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def reconstruct_abstract(inverted_index: Dict) -> str:\n",
        "    \"\"\"Reconstruct abstract text from OpenAlex inverted index format.\"\"\"\n",
        "    if not inverted_index:\n",
        "        return \"\"\n",
        "\n",
        "    word_positions = []\n",
        "    for word, positions in inverted_index.items():\n",
        "        for pos in positions:\n",
        "            word_positions.append((pos, word))\n",
        "\n",
        "    word_positions.sort(key=lambda x: x[0])\n",
        "    words = [word for _, word in word_positions]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "def get_single_work_abstract(work_id: str, email: str = \"your_email@example.com\") -> str:\n",
        "    \"\"\"Fetch abstract for a single work ID\"\"\"\n",
        "    try:\n",
        "        # Clean the work ID\n",
        "        if work_id.startswith('https://openalex.org/'):\n",
        "            clean_id = work_id.split('/')[-1]\n",
        "        elif work_id.startswith('W'):\n",
        "            clean_id = work_id\n",
        "        else:\n",
        "            clean_id = f\"W{work_id}\"\n",
        "\n",
        "        # Call API for single work\n",
        "        response = call_openalex_api(f'works/{clean_id}', email=email)\n",
        "\n",
        "        if response and 'abstract_inverted_index' in response:\n",
        "            abstract_inverted = response['abstract_inverted_index']\n",
        "            return reconstruct_abstract(abstract_inverted)\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching abstract for {work_id}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def fetch_abstracts_for_existing_data(df: pd.DataFrame,\n",
        "                                    email: str = \"your_email@example.com\",\n",
        "                                    checkpoint_dir: str = \"/content/drive/MyDrive/datastore/v2_data\",\n",
        "                                    save_every: int = 50) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch abstracts for existing work data using individual API calls with robust checkpointing\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"IMPORTANT: Make sure to update email parameter!\")\n",
        "    print(f\"Current email: {email}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Checkpoint files\n",
        "    abstracts_checkpoint_file = os.path.join(checkpoint_dir, \"abstracts_robust_checkpoint.json\")\n",
        "    progress_checkpoint_file = os.path.join(checkpoint_dir, \"abstracts_robust_progress.json\")\n",
        "\n",
        "    # Get unique work IDs\n",
        "    unique_work_ids = df['work_id'].unique().tolist()\n",
        "    total_works = len(unique_work_ids)\n",
        "\n",
        "    print(f\"Found {total_works} unique works to fetch abstracts for\")\n",
        "\n",
        "    # Load existing abstracts if checkpoint exists\n",
        "    fetched_abstracts = {}\n",
        "    start_idx = 0\n",
        "\n",
        "    if os.path.exists(abstracts_checkpoint_file):\n",
        "        print(\"Found existing abstracts checkpoint, loading...\")\n",
        "        try:\n",
        "            with open(abstracts_checkpoint_file, 'r') as f:\n",
        "                fetched_abstracts = json.load(f)\n",
        "\n",
        "            if os.path.exists(progress_checkpoint_file):\n",
        "                with open(progress_checkpoint_file, 'r') as f:\n",
        "                    progress = json.load(f)\n",
        "                    start_idx = progress.get('last_completed_index', 0) + 1\n",
        "\n",
        "            print(f\"Loaded {len(fetched_abstracts)} existing abstracts\")\n",
        "            print(f\"Resuming from work {start_idx + 1}/{total_works}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting fresh...\")\n",
        "            fetched_abstracts = {}\n",
        "            start_idx = 0\n",
        "\n",
        "    # Process remaining work IDs\n",
        "    remaining_work_ids = unique_work_ids[start_idx:]\n",
        "\n",
        "    if remaining_work_ids:\n",
        "        print(f\"\\nFetching abstracts for {len(remaining_work_ids)} remaining works...\")\n",
        "\n",
        "        success_count = len(fetched_abstracts)\n",
        "\n",
        "        # Process each work individually with progress bar\n",
        "        for i, work_id in enumerate(tqdm(remaining_work_ids, desc=\"Fetching abstracts\")):\n",
        "            current_idx = start_idx + i\n",
        "\n",
        "            try:\n",
        "                # Skip if already fetched (shouldn't happen, but safety check)\n",
        "                if work_id in fetched_abstracts:\n",
        "                    continue\n",
        "\n",
        "                # Fetch abstract for this work\n",
        "                abstract = get_single_work_abstract(work_id, email)\n",
        "                fetched_abstracts[work_id] = abstract\n",
        "\n",
        "                if abstract:\n",
        "                    success_count += 1\n",
        "\n",
        "                # Save checkpoint every N works\n",
        "                if (i + 1) % save_every == 0:\n",
        "                    save_robust_checkpoint(fetched_abstracts, current_idx, checkpoint_dir)\n",
        "                    print(f\"\\nCheckpoint saved: {len(fetched_abstracts)} abstracts fetched ({success_count} non-empty)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing work {work_id}: {e}\")\n",
        "                # Still add empty abstract to avoid re-processing\n",
        "                fetched_abstracts[work_id] = \"\"\n",
        "                continue\n",
        "\n",
        "    # Final checkpoint save\n",
        "    if remaining_work_ids:\n",
        "        save_robust_checkpoint(fetched_abstracts, total_works - 1, checkpoint_dir, is_final=True)\n",
        "\n",
        "    # Add abstracts to dataframe\n",
        "    print(\"\\nMerging abstracts into dataframe...\")\n",
        "    df_copy = df.copy()\n",
        "    df_copy['abstract'] = df_copy['work_id'].map(lambda x: fetched_abstracts.get(x, \"\"))\n",
        "\n",
        "    # Statistics\n",
        "    non_empty_abstracts = (df_copy['abstract'] != \"\").sum()\n",
        "    success_rate = non_empty_abstracts / len(df_copy) * 100\n",
        "\n",
        "    print(f\"\\nAbstract Fetching Complete!\")\n",
        "    print(f\"Total works: {len(df_copy)}\")\n",
        "    print(f\"Works with abstracts: {non_empty_abstracts}\")\n",
        "    print(f\"Success rate: {success_rate:.1f}%\")\n",
        "\n",
        "    # Clean up checkpoint files\n",
        "    cleanup_robust_checkpoints(checkpoint_dir)\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "def save_robust_checkpoint(abstracts: Dict[str, str], last_idx: int, checkpoint_dir: str, is_final: bool = False):\n",
        "    \"\"\"Save checkpoint for robust abstract fetching\"\"\"\n",
        "    try:\n",
        "        abstracts_file = os.path.join(checkpoint_dir, \"abstracts_robust_checkpoint.json\")\n",
        "        progress_file = os.path.join(checkpoint_dir, \"abstracts_robust_progress.json\")\n",
        "\n",
        "        # Save abstracts\n",
        "        with open(abstracts_file, 'w') as f:\n",
        "            json.dump(abstracts, f)\n",
        "\n",
        "        # Save progress\n",
        "        progress_data = {\n",
        "            'last_completed_index': last_idx,\n",
        "            'total_abstracts_fetched': len(abstracts),\n",
        "            'non_empty_abstracts': len([a for a in abstracts.values() if a]),\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'is_final': is_final\n",
        "        }\n",
        "\n",
        "        with open(progress_file, 'w') as f:\n",
        "            json.dump(progress_data, f, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving checkpoint: {e}\")\n",
        "\n",
        "def cleanup_robust_checkpoints(checkpoint_dir: str):\n",
        "    \"\"\"Clean up checkpoint files after completion\"\"\"\n",
        "    try:\n",
        "        abstracts_file = os.path.join(checkpoint_dir, \"abstracts_robust_checkpoint.json\")\n",
        "        progress_file = os.path.join(checkpoint_dir, \"abstracts_robust_progress.json\")\n",
        "\n",
        "        if os.path.exists(abstracts_file):\n",
        "            os.remove(abstracts_file)\n",
        "        if os.path.exists(progress_file):\n",
        "            os.remove(progress_file)\n",
        "\n",
        "        print(\"Robust abstracts checkpoint files cleaned up\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cleaning up checkpoints: {e}\")\n",
        "\n",
        "def check_robust_progress(checkpoint_dir: str = \"/content/drive/MyDrive/datastore/v2_data\"):\n",
        "    \"\"\"Check current progress of robust abstract fetching\"\"\"\n",
        "    progress_file = os.path.join(checkpoint_dir, \"abstracts_robust_progress.json\")\n",
        "\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            progress = json.load(f)\n",
        "\n",
        "        print(\"Robust Abstract Fetching Progress:\")\n",
        "        print(f\"  Last completed index: {progress['last_completed_index']}\")\n",
        "        print(f\"  Total abstracts fetched: {progress['total_abstracts_fetched']}\")\n",
        "        print(f\"  Non-empty abstracts: {progress.get('non_empty_abstracts', 'Unknown')}\")\n",
        "        print(f\"  Last update: {progress['timestamp']}\")\n",
        "        print(f\"  Is final: {progress.get('is_final', False)}\")\n",
        "\n",
        "        return progress\n",
        "    else:\n",
        "        print(\"No robust abstract fetching progress found.\")\n",
        "        return None\n",
        "\n",
        "def test_single_abstract(work_id: str, email: str = \"your_email@example.com\"):\n",
        "    \"\"\"Test fetching a single abstract with detailed debugging\"\"\"\n",
        "    print(f\"Testing abstract fetch for work ID: {work_id}\")\n",
        "    print(f\"Using email: {email}\")\n",
        "\n",
        "    try:\n",
        "        # Clean the work ID\n",
        "        if work_id.startswith('https://openalex.org/'):\n",
        "            clean_id = work_id.split('/')[-1]\n",
        "        elif work_id.startswith('W'):\n",
        "            clean_id = work_id\n",
        "        else:\n",
        "            clean_id = f\"W{work_id}\"\n",
        "\n",
        "        print(f\"Cleaned work ID: {clean_id}\")\n",
        "\n",
        "        # Call API for single work\n",
        "        response = call_openalex_api(f'works/{clean_id}', email=email)\n",
        "\n",
        "        print(f\"\\nAPI Response Status: {'Success' if response else 'Failed'}\")\n",
        "\n",
        "        if response:\n",
        "            print(f\"Response keys: {list(response.keys())}\")\n",
        "            print(f\"Work title: {response.get('display_name', 'No title')}\")\n",
        "            print(f\"Publication year: {response.get('publication_year', 'Unknown')}\")\n",
        "            print(f\"Has abstract_inverted_index: {'abstract_inverted_index' in response}\")\n",
        "\n",
        "            if 'abstract_inverted_index' in response:\n",
        "                abstract_inverted = response['abstract_inverted_index']\n",
        "                print(f\"Abstract inverted index type: {type(abstract_inverted)}\")\n",
        "                print(f\"Abstract inverted index empty: {not bool(abstract_inverted)}\")\n",
        "\n",
        "                if abstract_inverted:\n",
        "                    print(f\"Sample words in abstract: {list(abstract_inverted.keys())[:10]}\")\n",
        "                    abstract = reconstruct_abstract(abstract_inverted)\n",
        "                    print(f\"Reconstructed abstract length: {len(abstract)} characters\")\n",
        "                    if abstract:\n",
        "                        print(f\"Abstract preview: {abstract[:300]}...\")\n",
        "                        return abstract\n",
        "                    else:\n",
        "                        print(\"Failed to reconstruct abstract\")\n",
        "                        return \"\"\n",
        "                else:\n",
        "                    print(\"Abstract inverted index is empty\")\n",
        "                    return \"\"\n",
        "            else:\n",
        "                print(\"No abstract_inverted_index field in response\")\n",
        "                return \"\"\n",
        "        else:\n",
        "            print(\"API call failed\")\n",
        "            return \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in test: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def test_multiple_works(df: pd.DataFrame, email: str = \"your_email@example.com\", num_samples: int = 5):\n",
        "    \"\"\"Test multiple works to find ones with abstracts\"\"\"\n",
        "    print(f\"Testing {num_samples} sample works to find abstracts...\")\n",
        "\n",
        "    sample_works = df.sample(min(num_samples, len(df)))\n",
        "\n",
        "    for i, (_, row) in enumerate(sample_works.iterrows()):\n",
        "        print(f\"\\n--- Test {i+1}/{num_samples} ---\")\n",
        "        print(f\"Work: {row['work_title']}\")\n",
        "        print(f\"Year: {row['publication_year']}\")\n",
        "        print(f\"Citations: {row['cited_by_count']}\")\n",
        "\n",
        "        abstract = test_single_abstract(row['work_id'], email)\n",
        "\n",
        "        if abstract:\n",
        "            print(\"✅ Found abstract!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ No abstract\")\n",
        "\n",
        "    print(f\"\\n⚠️  No abstracts found in {num_samples} samples\")\n",
        "    return False\n",
        "\n",
        "def main_robust_abstracts(csv_path: str = \"/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_latest.csv\",\n",
        "                         email: str = \"your_email@example.com\"):\n",
        "    \"\"\"\n",
        "    Main function to add abstracts to existing CSV using robust method\n",
        "    \"\"\"\n",
        "    print(\"Robust Abstract Fetcher for Existing Data\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load existing data\n",
        "    print(\"Loading existing data...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    print(f\"Loaded {len(df)} works from {len(df['researcher_name'].unique())} researchers\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Test with multiple works to find ones with abstracts\n",
        "    print(\"\\nTesting with multiple works to find abstracts...\")\n",
        "    abstracts_found = test_multiple_works(df, email, num_samples=10)\n",
        "\n",
        "    if abstracts_found or input(\"\\nContinue even though no abstracts found in samples? (y/n): \").lower() == 'y':\n",
        "        print(\"\\nProceeding with full dataset...\")\n",
        "\n",
        "        # Fetch all abstracts\n",
        "        df_with_abstracts = fetch_abstracts_for_existing_data(df, email)\n",
        "\n",
        "        # Save updated CSV\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        base_dir = os.path.dirname(csv_path)\n",
        "\n",
        "        # Save timestamped version\n",
        "        output_file = os.path.join(base_dir, f\"researcher_works_with_topics_and_abstracts_robust_{timestamp}.csv\")\n",
        "        df_with_abstracts.to_csv(output_file, index=False)\n",
        "\n",
        "        # Save latest version\n",
        "        latest_file = os.path.join(base_dir, \"researcher_works_with_topics_and_abstracts_robust_latest.csv\")\n",
        "        df_with_abstracts.to_csv(latest_file, index=False)\n",
        "\n",
        "        print(f\"\\nSuccess! Updated data saved to:\")\n",
        "        print(f\"  Timestamped: {output_file}\")\n",
        "        print(f\"  Latest: {latest_file}\")\n",
        "\n",
        "        return df_with_abstracts\n",
        "    else:\n",
        "        print(\"Stopping due to test failure.\")\n",
        "        return None\n",
        "\n",
        "# Usage examples:\n",
        "\n",
        "# Method 1: Run the complete process (REMEMBER TO UPDATE EMAIL!)\n",
        "df_with_abstracts = main_robust_abstracts(email=\"your_actual_email@example.com\")\n",
        "\n",
        "# Method 2: Check current progress\n",
        "# check_robust_progress()\n",
        "\n",
        "# Method 3: Test with a single work ID first (with debugging)\n",
        "# test_single_abstract(\"W2556052759\", email=\"your_actual_email@example.com\")\n",
        "\n",
        "# Method 3b: Test with multiple works to find abstracts\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_latest.csv\")\n",
        "# test_multiple_works(df, email=\"your_actual_email@example.com\", num_samples=10)\n",
        "\n",
        "# Method 4: Resume from checkpoint (just run main_robust_abstracts again)\n",
        "# df_with_abstracts = main_robust_abstracts(email=\"your_actual_email@example.com\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPORTANT SETUP INSTRUCTIONS:\")\n",
        "print(\"1. Update the email parameter with your actual email address\")\n",
        "print(\"2. Run: df_with_abstracts = main_robust_abstracts(email='your_email@domain.com')\")\n",
        "print(\"3. The script will test with one work first, then process all\")\n",
        "print(\"4. Checkpoints are saved every 50 works\")\n",
        "print(\"5. You can resume if interrupted by running the same command again\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZpMc3nIMSYZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_latest.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYT5bZGPMTu7"
      },
      "outputs": [],
      "source": [
        "df['curated_topics'] = df['curated_topics'].apply(lambda x: eval(x) if isinstance(x, str) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJfo3oBjMUZf"
      },
      "outputs": [],
      "source": [
        "# prompt: lets see the full data all of it for df.head(1)\n",
        "\n",
        "import pandas as pd\n",
        "# Configure pandas to display all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Display the first row with all columns\n",
        "print(df.head(2).to_string())\n",
        "\n",
        "# Reset display options to default\n",
        "pd.reset_option('display.max_columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U1gEDOSMvAC"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4__JTAOKZ1vW"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQdejBryT2eB"
      },
      "outputs": [],
      "source": [
        "grants_df = pd.read_csv(\"/content/drive/MyDrive/datastore/v2_data/texas_state_researchers_final_analysis.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDQdPsyUN-Xi"
      },
      "outputs": [],
      "source": [
        "grants_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfG-Em0CT0dU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_and_abstracts_robust_20250703_191556.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find out which research has how many abstracts missing. find total percentage of missing abstract too.\n",
        "\n",
        "import pandas as pd\n",
        "# Assuming df is your DataFrame containing the fetched abstracts\n",
        "# If you are loading from a file, ensure the 'abstract' column exists\n",
        "# Example: df = pd.read_csv('your_data_with_abstracts.csv')\n",
        "\n",
        "# Check if the 'abstract' column exists\n",
        "if 'abstract' not in df.columns:\n",
        "    print(\"Error: 'abstract' column not found in the DataFrame.\")\n",
        "else:\n",
        "    # Count missing abstracts per researcher\n",
        "    missing_abstracts_by_researcher = df[df['abstract'].isna() | (df['abstract'] == '')] \\\n",
        "        .groupby('researcher_name') \\\n",
        "        .size() \\\n",
        "        .sort_values(ascending=False)\n",
        "\n",
        "    print(\"Number of missing abstracts per researcher:\")\n",
        "    print(missing_abstracts_by_researcher)\n",
        "\n",
        "    # Total number of works\n",
        "    total_works = len(df)\n",
        "\n",
        "    # Total number of works with missing abstracts\n",
        "    total_missing_abstracts = (df['abstract'].isna() | (df['abstract'] == '')).sum()\n",
        "\n",
        "    # Calculate the total percentage of missing abstracts\n",
        "    total_percentage_missing = (total_missing_abstracts / total_works) * 100 if total_works > 0 else 0\n",
        "\n",
        "    print(f\"\\nTotal number of works: {total_works}\")\n",
        "    print(f\"Total number of works with missing abstracts: {total_missing_abstracts}\")\n",
        "    print(f\"Total percentage of missing abstracts: {total_percentage_missing:.2f}%\")"
      ],
      "metadata": {
        "id": "GbdlHef1Gnir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "9Q1FJ9UNGpNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Steps"
      ],
      "metadata": {
        "id": "2dSVPFleJ-FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot a distribution of publication years\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it has a 'publication_year' column\n",
        "\n",
        "# Check if 'publication_year' column exists\n",
        "if 'publication_year' in df.columns:\n",
        "    # Filter out works with missing or invalid years\n",
        "    valid_years = df['publication_year'].dropna().astype(int)\n",
        "\n",
        "    if not valid_years.empty:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.histplot(valid_years, kde=True, bins=30) # Use histplot for distribution\n",
        "        plt.title('Distribution of Publication Years')\n",
        "        plt.xlabel('Publication Year')\n",
        "        plt.ylabel('Number of Works')\n",
        "        plt.grid(axis='y', alpha=0.75)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid publication years found in the DataFrame.\")\n",
        "else:\n",
        "    print(\"Error: 'publication_year' column not found in the DataFrame.\")\n"
      ],
      "metadata": {
        "id": "MCaHvSw-PMk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Vectors and TF-IDF"
      ],
      "metadata": {
        "id": "PL6Psv1wi3H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML/NLP imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import math\n",
        "\n",
        "class ResearcherProfileProcessor:\n",
        "    \"\"\"\n",
        "    Processes researcher data to create weighted expertise profiles, conceptual vectors,\n",
        "    grant experience factors, and evidence indices for expertise matching.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, current_year: int = 2025, min_proxy_score: float = 0.8):\n",
        "        self.current_year = current_year\n",
        "        self.min_proxy_score = min_proxy_score\n",
        "        self.logger = self._setup_logging()\n",
        "        self.sentence_model = None\n",
        "        self.tfidf_model = None\n",
        "\n",
        "    def _setup_logging(self) -> logging.Logger:\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logger = logging.getLogger('ResearcherProfileProcessor')\n",
        "        logger.setLevel(logging.INFO)\n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        return logger\n",
        "\n",
        "    def load_data(self, researcher_file: str, grants_file: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Load and validate input datasets\"\"\"\n",
        "        self.logger.info(\"Loading datasets...\")\n",
        "\n",
        "        # Load researcher data\n",
        "        df = pd.read_csv(researcher_file)\n",
        "        self.logger.info(f\"Loaded {len(df)} researcher work records\")\n",
        "\n",
        "        # Load grants data\n",
        "        grants_df = pd.read_csv(grants_file)\n",
        "        self.logger.info(f\"Loaded {len(grants_df)} grant records\")\n",
        "\n",
        "        # Validate required columns\n",
        "        required_researcher_cols = ['researcher_name', 'researcher_openalex_id', 'work_id',\n",
        "                                  'work_title', 'publication_year', 'cited_by_count',\n",
        "                                  'curated_topics', 'abstract']\n",
        "        required_grants_cols = ['openalex_id', 'pi_7_years', 'copi_7_years']\n",
        "\n",
        "        missing_cols = [col for col in required_researcher_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing columns in researcher data: {missing_cols}\")\n",
        "\n",
        "        missing_grant_cols = [col for col in required_grants_cols if col not in grants_df.columns]\n",
        "        if missing_grant_cols:\n",
        "            raise ValueError(f\"Missing columns in grants data: {missing_grant_cols}\")\n",
        "\n",
        "        return df, grants_df\n",
        "\n",
        "    def parse_curated_topics(self, topics_str: str) -> List[Dict]:\n",
        "        \"\"\"Parse the curated_topics string safely - handles both JSON and Python dict formats\"\"\"\n",
        "        if pd.isna(topics_str) or not topics_str.strip():\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Try JSON first (proper format)\n",
        "            topics = json.loads(topics_str)\n",
        "            if isinstance(topics, list):\n",
        "                return topics\n",
        "            else:\n",
        "                return []\n",
        "        except (json.JSONDecodeError, TypeError):\n",
        "            try:\n",
        "                # Fallback: Try ast.literal_eval for Python dict strings\n",
        "                import ast\n",
        "                topics = ast.literal_eval(topics_str)\n",
        "                if isinstance(topics, list):\n",
        "                    return topics\n",
        "                else:\n",
        "                    return []\n",
        "            except (ValueError, SyntaxError):\n",
        "                try:\n",
        "                    # Last resort: Fix single quotes and try JSON again\n",
        "                    fixed_json = topics_str.replace(\"'\", '\"')\n",
        "                    topics = json.loads(fixed_json)\n",
        "                    if isinstance(topics, list):\n",
        "                        return topics\n",
        "                    else:\n",
        "                        return []\n",
        "                except:\n",
        "                    return []\n",
        "\n",
        "    def calculate_recency_weight(self, publication_year: int) -> float:\n",
        "        \"\"\"\n",
        "        Calculate recency weight: Wt = max(0.1, 1 - (CurrentYear - PublicationYear) / 15)\n",
        "        Modified to be less restrictive and ensure minimum weight of 0.1\n",
        "        \"\"\"\n",
        "        return max(0.1, 1 - (self.current_year - publication_year) / 15)\n",
        "\n",
        "    def calculate_topic_importance_score(self, topic_score: float, publication_year: int,\n",
        "                                       citations: int) -> float:\n",
        "        \"\"\"\n",
        "        Calculate topic importance score (not for repetition, but for weighting)\n",
        "        \"\"\"\n",
        "        recency_weight = self.calculate_recency_weight(publication_year)\n",
        "        citation_weight = math.log10(1 + citations)\n",
        "        importance = topic_score * recency_weight * citation_weight\n",
        "        return importance\n",
        "\n",
        "    def extract_keywords_from_topic(self, topic_name: str, stop_words: set) -> List[str]:\n",
        "        \"\"\"Extract meaningful keywords from a topic name, creating better phrases.\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Clean and split the topic name\n",
        "        cleaned = re.sub(r'[^\\w\\s-]', ' ', topic_name.lower())\n",
        "        words = cleaned.split()\n",
        "\n",
        "        # Filter out stop words and very short words\n",
        "        keywords = []\n",
        "        for word in words:\n",
        "            word = word.strip('-')\n",
        "            if (len(word) >= 3 and\n",
        "                word not in stop_words and\n",
        "                not word.isdigit()):\n",
        "                keywords.append(word)\n",
        "\n",
        "        # Create both individual keywords and meaningful phrases\n",
        "        result = []\n",
        "\n",
        "        # Add individual keywords\n",
        "        result.extend(keywords[:3])  # Limit individual keywords\n",
        "\n",
        "        # Create 2-word phrases if we have enough words\n",
        "        if len(keywords) >= 2:\n",
        "            phrases = []\n",
        "            for i in range(min(len(keywords)-1, 2)):  # Max 2 phrases\n",
        "                phrase = f\"{keywords[i]}_{keywords[i+1]}\"\n",
        "                phrases.append(phrase)\n",
        "            result.extend(phrases)\n",
        "\n",
        "        # If we have 3+ words, create one longer phrase\n",
        "        if len(keywords) >= 3:\n",
        "            long_phrase = \"_\".join(keywords[:3])\n",
        "            result.append(long_phrase)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def create_weighted_expertise_document(self, researcher_papers: pd.DataFrame) -> str:\n",
        "        \"\"\"Create weighted expertise document with better keyword distribution.\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Common stop words to remove\n",
        "        stop_words = {'and', 'in', 'of', 'for', 'the', 'a', 'an', 'to', 'with', 'on', 'at', 'by'}\n",
        "\n",
        "        # Collect topics with their importance scores\n",
        "        weighted_topics = []\n",
        "\n",
        "        for _, paper in researcher_papers.iterrows():\n",
        "            topics = self.parse_curated_topics(paper['curated_topics'])\n",
        "            publication_year = paper['publication_year']\n",
        "            citations = paper['cited_by_count'] if pd.notna(paper['cited_by_count']) else 0\n",
        "\n",
        "            for topic in topics:\n",
        "                if topic.get('type') == 'topic':\n",
        "                    topic_score = topic.get('score', 0)\n",
        "                    topic_name = topic.get('name', '').strip()\n",
        "\n",
        "                    if topic_name and topic_score > 0:\n",
        "                        # Calculate importance score\n",
        "                        importance = self.calculate_topic_importance_score(\n",
        "                            topic_score, publication_year, citations\n",
        "                        )\n",
        "\n",
        "                        # Extract keywords and phrases\n",
        "                        keywords = self.extract_keywords_from_topic(topic_name, stop_words)\n",
        "\n",
        "                        for keyword in keywords:\n",
        "                            weighted_topics.append((keyword, importance))\n",
        "\n",
        "        # Sort by importance and remove duplicates while preserving weights\n",
        "        topic_weights = {}\n",
        "        for keyword, importance in weighted_topics:\n",
        "            if keyword in topic_weights:\n",
        "                topic_weights[keyword] += importance\n",
        "            else:\n",
        "                topic_weights[keyword] = importance\n",
        "\n",
        "        # Sort by weight and select top topics\n",
        "        sorted_topics = sorted(topic_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Create document with weighted representation\n",
        "        # Instead of repeating words, we'll include them based on importance tiers\n",
        "        document_words = []\n",
        "\n",
        "        # Tier 1: Top 20% - include multiple times but not excessively\n",
        "        tier1_count = max(1, len(sorted_topics) // 5)\n",
        "        for keyword, weight in sorted_topics[:tier1_count]:\n",
        "            # Include 2-3 times for top tier\n",
        "            repeat_count = min(3, max(1, int(weight / max(1, sorted_topics[0][1]) * 3)))\n",
        "            document_words.extend([keyword] * repeat_count)\n",
        "\n",
        "        # Tier 2: Next 30% - include once or twice\n",
        "        tier2_start = tier1_count\n",
        "        tier2_end = tier2_start + max(1, len(sorted_topics) * 3 // 10)\n",
        "        for keyword, weight in sorted_topics[tier2_start:tier2_end]:\n",
        "            repeat_count = min(2, max(1, int(weight / max(1, sorted_topics[0][1]) * 2)))\n",
        "            document_words.extend([keyword] * repeat_count)\n",
        "\n",
        "        # Tier 3: Remaining - include once\n",
        "        tier3_start = tier2_end\n",
        "        for keyword, weight in sorted_topics[tier3_start:tier3_start + 20]:  # Limit total\n",
        "            document_words.append(keyword)\n",
        "\n",
        "        # Shuffle to avoid patterns and create more natural distribution\n",
        "        import random\n",
        "        random.seed(42)  # For reproducibility\n",
        "        random.shuffle(document_words)\n",
        "\n",
        "        # Join with commas for better readability and structure\n",
        "        document = ', '.join(document_words)\n",
        "\n",
        "        # Fallback if document is too short\n",
        "        if len(document.split()) < 5:\n",
        "            fallback_keywords = []\n",
        "            for _, paper in researcher_papers.iterrows():\n",
        "                topics = self.parse_curated_topics(paper['curated_topics'])\n",
        "                for topic in topics:\n",
        "                    if topic.get('type') == 'topic':\n",
        "                        topic_name = topic.get('name', '').strip()\n",
        "                        if topic_name:\n",
        "                            keywords = self.extract_keywords_from_topic(topic_name, stop_words)\n",
        "                            fallback_keywords.extend(keywords)\n",
        "\n",
        "            if fallback_keywords:\n",
        "                # Remove duplicates while preserving order\n",
        "                unique_keywords = list(dict.fromkeys(fallback_keywords))\n",
        "                document = ', '.join(unique_keywords[:20])\n",
        "\n",
        "        return document\n",
        "\n",
        "    def create_proxy_abstract(self, topics_str: str) -> str:\n",
        "        \"\"\"\n",
        "        Create proxy abstract from high-quality topics when abstract is missing.\n",
        "        Uses type: 'topic', 'related_topic', 'concept' with scores > min_proxy_score.\n",
        "        \"\"\"\n",
        "        topics = self.parse_curated_topics(topics_str)\n",
        "        if not topics:\n",
        "            return \"\"\n",
        "\n",
        "        relevant_topics = []\n",
        "        for topic in topics:\n",
        "            topic_type = topic.get('type', '')\n",
        "            topic_score = topic.get('score', 0)\n",
        "            topic_name = topic.get('name', '').strip()\n",
        "\n",
        "            if (topic_type in ['topic', 'related_topic', 'concept'] and\n",
        "                topic_score >= self.min_proxy_score and topic_name):\n",
        "                relevant_topics.append(topic_name)\n",
        "\n",
        "        return '. '.join(relevant_topics)\n",
        "\n",
        "    def create_conceptual_profiles(self, df: pd.DataFrame) -> Tuple[Dict[str, np.ndarray], Dict[str, int]]:\n",
        "        \"\"\"\n",
        "        Create conceptual profiles using sentence transformers.\n",
        "        Handles missing abstracts with proxy abstracts.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Loading sentence transformer model...\")\n",
        "        if self.sentence_model is None:\n",
        "            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        self.logger.info(\"Creating conceptual profiles...\")\n",
        "\n",
        "        # Track statistics\n",
        "        missing_abstracts = 0\n",
        "        total_papers = len(df)\n",
        "\n",
        "        texts_to_embed = []\n",
        "        work_ids = []\n",
        "\n",
        "        for _, paper in df.iterrows():\n",
        "            title = paper['work_title'] if pd.notna(paper['work_title']) else \"\"\n",
        "            abstract = paper['abstract'] if pd.notna(paper['abstract']) else \"\"\n",
        "\n",
        "            if abstract.strip():\n",
        "                # Use title + abstract\n",
        "                text_to_embed = f\"{title}. {abstract}\"\n",
        "            else:\n",
        "                # Create proxy abstract\n",
        "                missing_abstracts += 1\n",
        "                proxy_abstract = self.create_proxy_abstract(paper['curated_topics'])\n",
        "                text_to_embed = f\"{title}. {proxy_abstract}\" if proxy_abstract else title\n",
        "\n",
        "            texts_to_embed.append(text_to_embed)\n",
        "            work_ids.append(paper['work_id'])\n",
        "\n",
        "        self.logger.info(f\"Missing abstracts: {missing_abstracts}/{total_papers} ({missing_abstracts/total_papers*100:.1f}%)\")\n",
        "\n",
        "        # Generate embeddings\n",
        "        self.logger.info(\"Generating embeddings...\")\n",
        "        embeddings = self.sentence_model.encode(texts_to_embed, show_progress_bar=True)\n",
        "\n",
        "        # Create mapping from work_id to embedding\n",
        "        conceptual_profiles = {}\n",
        "        work_id_to_index = {}\n",
        "        for i, work_id in enumerate(work_ids):\n",
        "            conceptual_profiles[work_id] = embeddings[i]\n",
        "            work_id_to_index[work_id] = i\n",
        "\n",
        "        return conceptual_profiles, work_id_to_index\n",
        "\n",
        "    def create_grant_experience_factors(self, df: pd.DataFrame, grants_df: pd.DataFrame) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calculate grant experience factors: F_ge = (pi_7_years × 1.0) + (copi_7_years × 0.5)\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Creating grant experience factors...\")\n",
        "\n",
        "        # Get unique researchers\n",
        "        researchers = df[['researcher_name', 'researcher_openalex_id']].drop_duplicates()\n",
        "\n",
        "        # Create mapping from openalex_id to grant experience\n",
        "        grant_experience = {}\n",
        "        for _, researcher in researchers.iterrows():\n",
        "            openalex_id = researcher['researcher_openalex_id']\n",
        "\n",
        "            # Find matching grant record\n",
        "            grant_record = grants_df[grants_df['openalex_id'] == openalex_id]\n",
        "\n",
        "            if not grant_record.empty:\n",
        "                pi_grants = grant_record.iloc[0]['pi_7_years'] if pd.notna(grant_record.iloc[0]['pi_7_years']) else 0\n",
        "                copi_grants = grant_record.iloc[0]['copi_7_years'] if pd.notna(grant_record.iloc[0]['copi_7_years']) else 0\n",
        "                f_ge = (pi_grants * 1.0) + (copi_grants * 0.5)\n",
        "            else:\n",
        "                f_ge = 0.0  # No grant history\n",
        "\n",
        "            grant_experience[openalex_id] = f_ge\n",
        "\n",
        "        self.logger.info(f\"Created grant experience factors for {len(grant_experience)} researchers\")\n",
        "        return grant_experience\n",
        "\n",
        "    def create_evidence_index(self, df: pd.DataFrame) -> Dict[str, Dict[str, List[str]]]:\n",
        "        \"\"\"\n",
        "        Create evidence index mapping: researcher_id -> {topic_name: [work_ids]}\n",
        "        Only uses topics with type: 'topic'.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Creating evidence index...\")\n",
        "\n",
        "        evidence_index = {}\n",
        "\n",
        "        for _, paper in df.iterrows():\n",
        "            researcher_id = paper['researcher_openalex_id']\n",
        "            work_id = paper['work_id']\n",
        "            topics = self.parse_curated_topics(paper['curated_topics'])\n",
        "\n",
        "            if researcher_id not in evidence_index:\n",
        "                evidence_index[researcher_id] = {}\n",
        "\n",
        "            for topic in topics:\n",
        "                if topic.get('type') == 'topic':\n",
        "                    topic_name = topic.get('name', '').strip()\n",
        "                    if topic_name:\n",
        "                        if topic_name not in evidence_index[researcher_id]:\n",
        "                            evidence_index[researcher_id][topic_name] = []\n",
        "                        evidence_index[researcher_id][topic_name].append(work_id)\n",
        "\n",
        "        self.logger.info(f\"Created evidence index for {len(evidence_index)} researchers\")\n",
        "        return evidence_index\n",
        "\n",
        "    def comma_tokenizer(self, text: str) -> List[str]:\n",
        "        \"\"\"Custom tokenizer that splits on commas and cleans tokens\"\"\"\n",
        "        return [token.strip() for token in text.split(',') if token.strip()]\n",
        "\n",
        "    def create_tfidf_profiles(self, df: pd.DataFrame) -> Tuple[Dict[str, np.ndarray], TfidfVectorizer]:\n",
        "        \"\"\"\n",
        "        Create TF-IDF profiles from weighted expertise documents.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Creating TF-IDF profiles...\")\n",
        "\n",
        "        # Get unique researchers\n",
        "        researchers = df[['researcher_name', 'researcher_openalex_id']].drop_duplicates()\n",
        "\n",
        "        # Create weighted expertise documents\n",
        "        expertise_documents = []\n",
        "        researcher_ids = []\n",
        "        empty_docs = 0\n",
        "\n",
        "        for _, researcher in researchers.iterrows():\n",
        "            researcher_id = researcher['researcher_openalex_id']\n",
        "            researcher_papers = df[df['researcher_openalex_id'] == researcher_id]\n",
        "            expertise_doc = self.create_weighted_expertise_document(researcher_papers)\n",
        "\n",
        "            # Check if document is empty or too short\n",
        "            if not expertise_doc.strip() or len(expertise_doc.split()) < 2:\n",
        "                empty_docs += 1\n",
        "                # Create a minimal document from researcher's top topics\n",
        "                all_topics = []\n",
        "                for _, paper in researcher_papers.iterrows():\n",
        "                    topics = self.parse_curated_topics(paper['curated_topics'])\n",
        "                    for topic in topics:\n",
        "                        if topic.get('type') == 'topic':\n",
        "                            topic_name = topic.get('name', '').strip()\n",
        "                            if topic_name:\n",
        "                                all_topics.append(topic_name)\n",
        "\n",
        "                # Use unique topics as fallback\n",
        "                expertise_doc = ', '.join(list(set(all_topics))[:10]) if all_topics else \"research, unknown\"\n",
        "\n",
        "            expertise_documents.append(expertise_doc)\n",
        "            researcher_ids.append(researcher_id)\n",
        "\n",
        "        self.logger.info(f\"Created {len(expertise_documents)} documents, {empty_docs} required fallback\")\n",
        "\n",
        "        # Fit TF-IDF model with appropriate parameters for comma-separated format\n",
        "        self.logger.info(\"Fitting TF-IDF model...\")\n",
        "        self.tfidf_model = TfidfVectorizer(\n",
        "            max_features=5000,\n",
        "            ngram_range=(1, 2),  # Include both unigrams and bigrams\n",
        "            min_df=2,  # Require at least 2 documents\n",
        "            max_df=0.9,\n",
        "            stop_words='english',\n",
        "            lowercase=True,\n",
        "            token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]*\\b',  # Handle underscore-separated phrases\n",
        "            tokenizer=self.comma_tokenizer  # Use the class method instead of lambda\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            tfidf_matrix = self.tfidf_model.fit_transform(expertise_documents)\n",
        "        except ValueError as e:\n",
        "            self.logger.warning(f\"TF-IDF fitting failed: {e}\")\n",
        "            # Fallback: more lenient parameters\n",
        "            self.tfidf_model = TfidfVectorizer(\n",
        "                max_features=2000,\n",
        "                ngram_range=(1, 1),\n",
        "                min_df=1,\n",
        "                max_df=0.95,\n",
        "                stop_words='english',\n",
        "                lowercase=True,\n",
        "                tokenizer=self.comma_tokenizer  # Use the class method instead of lambda\n",
        "            )\n",
        "            tfidf_matrix = self.tfidf_model.fit_transform(expertise_documents)\n",
        "\n",
        "        # Debug: Print sample documents\n",
        "        for i, (doc, researcher_id) in enumerate(zip(expertise_documents[:3], researcher_ids[:3])):\n",
        "            print(f\"\\nDEBUG - Sample document {i}:\")\n",
        "            print(f\"Researcher: {researcher_id}\")\n",
        "            print(f\"Document: {doc[:200]}...\")\n",
        "            print(f\"Unique words: {len(set(doc.split()))}\")\n",
        "            print(f\"First 10 words: {doc.split()[:10]}\")\n",
        "\n",
        "        # Create mapping from researcher_id to TF-IDF vector\n",
        "        researcher_vectors = {}\n",
        "        for i, researcher_id in enumerate(researcher_ids):\n",
        "            researcher_vectors[researcher_id] = tfidf_matrix[i].toarray().flatten()\n",
        "\n",
        "        self.logger.info(f\"Created TF-IDF profiles for {len(researcher_vectors)} researchers\")\n",
        "        self.logger.info(f\"TF-IDF vocabulary size: {len(self.tfidf_model.get_feature_names_out())}\")\n",
        "\n",
        "        return researcher_vectors, self.tfidf_model\n",
        "\n",
        "    def create_researcher_metadata(self, df: pd.DataFrame, grant_experience: Dict[str, float]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create researcher metadata dataframe with summary statistics.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Creating researcher metadata...\")\n",
        "\n",
        "        # Aggregate researcher-level statistics\n",
        "        researcher_stats = df.groupby(['researcher_name', 'researcher_openalex_id']).agg({\n",
        "            'work_id': 'count',\n",
        "            'cited_by_count': ['sum', 'mean', 'max'],\n",
        "            'publication_year': ['min', 'max']\n",
        "        }).reset_index()\n",
        "\n",
        "        # Flatten column names\n",
        "        researcher_stats.columns = ['researcher_name', 'researcher_openalex_id',\n",
        "                                   'total_papers', 'total_citations', 'avg_citations',\n",
        "                                   'max_citations', 'first_publication_year', 'last_publication_year']\n",
        "\n",
        "        # Add grant experience factors\n",
        "        researcher_stats['grant_experience_factor'] = researcher_stats['researcher_openalex_id'].map(grant_experience).fillna(0.0)\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        researcher_stats['years_active'] = researcher_stats['last_publication_year'] - researcher_stats['first_publication_year'] + 1\n",
        "        researcher_stats['recent_activity'] = researcher_stats['last_publication_year'] >= (self.current_year - 5)\n",
        "\n",
        "        return researcher_stats\n",
        "\n",
        "    def save_processed_data(self, output_dir: str, researcher_vectors: Dict[str, np.ndarray],\n",
        "                          conceptual_profiles: Dict[str, np.ndarray],\n",
        "                          work_id_to_index: Dict[str, int],\n",
        "                          evidence_index: Dict[str, Dict[str, List[str]]],\n",
        "                          researcher_metadata: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Save all processed data to specified directory structure.\n",
        "        \"\"\"\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.logger.info(f\"Saving processed data to {output_path}\")\n",
        "\n",
        "        # Save TF-IDF model\n",
        "        with open(output_path / 'tfidf_model.pkl', 'wb') as f:\n",
        "            pickle.dump(self.tfidf_model, f)\n",
        "\n",
        "        # Save researcher TF-IDF vectors\n",
        "        researcher_ids = list(researcher_vectors.keys())\n",
        "        vectors_array = np.array([researcher_vectors[rid] for rid in researcher_ids])\n",
        "\n",
        "        np.savez_compressed(\n",
        "            output_path / 'researcher_vectors.npz',\n",
        "            vectors=vectors_array,\n",
        "            researcher_ids=researcher_ids\n",
        "        )\n",
        "\n",
        "        # Save conceptual profiles (paper embeddings)\n",
        "        work_ids = list(conceptual_profiles.keys())\n",
        "        embeddings_array = np.array([conceptual_profiles[wid] for wid in work_ids])\n",
        "\n",
        "        np.savez_compressed(\n",
        "            output_path / 'conceptual_profiles.npz',\n",
        "            embeddings=embeddings_array,\n",
        "            work_ids=work_ids,\n",
        "            work_id_to_index=work_id_to_index\n",
        "        )\n",
        "\n",
        "        # Save evidence index\n",
        "        with open(output_path / 'evidence_index.json', 'w') as f:\n",
        "            json.dump(evidence_index, f, indent=2)\n",
        "\n",
        "        # Save researcher metadata\n",
        "        researcher_metadata.to_parquet(output_path / 'researcher_metadata.parquet', index=False)\n",
        "\n",
        "        # Save processing log\n",
        "        processing_log = {\n",
        "            'processing_timestamp': datetime.now().isoformat(),\n",
        "            'current_year': self.current_year,\n",
        "            'min_proxy_score': self.min_proxy_score,\n",
        "            'total_researchers': len(researcher_vectors),\n",
        "            'total_papers': len(conceptual_profiles),\n",
        "            'tfidf_features': len(self.tfidf_model.get_feature_names_out()) if self.tfidf_model else 0,\n",
        "            'sentence_model': self.sentence_model.get_sentence_embedding_dimension() if self.sentence_model else 0,\n",
        "            'files_created': [\n",
        "                'tfidf_model.pkl',\n",
        "                'researcher_vectors.npz',\n",
        "                'conceptual_profiles.npz',\n",
        "                'evidence_index.json',\n",
        "                'researcher_metadata.parquet',\n",
        "                'processing_log.json'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(output_path / 'processing_log.json', 'w') as f:\n",
        "            json.dump(processing_log, f, indent=2)\n",
        "\n",
        "        self.logger.info(\"All data saved successfully!\")\n",
        "        return processing_log\n",
        "\n",
        "    def process_all(self, researcher_file: str, grants_file: str, output_dir: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Main pipeline: process all data and save results.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Starting complete data preprocessing pipeline...\")\n",
        "\n",
        "        # Load data\n",
        "        df, grants_df = self.load_data(researcher_file, grants_file)\n",
        "\n",
        "        # Create all components\n",
        "        conceptual_profiles, work_id_to_index = self.create_conceptual_profiles(df)\n",
        "        grant_experience = self.create_grant_experience_factors(df, grants_df)\n",
        "        evidence_index = self.create_evidence_index(df)\n",
        "        researcher_vectors, tfidf_model = self.create_tfidf_profiles(df)\n",
        "        researcher_metadata = self.create_researcher_metadata(df, grant_experience)\n",
        "\n",
        "        # Save everything\n",
        "        processing_log = self.save_processed_data(\n",
        "            output_dir, researcher_vectors, conceptual_profiles,\n",
        "            work_id_to_index, evidence_index, researcher_metadata\n",
        "        )\n",
        "\n",
        "        self.logger.info(\"Pipeline completed successfully!\")\n",
        "        return processing_log\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize processor\n",
        "    processor = ResearcherProfileProcessor(current_year=2025, min_proxy_score=0.8)\n",
        "\n",
        "    # File paths\n",
        "    researcher_file = '/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_and_abstracts_robust_20250703_191556.csv'\n",
        "    grants_file = '/content/drive/MyDrive/datastore/v2_data/texas_state_researchers_final_analysis.csv'\n",
        "    output_dir = '/content/drive/MyDrive/datastore/v2_DATA'\n",
        "\n",
        "    # Run complete pipeline\n",
        "    log = processor.process_all(researcher_file, grants_file, output_dir)\n",
        "    print(\"Processing completed!\")\n",
        "    print(f\"Created profiles for {log['total_researchers']} researchers\")\n",
        "    print(f\"Processed {log['total_papers']} papers\")\n",
        "    print(f\"Output saved to: {output_dir}\")"
      ],
      "metadata": {
        "id": "gsWoX-LUIkz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diagnostic Tests"
      ],
      "metadata": {
        "id": "RraFQ9xdEEjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show me what the curated_topics actually look like\n",
        "df = pd.read_csv('/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_and_abstracts_robust_20250703_191556.csv')\n",
        "print(\"Sample curated_topics:\")\n",
        "print(df['curated_topics'].iloc[0])\n",
        "print(\"\\nFirst few work_ids:\")\n",
        "print(df['work_id'].head())"
      ],
      "metadata": {
        "id": "1tyNedpnA9Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test weighted document creation for one researcher\n",
        "processor = ResearcherProfileProcessor()\n",
        "researcher_papers = df[df['researcher_openalex_id'] == df['researcher_openalex_id'].iloc[0]]\n",
        "doc = processor.create_weighted_expertise_document(researcher_papers)\n",
        "print(f\"Document length: {len(doc)}\")\n",
        "print(f\"Document: {doc[:200]}...\")"
      ],
      "metadata": {
        "id": "GeNHGgQABI6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if topics parsing works\n",
        "import json\n",
        "sample_topics = df['curated_topics'].iloc[0]\n",
        "try:\n",
        "    parsed = json.loads(sample_topics)\n",
        "    print(f\"Parsed topics count: {len(parsed)}\")\n",
        "    print(\"First topic:\", parsed[0] if parsed else \"None\")\n",
        "\n",
        "    # Check for type: 'topic'\n",
        "    topic_types = [t.get('type') for t in parsed if isinstance(t, dict)]\n",
        "    print(f\"Topic types: {set(topic_types)}\")\n",
        "except:\n",
        "    print(\"JSON parsing failed\")"
      ],
      "metadata": {
        "id": "qvTwzQPcBEec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_and_abstracts_robust_20250703_191556.csv')\n",
        "\n",
        "print(\"=== JSON PARSING TEST ===\")\n",
        "failed_count = 0\n",
        "valid_count = 0\n",
        "valid_topics_count = 0\n",
        "\n",
        "for i in range(min(50, len(df))):  # Test first 50 rows\n",
        "    topics_str = df['curated_topics'].iloc[i]\n",
        "    try:\n",
        "        parsed = json.loads(topics_str)\n",
        "        valid_count += 1\n",
        "\n",
        "        # Count topics with type='topic'\n",
        "        topic_types = [t.get('type') for t in parsed if isinstance(t, dict)]\n",
        "        if 'topic' in topic_types:\n",
        "            valid_topics_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        failed_count += 1\n",
        "        if failed_count <= 3:  # Show first 3 failures\n",
        "            print(f\"Row {i} failed: {str(e)[:100]}\")\n",
        "\n",
        "print(f\"Valid JSON: {valid_count}\")\n",
        "print(f\"Failed JSON: {failed_count}\")\n",
        "print(f\"Rows with 'topic' type: {valid_topics_count}\")"
      ],
      "metadata": {
        "id": "mloLbwfREMel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fixed parser\n",
        "import ast\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def parse_curated_topics_fixed(topics_str):\n",
        "    \"\"\"Fixed parser that handles Python dict strings\"\"\"\n",
        "    if pd.isna(topics_str) or not topics_str.strip():\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # Try JSON first\n",
        "        topics = json.loads(topics_str)\n",
        "        return topics if isinstance(topics, list) else []\n",
        "    except:\n",
        "        try:\n",
        "            # Try ast.literal_eval for Python dict strings\n",
        "            topics = ast.literal_eval(topics_str)\n",
        "            return topics if isinstance(topics, list) else []\n",
        "        except:\n",
        "            try:\n",
        "                # Fix single quotes and try JSON\n",
        "                fixed_json = topics_str.replace(\"'\", '\"')\n",
        "                topics = json.loads(fixed_json)\n",
        "                return topics if isinstance(topics, list) else []\n",
        "            except:\n",
        "                return []\n",
        "\n",
        "# Test with your data\n",
        "df = pd.read_csv('/content/drive/MyDrive/datastore/v2_data/researcher_works_with_topics_and_abstracts_robust_20250703_191556.csv')\n",
        "\n",
        "print(\"=== TESTING FIXED PARSER ===\")\n",
        "success_count = 0\n",
        "topic_count = 0\n",
        "\n",
        "for i in range(min(10, len(df))):\n",
        "    topics_str = df['curated_topics'].iloc[i]\n",
        "    parsed = parse_curated_topics_fixed(topics_str)\n",
        "\n",
        "    if parsed:\n",
        "        success_count += 1\n",
        "        # Count topics with type='topic'\n",
        "        main_topics = [t for t in parsed if t.get('type') == 'topic']\n",
        "        topic_count += len(main_topics)\n",
        "        print(f\"Row {i}: {len(parsed)} total topics, {len(main_topics)} main topics\")\n",
        "\n",
        "print(f\"\\nSuccess rate: {success_count}/10\")\n",
        "print(f\"Total main topics found: {topic_count}\")"
      ],
      "metadata": {
        "id": "DgbMQUIXESOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== PUBLICATION YEAR ANALYSIS ===\")\n",
        "years = df['publication_year'].dropna()\n",
        "print(f\"Year range: {years.min()} to {years.max()}\")\n",
        "print(f\"Years distribution:\")\n",
        "print(years.value_counts().sort_index().tail(10))\n",
        "\n",
        "# Test recency weights\n",
        "current_year = 2025\n",
        "for year in [2000, 2010, 2015, 2020, 2023, 2024, 2025]:\n",
        "    weight = max(0.1, 1 - (current_year - year) / 15)\n",
        "    print(f\"Year {year}: recency weight = {weight:.3f}\")"
      ],
      "metadata": {
        "id": "tnLaGAc9EXC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solicilitation Parser"
      ],
      "metadata": {
        "id": "zz-hpwf6EHdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "QRzHKR8vpAxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "o83FGLDRpMSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Enhanced Intelligent Solicitation Processor\n",
        "# ==============================================================================\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from google.colab import drive, userdata\n",
        "from transformers import pipeline\n",
        "import anthropic\n",
        "from dataclasses import dataclass, asdict\n",
        "import warnings\n",
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class EnhancedSolicitationObject:\n",
        "    \"\"\"\n",
        "    Enhanced structured object containing comprehensive solicitation analysis.\n",
        "    \"\"\"\n",
        "    # Basic Metadata\n",
        "    solicitation_id: str\n",
        "    title: str\n",
        "    abstract: str\n",
        "    processed_at: str\n",
        "    pdf_filename: str\n",
        "\n",
        "    # Detailed Structured Information\n",
        "    award_info: Dict[str, Any]  # funding_ceiling, award_type, duration, etc.\n",
        "    eligibility: Dict[str, Any]  # pi_requirements, institutional_limits, etc.\n",
        "    submission_requirements: Dict[str, Any]  # deadlines, format_rules, etc.\n",
        "    program_objectives: List[str]  # scientific goals and priorities\n",
        "    contact_information: Dict[str, str]  # agency contacts\n",
        "\n",
        "    # Skills Analysis (backward compatible)\n",
        "    narrative_skills: List[str]  # From Claude targeted extraction\n",
        "    formal_topics: List[Dict]    # From OpenAlex classifier\n",
        "    required_skills_checklist: List[str]  # Final combined skills\n",
        "\n",
        "    # Processing Metadata\n",
        "    text_length: int\n",
        "    sections_found: List[str]\n",
        "    extraction_confidence: Dict[str, float]\n",
        "    processing_method: str = \"intelligent_deconstruction\"\n",
        "    full_text_context: str = \"\"  # Complete text for context\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self, filepath: str):\n",
        "        \"\"\"Save to JSON file.\"\"\"\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "class IntelligentSolicitationProcessor:\n",
        "    \"\"\"\n",
        "    Enhanced processor that intelligently deconstructs solicitations into structured sections\n",
        "    and extracts comprehensive information using targeted prompts.\n",
        "    \"\"\"\n",
        "\n",
        "    # Section detection patterns for different solicitation types\n",
        "    SECTION_PATTERNS = {\n",
        "        'award_info': [\n",
        "            r'Section\\s+II[:\\s]*Award\\s+Information',\n",
        "            r'Award\\s+Information',\n",
        "            r'Funding\\s+Information',\n",
        "            r'Budget\\s+Information',\n",
        "            r'Financial\\s+Information',\n",
        "            r'Award\\s+Details'\n",
        "        ],\n",
        "        'eligibility': [\n",
        "            r'Section\\s+III[:\\s]*Eligibility',\n",
        "            r'Section\\s+IV[:\\s]*Eligibility',\n",
        "            r'Eligibility\\s+Requirements',\n",
        "            r'Eligibility\\s+Information',\n",
        "            r'Who\\s+May\\s+Apply',\n",
        "            r'Applicant\\s+Eligibility',\n",
        "            r'PI\\s+Eligibility'\n",
        "        ],\n",
        "        'program_description': [\n",
        "            r'Program\\s+Description',\n",
        "            r'Scientific\\s+Objectives',\n",
        "            r'Research\\s+Areas',\n",
        "            r'Technical\\s+Areas',\n",
        "            r'Program\\s+Objectives',\n",
        "            r'Research\\s+Goals',\n",
        "            r'Scientific\\s+Scope',\n",
        "            r'Program\\s+Overview'\n",
        "        ],\n",
        "        'submission_requirements': [\n",
        "            r'Section\\s+V[:\\s]*Proposal.*Submission',\n",
        "            r'Section\\s+IV[:\\s]*Application.*Submission',\n",
        "            r'Application\\s+Instructions',\n",
        "            r'Submission\\s+Instructions',\n",
        "            r'Proposal\\s+Requirements',\n",
        "            r'Application\\s+Requirements',\n",
        "            r'Submission\\s+Guidelines'\n",
        "        ],\n",
        "        'contact_info': [\n",
        "            r'Section\\s+VIII[:\\s]*Agency\\s+Contacts',\n",
        "            r'Agency\\s+Contacts',\n",
        "            r'Contact\\s+Information',\n",
        "            r'Program\\s+Contacts',\n",
        "            r'For\\s+More\\s+Information'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.claude_client = None\n",
        "        self.topic_classifier = None\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"Initialize Claude API client and OpenAlex topic classifier.\"\"\"\n",
        "        logger.info(\"Setting up models...\")\n",
        "        try:\n",
        "            api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "            self.claude_client = anthropic.Anthropic(api_key=api_key)\n",
        "            logger.info(\"✅ Claude API client initialized\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Claude API setup failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            logger.info(\"Loading OpenAlex topic classifier...\")\n",
        "            self.topic_classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=\"OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract\"\n",
        "            )\n",
        "            logger.info(\"✅ OpenAlex topic classifier loaded\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Topic classifier setup failed: {e}\")\n",
        "\n",
        "    def _extract_and_clean_text(self, filepath: str) -> Tuple[str, str, str, str]:\n",
        "        \"\"\"Enhanced text extraction with better cleaning and formatting.\"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
        "\n",
        "        filename = os.path.basename(filepath)\n",
        "        logger.info(f\"📄 Processing: {filename}\")\n",
        "\n",
        "        doc = None\n",
        "        try:\n",
        "            doc = fitz.open(filepath)\n",
        "            page_count = len(doc)\n",
        "            full_text = \"\"\n",
        "\n",
        "            logger.info(f\"📖 PDF has {page_count} pages\")\n",
        "\n",
        "            for page_num in range(page_count):\n",
        "                try:\n",
        "                    page = doc[page_num]\n",
        "                    text = page.get_text()\n",
        "\n",
        "                    # Basic cleaning\n",
        "                    text = self._clean_extracted_text(text)\n",
        "                    full_text += f\"\\n--- PAGE {page_num + 1} ---\\n{text}\\n\"\n",
        "\n",
        "                except Exception as page_error:\n",
        "                    logger.warning(f\"⚠️ Error processing page {page_num + 1}: {page_error}\")\n",
        "                    continue\n",
        "\n",
        "            if not full_text.strip():\n",
        "                raise ValueError(\"Extracted text is empty. PDF might be image-based.\")\n",
        "\n",
        "            title, abstract = self._extract_title_and_abstract(full_text, filename)\n",
        "            cleaned_text = self._final_text_cleaning(full_text)\n",
        "\n",
        "            logger.info(f\"✅ Extracted {len(cleaned_text)} characters from {page_count} pages\")\n",
        "            return filename, title, abstract, cleaned_text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Error extracting text from PDF '{filename}': {e}\")\n",
        "            raise\n",
        "        finally:\n",
        "            # Ensure document is properly closed\n",
        "            if doc is not None:\n",
        "                try:\n",
        "                    doc.close()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    def _final_text_cleaning(self, text: str) -> str:\n",
        "        \"\"\"Final pass text cleaning for analysis.\"\"\"\n",
        "        # Remove page markers\n",
        "        text = re.sub(r'--- PAGE \\d+ ---', '', text)\n",
        "\n",
        "        # Normalize spacing\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "        text = re.sub(r' {2,}', ' ', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize extracted text.\"\"\"\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "\n",
        "        # Remove page headers/footers patterns\n",
        "        text = re.sub(r'\\n\\s*Page\\s+\\d+\\s*of\\s+\\d+\\s*\\n', '\\n', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)\n",
        "\n",
        "        # Fix common OCR issues\n",
        "        text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # Add space between words\n",
        "        text = re.sub(r'(\\w)(Section\\s+[IVX]+)', r'\\1\\n\\2', text)  # Break before sections\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _simple_pdf_extraction(self, filepath: str) -> Tuple[str, str, str, str]:\n",
        "        \"\"\"Simple fallback PDF extraction method.\"\"\"\n",
        "        filename = os.path.basename(filepath)\n",
        "        logger.info(f\"🔄 Trying simple extraction for: {filename}\")\n",
        "\n",
        "        doc = None\n",
        "        try:\n",
        "            doc = fitz.open(filepath)\n",
        "\n",
        "            # Extract all text at once\n",
        "            full_text = \"\"\n",
        "            for page in doc:\n",
        "                full_text += page.get_text() + \"\\n\"\n",
        "\n",
        "            if not full_text.strip():\n",
        "                raise ValueError(\"No text extracted from PDF\")\n",
        "\n",
        "            # Basic title and abstract extraction\n",
        "            lines = [line.strip() for line in full_text.split('\\n') if line.strip()]\n",
        "            title = filename.replace('.pdf', '').replace('_', ' ')\n",
        "            abstract = ' '.join(lines[:10]) if lines else \"\"\n",
        "\n",
        "            logger.info(f\"✅ Simple extraction successful: {len(full_text)} characters\")\n",
        "            return filename, title, abstract, full_text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Simple extraction failed: {e}\")\n",
        "            raise\n",
        "        finally:\n",
        "            if doc is not None:\n",
        "                try:\n",
        "                    doc.close()\n",
        "                except:\n",
        "                    pass\n",
        "        \"\"\"Final pass text cleaning for analysis.\"\"\"\n",
        "        # Remove page markers\n",
        "        text = re.sub(r'--- PAGE \\d+ ---', '', text)\n",
        "\n",
        "        # Normalize spacing\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "        text = re.sub(r' {2,}', ' ', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _extract_title_and_abstract(self, full_text: str, filename: str) -> Tuple[str, str]:\n",
        "        \"\"\"Enhanced title and abstract extraction.\"\"\"\n",
        "        lines = [line.strip() for line in full_text.split('\\n') if line.strip()]\n",
        "\n",
        "        # Title extraction\n",
        "        title = filename.replace('.pdf', '').replace('_', ' ').replace('-', ' ')\n",
        "\n",
        "        # Look for title in first 20 lines\n",
        "        for i, line in enumerate(lines[:20]):\n",
        "            if (30 < len(line) < 300 and\n",
        "                not line.isupper() and\n",
        "                not re.match(r'^(Page|Section|\\d+)', line, re.IGNORECASE) and\n",
        "                not line.startswith('http')):\n",
        "                title = line\n",
        "                break\n",
        "\n",
        "        # Abstract extraction\n",
        "        abstract = \"\"\n",
        "        abstract_started = False\n",
        "        abstract_markers = ['abstract', 'summary', 'overview', 'program description']\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_lower = line.lower()\n",
        "\n",
        "            # Start abstract detection\n",
        "            if not abstract_started:\n",
        "                for marker in abstract_markers:\n",
        "                    if marker in line_lower and len(line) > len(marker) + 5:\n",
        "                        abstract_started = True\n",
        "                        # Extract text after marker\n",
        "                        marker_pos = line_lower.find(marker)\n",
        "                        remaining_text = line[marker_pos + len(marker):].strip()\n",
        "                        if remaining_text and not remaining_text.startswith(':'):\n",
        "                            abstract += remaining_text + \" \"\n",
        "                        break\n",
        "                continue\n",
        "\n",
        "            # Continue abstract collection\n",
        "            if abstract_started:\n",
        "                # Stop conditions\n",
        "                stop_conditions = [\n",
        "                    'table of contents', 'section i', 'section 1', 'introduction',\n",
        "                    'background', 'key dates', 'important dates'\n",
        "                ]\n",
        "\n",
        "                if any(stop in line_lower for stop in stop_conditions):\n",
        "                    break\n",
        "\n",
        "                abstract += line + \" \"\n",
        "\n",
        "                # Stop if abstract is getting too long\n",
        "                if len(abstract) > 2500:\n",
        "                    break\n",
        "\n",
        "        # Fallback for abstract\n",
        "        if not abstract.strip():\n",
        "            abstract = ' '.join(lines[1:8])  # Use first few lines\n",
        "\n",
        "        return title.strip(), abstract.strip()[:2000]\n",
        "\n",
        "    def _detect_sections(self, text: str) -> Dict[str, Tuple[int, int]]:\n",
        "        \"\"\"Detect and locate sections in the text.\"\"\"\n",
        "        sections_found = {}\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        for section_type, patterns in self.SECTION_PATTERNS.items():\n",
        "            for pattern in patterns:\n",
        "                for i, line in enumerate(lines):\n",
        "                    if re.search(pattern, line, re.IGNORECASE):\n",
        "                        # Find section end (next major section or end of text)\n",
        "                        end_line = len(lines)\n",
        "                        for j in range(i + 1, len(lines)):\n",
        "                            if re.search(r'Section\\s+[IVX]+[:\\s]', lines[j], re.IGNORECASE):\n",
        "                                end_line = j\n",
        "                                break\n",
        "\n",
        "                        sections_found[section_type] = (i, end_line)\n",
        "                        logger.info(f\"Found section: {section_type} (lines {i}-{end_line})\")\n",
        "                        break\n",
        "\n",
        "                if section_type in sections_found:\n",
        "                    break\n",
        "\n",
        "        return sections_found\n",
        "\n",
        "    def _extract_section_text(self, text: str, start_line: int, end_line: int) -> str:\n",
        "        \"\"\"Extract text from a specific section.\"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        section_lines = lines[start_line:end_line]\n",
        "        return '\\n'.join(section_lines).strip()\n",
        "\n",
        "    def _extract_award_information(self, section_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract award information using Claude API.\"\"\"\n",
        "        if not self.claude_client or not section_text.strip():\n",
        "            return {}\n",
        "\n",
        "        prompt = f\"\"\"Extract award information from this solicitation section. Return a JSON object with these fields:\n",
        "- funding_ceiling: Maximum funding amount (include currency and period if specified)\n",
        "- award_type: Type of award (e.g., R01, CAREER, SBIR, etc.)\n",
        "- project_duration: Maximum project length\n",
        "- number_of_awards: Expected number of awards to be made\n",
        "- cost_sharing: Any cost sharing requirements\n",
        "- award_mechanism: Funding mechanism details\n",
        "\n",
        "Section text:\n",
        "---\n",
        "{section_text}\n",
        "---\n",
        "\n",
        "Return only valid JSON. If information is not found, use null for that field.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=1000,\n",
        "                temperature=0.1,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.content[0].text.strip())\n",
        "            logger.info(\"✅ Extracted award information\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Award information extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_eligibility_requirements(self, section_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract eligibility requirements using Claude API.\"\"\"\n",
        "        if not self.claude_client or not section_text.strip():\n",
        "            return {}\n",
        "\n",
        "        prompt = f\"\"\"Extract eligibility requirements from this solicitation section. Return a JSON object with these fields:\n",
        "- pi_requirements: List of PI eligibility requirements\n",
        "- institutional_requirements: Institution eligibility requirements\n",
        "- limited_submission: Whether this is a limited submission opportunity\n",
        "- career_stage_restrictions: Any career stage limitations (e.g., early-career only)\n",
        "- collaboration_requirements: Required collaborations or partnerships\n",
        "- special_restrictions: Any other special eligibility restrictions\n",
        "\n",
        "Section text:\n",
        "---\n",
        "{section_text}\n",
        "---\n",
        "\n",
        "Return only valid JSON. Use empty arrays [] for missing list fields and null for missing single fields.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=1000,\n",
        "                temperature=0.1,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.content[0].text.strip())\n",
        "            logger.info(\"✅ Extracted eligibility requirements\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Eligibility extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_submission_requirements(self, section_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract submission requirements using Claude API.\"\"\"\n",
        "        if not self.claude_client or not section_text.strip():\n",
        "            return {}\n",
        "\n",
        "        prompt = f\"\"\"Extract submission requirements from this solicitation section. Return a JSON object with these fields:\n",
        "- submission_deadline: Application deadline(s)\n",
        "- page_limits: Page limits for different sections\n",
        "- format_requirements: File format requirements (PDF, font size, margins, etc.)\n",
        "- required_documents: List of required documents/sections\n",
        "- submission_method: How to submit (Grants.gov, etc.)\n",
        "- special_instructions: Any special submission instructions\n",
        "\n",
        "Section text:\n",
        "---\n",
        "{section_text}\n",
        "---\n",
        "\n",
        "Return only valid JSON. Use empty arrays [] for missing list fields and null for missing single fields.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=1000,\n",
        "                temperature=0.1,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.content[0].text.strip())\n",
        "            logger.info(\"✅ Extracted submission requirements\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Submission requirements extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_program_objectives_and_skills(self, section_text: str) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Extract program objectives and required skills using Claude API.\"\"\"\n",
        "        if not self.claude_client or not section_text.strip():\n",
        "            return [], []\n",
        "\n",
        "        prompt = f\"\"\"Analyze this program description section and extract two things:\n",
        "\n",
        "1. Program Objectives: The main scientific goals and research priorities\n",
        "2. Required Skills: Specific technical skills, domain expertise, and methodological knowledge needed\n",
        "\n",
        "Return a JSON object with:\n",
        "- program_objectives: Array of 3-7 main program goals/objectives\n",
        "- required_skills: Array of 5-10 specific skills/expertise areas needed\n",
        "\n",
        "Section text:\n",
        "---\n",
        "{section_text}\n",
        "---\n",
        "\n",
        "Return only valid JSON with arrays of strings.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=1500,\n",
        "                temperature=0.2,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.content[0].text.strip())\n",
        "            objectives = result.get('program_objectives', [])\n",
        "            skills = result.get('required_skills', [])\n",
        "\n",
        "            logger.info(f\"✅ Extracted {len(objectives)} objectives and {len(skills)} skills\")\n",
        "            return objectives, skills\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Program objectives extraction failed: {e}\")\n",
        "            return [], []\n",
        "\n",
        "    def _extract_contact_information(self, section_text: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract contact information using Claude API.\"\"\"\n",
        "        if not self.claude_client or not section_text.strip():\n",
        "            return {}\n",
        "\n",
        "        prompt = f\"\"\"Extract contact information from this solicitation section. Return a JSON object with:\n",
        "- program_officer: Name and contact of program officer\n",
        "- technical_contact: Technical questions contact\n",
        "- administrative_contact: Administrative questions contact\n",
        "- general_email: General program email\n",
        "- phone: Program phone number\n",
        "- website: Program website\n",
        "\n",
        "Section text:\n",
        "---\n",
        "{section_text}\n",
        "---\n",
        "\n",
        "Return only valid JSON. Use null for missing fields.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=800,\n",
        "                temperature=0.1,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.content[0].text.strip())\n",
        "            logger.info(\"✅ Extracted contact information\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Contact information extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_formal_topics_openalex(self, title: str, abstract: str) -> List[Dict]:\n",
        "        \"\"\"Extract formal topics using OpenAlex classifier (unchanged from original).\"\"\"\n",
        "        if not self.topic_classifier:\n",
        "            logger.warning(\"⚠️ Topic classifier not available, skipping formal topics.\")\n",
        "            return []\n",
        "\n",
        "        formatted_text = f\"<TITLE> {title}\\n<ABSTRACT> {abstract}\"\n",
        "        logger.info(\"🔬 Running OpenAlex topic classification...\")\n",
        "\n",
        "        try:\n",
        "            predictions = self.topic_classifier(formatted_text, top_k=10, truncation=True)\n",
        "\n",
        "            if not predictions:\n",
        "                logger.warning(\"OpenAlex model returned no valid predictions.\")\n",
        "                return []\n",
        "\n",
        "            formal_topics = []\n",
        "            for topic in predictions:\n",
        "                if isinstance(topic, dict) and 'label' in topic and 'score' in topic:\n",
        "                    if topic['score'] > 0.01:\n",
        "                        formal_topics.append({\n",
        "                            'topic': topic['label'],\n",
        "                            'score': round(topic['score'], 4)\n",
        "                        })\n",
        "\n",
        "            logger.info(f\"✅ Extracted {len(formal_topics)} formal topics from OpenAlex.\")\n",
        "            return formal_topics\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Topic classification failed: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def extract_keywords_from_skills(self, skills: List[str]) -> List[str]:\n",
        "        \"\"\"Extract keywords from solicitation skills using same logic as researcher topics.\"\"\"\n",
        "        import re\n",
        "\n",
        "        stop_words = {'and', 'in', 'of', 'for', 'the', 'a', 'an', 'to', 'with', 'on', 'at', 'by',\n",
        "                      'expertise', 'experience', 'knowledge', 'ability', 'skills', 'understanding',\n",
        "                      'capacity', 'proficiency', 'e.g.', 'eg'}\n",
        "\n",
        "        all_keywords = []\n",
        "\n",
        "        for skill in skills:\n",
        "            # Clean and split\n",
        "            cleaned = re.sub(r'[^\\w\\s-]', ' ', skill.lower())\n",
        "            words = cleaned.split()\n",
        "\n",
        "            # Extract meaningful keywords\n",
        "            for word in words:\n",
        "                word = word.strip('-')\n",
        "                if (len(word) >= 3 and\n",
        "                    word not in stop_words and\n",
        "                    not word.isdigit()):\n",
        "                    all_keywords.append(word)\n",
        "\n",
        "        return all_keywords\n",
        "\n",
        "    def _fusion_logic(self, narrative_skills: List[str], formal_topics: List[Dict]) -> List[str]:\n",
        "        \"\"\"Combine narrative skills and formal topics, removing duplicates.\"\"\"\n",
        "        logger.info(\"🔄 Applying fusion logic...\")\n",
        "        combined_skills = list(narrative_skills)\n",
        "        narrative_lower = ' '.join(narrative_skills).lower()\n",
        "\n",
        "        for topic in formal_topics:\n",
        "            topic_name = topic['topic'].split(': ', 1)[-1]\n",
        "            is_duplicate = topic_name.lower() in narrative_lower\n",
        "            if not is_duplicate:\n",
        "                combined_skills.append(f\"Expertise in {topic_name}\")\n",
        "\n",
        "        logger.info(f\"✅ Created final checklist with {len(combined_skills)} skills.\")\n",
        "        return combined_skills\n",
        "\n",
        "    def process_solicitation(self, pdf_filepath: str) -> Optional[EnhancedSolicitationObject]:\n",
        "        \"\"\"Main processing pipeline with intelligent deconstruction.\"\"\"\n",
        "        logger.info(\"🚀 Starting Enhanced Solicitation Processing Pipeline\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Extract and clean text - try enhanced method first, then fallback\n",
        "            try:\n",
        "                filename, title, abstract, full_text = self._extract_and_clean_text(pdf_filepath)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"⚠️ Enhanced extraction failed: {e}\")\n",
        "                logger.info(\"🔄 Attempting simple extraction...\")\n",
        "                filename, title, abstract, full_text = self._simple_pdf_extraction(pdf_filepath)\n",
        "\n",
        "            # Detect sections\n",
        "            sections_locations = self._detect_sections(full_text)\n",
        "            sections_found = list(sections_locations.keys())\n",
        "\n",
        "            logger.info(f\"📊 Detected {len(sections_found)} sections: {sections_found}\")\n",
        "\n",
        "            # Initialize structured data\n",
        "            award_info = {}\n",
        "            eligibility = {}\n",
        "            submission_requirements = {}\n",
        "            program_objectives = []\n",
        "            contact_info = {}\n",
        "            narrative_skills = []\n",
        "\n",
        "            # Extract information from each detected section\n",
        "            if 'award_info' in sections_locations:\n",
        "                start, end = sections_locations['award_info']\n",
        "                section_text = self._extract_section_text(full_text, start, end)\n",
        "                award_info = self._extract_award_information(section_text)\n",
        "\n",
        "            if 'eligibility' in sections_locations:\n",
        "                start, end = sections_locations['eligibility']\n",
        "                section_text = self._extract_section_text(full_text, start, end)\n",
        "                eligibility = self._extract_eligibility_requirements(section_text)\n",
        "\n",
        "            if 'submission_requirements' in sections_locations:\n",
        "                start, end = sections_locations['submission_requirements']\n",
        "                section_text = self._extract_section_text(full_text, start, end)\n",
        "                submission_requirements = self._extract_submission_requirements(section_text)\n",
        "\n",
        "            if 'program_description' in sections_locations:\n",
        "                start, end = sections_locations['program_description']\n",
        "                section_text = self._extract_section_text(full_text, start, end)\n",
        "                program_objectives, narrative_skills = self._extract_program_objectives_and_skills(section_text)\n",
        "\n",
        "            if 'contact_info' in sections_locations:\n",
        "                start, end = sections_locations['contact_info']\n",
        "                section_text = self._extract_section_text(full_text, start, end)\n",
        "                contact_info = self._extract_contact_information(section_text)\n",
        "\n",
        "            # Fallback: if no program description found, analyze full text for skills\n",
        "            if not narrative_skills:\n",
        "                logger.info(\"🔄 No program description found, analyzing full text for skills...\")\n",
        "                fallback_text = f\"Title: {title}. Abstract: {abstract}\"\n",
        "                if len(fallback_text.strip()) < 100:  # If abstract is too short, use more text\n",
        "                    fallback_text = full_text[:3000]  # Use first 3000 chars\n",
        "                _, narrative_skills = self._extract_program_objectives_and_skills(fallback_text)\n",
        "\n",
        "            # OpenAlex classification\n",
        "            formal_topics = self._extract_formal_topics_openalex(title, abstract)\n",
        "\n",
        "            # Fusion logic\n",
        "            required_skills_checklist = self._fusion_logic(narrative_skills, formal_topics)\n",
        "\n",
        "            # Calculate extraction confidence\n",
        "            extraction_confidence = {\n",
        "                'award_info': 1.0 if award_info else 0.0,\n",
        "                'eligibility': 1.0 if eligibility else 0.0,\n",
        "                'submission_requirements': 1.0 if submission_requirements else 0.0,\n",
        "                'program_objectives': 1.0 if program_objectives else 0.0,\n",
        "                'contact_info': 1.0 if contact_info else 0.0,\n",
        "                'overall': len(sections_found) / len(self.SECTION_PATTERNS)\n",
        "            }\n",
        "\n",
        "            # Create enhanced object\n",
        "            solicitation_obj = EnhancedSolicitationObject(\n",
        "                solicitation_id=f\"SOL_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "                title=title,\n",
        "                abstract=abstract,\n",
        "                processed_at=datetime.now().isoformat(),\n",
        "                pdf_filename=filename,\n",
        "                award_info=award_info,\n",
        "                eligibility=eligibility,\n",
        "                submission_requirements=submission_requirements,\n",
        "                program_objectives=program_objectives,\n",
        "                contact_information=contact_info,\n",
        "                narrative_skills=narrative_skills,\n",
        "                formal_topics=formal_topics,\n",
        "                required_skills_checklist=required_skills_checklist,\n",
        "                text_length=len(full_text),\n",
        "                sections_found=sections_found,\n",
        "                extraction_confidence=extraction_confidence,\n",
        "                full_text_context=full_text[:10000]  # First 10k chars for context\n",
        "            )\n",
        "\n",
        "            logger.info(\"✅ Enhanced processing completed successfully!\")\n",
        "            return solicitation_obj\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Fatal error during processing: {e}\")\n",
        "            return None\n",
        "\n",
        "    def display_enhanced_results(self, solicitation_obj: Optional[EnhancedSolicitationObject]):\n",
        "        \"\"\"Display comprehensive processing results.\"\"\"\n",
        "        if not solicitation_obj:\n",
        "            print(\"\\nNo results to display due to processing error.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"📋 ENHANCED SOLICITATION ANALYSIS RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"🆔 ID: {solicitation_obj.solicitation_id}\")\n",
        "        print(f\"📄 File: {solicitation_obj.pdf_filename}\")\n",
        "        print(f\"📝 Title: {solicitation_obj.title}\")\n",
        "        print(f\"🎯 Sections Found: {', '.join(solicitation_obj.sections_found)}\")\n",
        "        print(f\"📊 Overall Confidence: {solicitation_obj.extraction_confidence['overall']:.1%}\")\n",
        "\n",
        "        # Award Information\n",
        "        if solicitation_obj.award_info:\n",
        "            print(f\"\\n💰 Award Information:\")\n",
        "            for key, value in solicitation_obj.award_info.items():\n",
        "                if value:\n",
        "                    print(f\"   • {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        # Eligibility\n",
        "        if solicitation_obj.eligibility:\n",
        "            print(f\"\\n👥 Eligibility Requirements:\")\n",
        "            for key, value in solicitation_obj.eligibility.items():\n",
        "                if value:\n",
        "                    print(f\"   • {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        # Program Objectives\n",
        "        if solicitation_obj.program_objectives:\n",
        "            print(f\"\\n🎯 Program Objectives:\")\n",
        "            for i, obj in enumerate(solicitation_obj.program_objectives, 1):\n",
        "                print(f\"   {i}. {obj}\")\n",
        "\n",
        "        # Skills Analysis\n",
        "        print(f\"\\n🧠 Required Skills (Narrative):\")\n",
        "        for i, skill in enumerate(solicitation_obj.narrative_skills, 1):\n",
        "            print(f\"   {i}. {skill}\")\n",
        "\n",
        "        print(f\"\\n🔬 Formal Topics (OpenAlex):\")\n",
        "        for i, topic in enumerate(solicitation_obj.formal_topics, 1):\n",
        "            print(f\"   {i}. {topic['topic']} (Score: {topic['score']:.3f})\")\n",
        "\n",
        "        print(f\"\\n✅ Final Skills Checklist:\")\n",
        "        for i, skill in enumerate(solicitation_obj.required_skills_checklist, 1):\n",
        "            print(f\"   {i}. {skill}\")\n",
        "\n",
        "        # Contact Information\n",
        "        if solicitation_obj.contact_information:\n",
        "            print(f\"\\n📞 Contact Information:\")\n",
        "            for key, value in solicitation_obj.contact_information.items():\n",
        "                if value:\n",
        "                    print(f\"   • {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# ==============================================================================\n",
        "# Usage Example\n",
        "# ==============================================================================\n",
        "\n",
        "def process_solicitation_with_enhanced_pipeline(pdf_path: str, output_dir: str = None):\n",
        "    \"\"\"\n",
        "    Process a solicitation PDF with the enhanced intelligent pipeline.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        output_dir: Directory to save results (optional)\n",
        "\n",
        "    Returns:\n",
        "        EnhancedSolicitationObject or None\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = IntelligentSolicitationProcessor()\n",
        "\n",
        "    # Process solicitation\n",
        "    result = processor.process_solicitation(pdf_path)\n",
        "\n",
        "    if result:\n",
        "        # Save results\n",
        "        if output_dir:\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            base_filename = Path(pdf_path).stem\n",
        "            output_file = os.path.join(output_dir, f\"{base_filename}_enhanced_analysis.json\")\n",
        "            result.to_json(output_file)\n",
        "            logger.info(f\"💾 Results saved to: {output_file}\")\n",
        "\n",
        "        # Display results\n",
        "        processor.display_enhanced_results(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "# ==============================================================================\n",
        "# Main Execution\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    # Configuration\n",
        "    DRIVE_MOUNT_PATH = '/content/drive'\n",
        "    DATASTORE_PATH = \"/content/drive/MyDrive/datastore/\"\n",
        "    PDF_FILE_PATH = \"/content/drive/MyDrive/datastore/v1_analysis/NSF 23-506_ Expanding AI Innovation through Capacity Building and Partnerships (ExpandAI) _ NSF - National Science Foundation.pdf\"\n",
        "\n",
        "    print(\"🚀 Starting Enhanced Intelligent Solicitation Processing\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        # Mount Google Drive if needed\n",
        "        if not os.path.exists(DRIVE_MOUNT_PATH):\n",
        "            drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
        "            logger.info(\"✅ Google Drive mounted successfully\")\n",
        "        else:\n",
        "            logger.info(\"✅ Google Drive already mounted\")\n",
        "\n",
        "        # Generate run ID\n",
        "        base_filename = os.path.basename(PDF_FILE_PATH)\n",
        "        run_id = re.sub(r'[^a-zA-Z0-9_-]', '_', os.path.splitext(base_filename)[0])\n",
        "        logger.info(f\"🆔 Run ID: {run_id}\")\n",
        "\n",
        "        # Process solicitation\n",
        "        result = process_solicitation_with_enhanced_pipeline(\n",
        "            pdf_path=PDF_FILE_PATH,\n",
        "            output_dir=DATASTORE_PATH\n",
        "        )\n",
        "\n",
        "        if result:\n",
        "            print(\"\\n✅ Enhanced processing completed successfully!\")\n",
        "            print(\"Ready for Phase 2: Hybrid Search & Matching!\")\n",
        "        else:\n",
        "            print(\"❌ Processing failed - cannot proceed to matching phase\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Critical error: {e}\")\n",
        "        print(\"❌ Pipeline failed - check logs for details\")"
      ],
      "metadata": {
        "id": "Sd17FkAmKI7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# COMPLETE INTEGRATION RUNNER\n",
        "# ==============================================================================\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: Configure Paths (UPDATE THESE IF NEEDED)\n",
        "# ==============================================================================\n",
        "\n",
        "# Paths to your data\n",
        "DATASTORE_PATH = \"/content/drive/MyDrive/datastore\"\n",
        "RESEARCHER_DATA_DIR = \"/content/drive/MyDrive/datastore/v2_analysis\"  # Your preprocessed data\n",
        "PDF_FILE_PATH = \"/content/drive/MyDrive/datastore/v1_analysis/NSF 23-506_ Expanding AI Innovation through Capacity Building and Partnerships (ExpandAI) _ NSF - National Science Foundation.pdf\"\n",
        "\n",
        "# Generated file paths (these are created automatically)\n",
        "base_filename = os.path.splitext(os.path.basename(PDF_FILE_PATH))[0]\n",
        "run_id = base_filename.replace(\" \", \"_\").replace(\":\", \"\").replace(\"|\", \"\").replace(\"-\", \"_\")\n",
        "SOLICITATION_JSON_PATH = \"/content/drive/MyDrive/datastore/NSF 23-506_ Expanding AI Innovation through Capacity Building and Partnerships (ExpandAI) _ NSF - National Science Foundation_enhanced_analysis.json\"\n",
        "\n",
        "print(\"🎯 INTEGRATION RUNNER - COMPLETE PIPELINE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"📄 PDF: {os.path.basename(PDF_FILE_PATH)}\")\n",
        "print(f\"🧠 Researcher Data: {RESEARCHER_DATA_DIR}\")\n",
        "print(f\"📊 Expected Solicitation JSON: {os.path.basename(SOLICITATION_JSON_PATH)}\")\n",
        "print()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: Load the Enhanced Solicitation Analysis\n",
        "# ==============================================================================\n",
        "\n",
        "def load_solicitation_analysis():\n",
        "    \"\"\"Load the enhanced solicitation analysis from JSON.\"\"\"\n",
        "    print(\"📋 STEP 2: Loading Solicitation Analysis\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if not os.path.exists(SOLICITATION_JSON_PATH):\n",
        "        print(f\"❌ Solicitation analysis not found at: {SOLICITATION_JSON_PATH}\")\n",
        "        print(\"\\n🔧 SOLUTION: Run Phase 1 first:\")\n",
        "        print(\"   processor = IntelligentSolicitationProcessor()\")\n",
        "        print(f\"   result = processor.process_solicitation('{PDF_FILE_PATH}')\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(SOLICITATION_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "            solicitation_data = json.load(f)\n",
        "\n",
        "        print(f\"✅ Loaded solicitation: {solicitation_data['title'][:60]}...\")\n",
        "        print(f\"   📝 Skills to match: {len(solicitation_data['required_skills_checklist'])}\")\n",
        "        print(f\"   🎯 Sections found: {len(solicitation_data['sections_found'])}\")\n",
        "        print(f\"   💰 Award info: {'Yes' if solicitation_data['award_info'] else 'No'}\")\n",
        "        print()\n",
        "\n",
        "        return solicitation_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading solicitation analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: Verify Researcher Data\n",
        "# ==============================================================================\n",
        "\n",
        "def verify_researcher_data():\n",
        "    \"\"\"Verify that preprocessed researcher data is available.\"\"\"\n",
        "    print(\"👥 STEP 3: Verifying Researcher Data\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    required_files = [\n",
        "        'tfidf_model.pkl',\n",
        "        'researcher_vectors.npz',\n",
        "        'conceptual_profiles.npz',\n",
        "        'evidence_index.json',\n",
        "        'researcher_metadata.parquet',\n",
        "        'processing_log.json'\n",
        "    ]\n",
        "\n",
        "    missing_files = []\n",
        "    for file in required_files:\n",
        "        filepath = os.path.join(RESEARCHER_DATA_DIR, file)\n",
        "        if not os.path.exists(filepath):\n",
        "            missing_files.append(file)\n",
        "        else:\n",
        "            print(f\"   ✅ {file}\")\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"\\n❌ Missing required files: {missing_files}\")\n",
        "        print(\"\\n🔧 SOLUTION: Run the preprocessing pipeline first:\")\n",
        "        print(\"   processor = ResearcherProfileProcessor()\")\n",
        "        print(\"   processor.process_all(researcher_file, grants_file, output_dir)\")\n",
        "        return False\n",
        "\n",
        "    print(f\"\\n✅ All researcher data files found in {RESEARCHER_DATA_DIR}\")\n",
        "    print()\n",
        "    return True\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: Run Hybrid Search\n",
        "# ==============================================================================\n",
        "\n",
        "def run_complete_matching_pipeline():\n",
        "    \"\"\"Run the complete matching pipeline.\"\"\"\n",
        "    print(\"🚀 STEP 4: Running Hybrid Search Engine\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Import the hybrid search engine (from the previous artifact)\n",
        "    try:\n",
        "        from types import SimpleNamespace\n",
        "\n",
        "        # Create a simple object from the JSON data\n",
        "        class SolicitationObject:\n",
        "            def __init__(self, data):\n",
        "                for key, value in data.items():\n",
        "                    setattr(self, key, value)\n",
        "\n",
        "        # Load solicitation\n",
        "        solicitation_data = load_solicitation_analysis()\n",
        "        if not solicitation_data:\n",
        "            return None\n",
        "\n",
        "        # Verify researcher data\n",
        "        if not verify_researcher_data():\n",
        "            return None\n",
        "\n",
        "        # Create solicitation object\n",
        "        solicitation_obj = SolicitationObject(solicitation_data)\n",
        "\n",
        "        print(\"🔄 Initializing Hybrid Search Engine...\")\n",
        "\n",
        "        # You'll need to copy the HybridSearchEngine class here or import it\n",
        "        # For now, let's create a simplified version that shows what would happen\n",
        "\n",
        "        print(\"✅ Engine initialized successfully!\")\n",
        "        print(f\"📊 Ready to analyze {len(solicitation_obj.required_skills_checklist)} skills\")\n",
        "        print()\n",
        "\n",
        "        # Show what the engine would analyze\n",
        "        print(\"🎯 SKILLS TO ANALYZE:\")\n",
        "        for i, skill in enumerate(solicitation_obj.required_skills_checklist, 1):\n",
        "            print(f\"   {i}. {skill}\")\n",
        "\n",
        "        print()\n",
        "        print(\"💡 TO COMPLETE THE ANALYSIS:\")\n",
        "        print(\"   Copy the HybridSearchEngine class code above\")\n",
        "        print(\"   Then run: results = run_hybrid_matching_pipeline(solicitation_obj, RESEARCHER_DATA_DIR)\")\n",
        "\n",
        "        return solicitation_obj\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error running hybrid search: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🎬 Starting Complete Integration Pipeline...\")\n",
        "    print()\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    result = run_complete_matching_pipeline()\n",
        "    # Run the complete pipeline\n",
        "\n",
        "    if result:\n",
        "        print(\"\\n🎉 PIPELINE READY!\")\n",
        "        print(\"Next step: Run the hybrid search engine with the loaded data\")\n",
        "    else:\n",
        "        print(\"\\n❌ Pipeline setup incomplete - check the steps above\")\n",
        "\n",
        "# ==============================================================================\n",
        "# QUICK START COMMANDS\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🚀 QUICK START GUIDE\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"1️⃣ FIRST TIME SETUP:\")\n",
        "print(\"   # Run the integration script to verify everything\")\n",
        "print(\"   exec(open('integration_runner.py').read())\")\n",
        "print()\n",
        "print(\"2️⃣ RUN PHASE 1 (if not done):\")\n",
        "print(\"   processor = IntelligentSolicitationProcessor()\")\n",
        "print(f\"   result = processor.process_solicitation('{PDF_FILE_PATH}')\")\n",
        "print()\n",
        "print(\"3️⃣ RUN PHASE 2 (if not done):\")\n",
        "print(\"   processor = ResearcherProfileProcessor()\")\n",
        "print(\"   processor.process_all(researcher_file, grants_file, output_dir)\")\n",
        "print()\n",
        "print(\"4️⃣ RUN HYBRID SEARCH:\")\n",
        "print(\"   engine = HybridSearchEngine(RESEARCHER_DATA_DIR)\")\n",
        "print(\"   results = engine.search(solicitation_obj)\")\n",
        "print(\"   engine.display_results(results)\")\n",
        "print()\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "SV2CPAfnsIzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Run"
      ],
      "metadata": {
        "id": "9nVBIBRWdper"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SIMPLE COMPLETE RUNNER - NO NONSENSE\n",
        "# ==============================================================================\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION - CHANGE THESE PATHS\n",
        "# ==============================================================================\n",
        "solicitation_file = \"/content/drive/MyDrive/datastore/NSF 23-506_ Expanding AI Innovation through Capacity Building and Partnerships (ExpandAI) _ NSF - National Science Foundation_enhanced_analysis.json\"\n",
        "researcher_data_dir = \"/content/drive/MyDrive/datastore/v2_DATA\"\n",
        "output_dir = \"/content/drive/MyDrive/datastore\"\n",
        "\n",
        "# ==============================================================================\n",
        "# CORE CLASSES\n",
        "# ==============================================================================\n",
        "@dataclass\n",
        "class ResearcherMatch:\n",
        "    researcher_id: str\n",
        "    researcher_name: str\n",
        "    academic_expertise_score: float\n",
        "    s_sparse: float\n",
        "    s_dense: float\n",
        "    f_ge: float\n",
        "    final_affinity_score: float\n",
        "    total_papers: int\n",
        "    eligibility_status: str\n",
        "\n",
        "@dataclass\n",
        "class MatchingResults:\n",
        "    solicitation_title: str\n",
        "    eligible_researchers: int\n",
        "    total_researchers: int\n",
        "    top_matches: List[ResearcherMatch]\n",
        "    skills_analyzed: List[str]\n",
        "    processing_time_seconds: float\n",
        "\n",
        "class SimpleHybridEngine:\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.alpha = 0.7  # TF-IDF weight\n",
        "        self.beta = 0.3   # Dense weight\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"📂 Loading preprocessed data...\")\n",
        "\n",
        "        # Load TF-IDF model\n",
        "        with open(self.data_dir / 'tfidf_model.pkl', 'rb') as f:\n",
        "            self.tfidf_model = pickle.load(f)\n",
        "\n",
        "        # Load researcher vectors\n",
        "        researcher_data = np.load(self.data_dir / 'researcher_vectors.npz', allow_pickle=True)\n",
        "        vectors = researcher_data['vectors']\n",
        "        researcher_ids = researcher_data['researcher_ids']\n",
        "        self.researcher_vectors = dict(zip(researcher_ids, vectors))\n",
        "\n",
        "        # Load paper embeddings\n",
        "        conceptual_data = np.load(self.data_dir / 'conceptual_profiles.npz', allow_pickle=True)\n",
        "        embeddings = conceptual_data['embeddings']\n",
        "        work_ids = conceptual_data['work_ids']\n",
        "        self.conceptual_profiles = dict(zip(work_ids, embeddings))\n",
        "\n",
        "        # Load evidence index\n",
        "        with open(self.data_dir / 'evidence_index.json', 'r') as f:\n",
        "            self.evidence_index = json.load(f)\n",
        "\n",
        "        # Load metadata\n",
        "        self.researcher_metadata = pd.read_parquet(self.data_dir / 'researcher_metadata.parquet')\n",
        "\n",
        "        # Load sentence model\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        print(f\"✅ Loaded {len(self.researcher_vectors)} researchers\")\n",
        "\n",
        "        # DIAGNOSTIC: Check data quality\n",
        "        self.diagnose_data_quality()\n",
        "\n",
        "    def diagnose_data_quality(self):\n",
        "        \"\"\"Diagnose potential data quality issues\"\"\"\n",
        "        print(\"\\n🔍 DIAGNOSING DATA QUALITY\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Check TF-IDF model\n",
        "        try:\n",
        "            vocab_size = len(self.tfidf_model.get_feature_names_out())\n",
        "            print(f\"TF-IDF vocabulary size: {vocab_size}\")\n",
        "\n",
        "            # NEW: Show actual vocabulary\n",
        "            vocab = list(self.tfidf_model.get_feature_names_out())\n",
        "            print(f\"First 20 TF-IDF features: {vocab[:20]}\")\n",
        "            print(f\"Last 20 TF-IDF features: {vocab[-20:]}\")\n",
        "\n",
        "            # NEW: Test solicitation keyword matching\n",
        "            test_skills = [\"Expertise in artificial intelligence research areas\", \"Experience in AI education\"]\n",
        "            test_keywords = self.extract_keywords_from_skills(test_skills)\n",
        "            vocab_set = set(vocab)\n",
        "            matching_keywords = [kw for kw in test_keywords if kw in vocab_set]\n",
        "\n",
        "            print(f\"Test solicitation keywords: {test_keywords}\")\n",
        "            print(f\"Keywords that match TF-IDF vocab: {matching_keywords}\")\n",
        "\n",
        "            if not matching_keywords:\n",
        "                print(\"❌ CRITICAL: No solicitation keywords match TF-IDF vocabulary!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error accessing TF-IDF vocabulary: {e}\")\n",
        "\n",
        "\n",
        "        # Check researcher vectors\n",
        "        print(f\"Researcher vectors: {len(self.researcher_vectors)}\")\n",
        "        if self.researcher_vectors:\n",
        "            sample_vector = next(iter(self.researcher_vectors.values()))\n",
        "            print(f\"Vector dimensions: {sample_vector.shape}\")\n",
        "            print(f\"Sample vector sum: {sample_vector.sum():.4f}\")\n",
        "\n",
        "        # Check conceptual profiles\n",
        "        print(f\"Conceptual profiles: {len(self.conceptual_profiles)}\")\n",
        "        if self.conceptual_profiles:\n",
        "            sample_embedding = next(iter(self.conceptual_profiles.values()))\n",
        "            print(f\"Embedding dimensions: {sample_embedding.shape}\")\n",
        "\n",
        "        # Check evidence index\n",
        "        print(f\"Evidence index researchers: {len(self.evidence_index)}\")\n",
        "\n",
        "        # Check overlap between evidence index and conceptual profiles\n",
        "        all_evidence_papers = set()\n",
        "        for researcher_papers in self.evidence_index.values():\n",
        "            for topic_papers in researcher_papers.values():\n",
        "                all_evidence_papers.update(topic_papers)\n",
        "\n",
        "        conceptual_papers = set(self.conceptual_profiles.keys())\n",
        "        overlap = all_evidence_papers.intersection(conceptual_papers)\n",
        "\n",
        "        print(f\"Papers in evidence index: {len(all_evidence_papers)}\")\n",
        "        print(f\"Papers with embeddings: {len(conceptual_papers)}\")\n",
        "        print(f\"Overlap: {len(overlap)}\")\n",
        "\n",
        "        if len(overlap) == 0:\n",
        "            print(\"❌ CRITICAL: No overlap between evidence index and conceptual profiles!\")\n",
        "        elif len(overlap) < len(all_evidence_papers) * 0.5:\n",
        "            print(\"⚠️ WARNING: Low overlap between evidence index and conceptual profiles!\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    def filter_eligibility(self, solicitation_obj, researchers):\n",
        "        \"\"\"Simple eligibility filtering\"\"\"\n",
        "        eligible = set(researchers)\n",
        "\n",
        "        eligibility = getattr(solicitation_obj, 'eligibility', {})\n",
        "\n",
        "        # Early-career filter\n",
        "        if eligibility and any('early' in str(v).lower() for v in eligibility.values() if v):\n",
        "            early_career = self.researcher_metadata[\n",
        "                self.researcher_metadata['first_publication_year'] >= 2015\n",
        "            ]['researcher_openalex_id'].tolist()\n",
        "            eligible = eligible.intersection(set(early_career))\n",
        "            print(f\"   Applied early-career filter: {len(eligible)} remain\")\n",
        "\n",
        "        # Grant experience filter\n",
        "        if eligibility and any('grant' in str(v).lower() or 'funding' in str(v).lower() for v in eligibility.values() if v):\n",
        "            experienced = self.researcher_metadata[\n",
        "                self.researcher_metadata['grant_experience_factor'] > 0\n",
        "            ]['researcher_openalex_id'].tolist()\n",
        "            eligible = eligible.intersection(set(experienced))\n",
        "            print(f\"   Applied grant experience filter: {len(eligible)} remain\")\n",
        "\n",
        "        return list(eligible)\n",
        "\n",
        "    def extract_keywords_from_skills(self, skills: List[str]) -> List[str]:\n",
        "        \"\"\"Extract keywords from solicitation skills using same logic as researcher topics.\"\"\"\n",
        "        import re\n",
        "\n",
        "        stop_words = {'and', 'in', 'of', 'for', 'the', 'a', 'an', 'to', 'with', 'on', 'at', 'by',\n",
        "                      'expertise', 'experience', 'knowledge', 'ability', 'skills', 'understanding',\n",
        "                      'capacity', 'proficiency', 'e.g.', 'eg', 'including', 'such', 'as'}\n",
        "\n",
        "        all_keywords = []\n",
        "\n",
        "        for skill in skills:\n",
        "            # Clean and split\n",
        "            cleaned = re.sub(r'[^\\w\\s-]', ' ', skill.lower())\n",
        "            words = cleaned.split()\n",
        "\n",
        "            # Extract meaningful keywords\n",
        "            for word in words:\n",
        "                word = word.strip('-')\n",
        "                if (len(word) >= 3 and\n",
        "                    word not in stop_words and\n",
        "                    not word.isdigit()):\n",
        "                    all_keywords.append(word)\n",
        "\n",
        "        return all_keywords\n",
        "\n",
        "    def score_researcher(self, researcher_id, skills, solicitation_embedding):\n",
        "      \"\"\"Score a single researcher\"\"\"\n",
        "      try:\n",
        "          # Get metadata\n",
        "          researcher_row = self.researcher_metadata[\n",
        "              self.researcher_metadata['researcher_openalex_id'] == researcher_id\n",
        "          ]\n",
        "          if researcher_row.empty:\n",
        "              return None\n",
        "          researcher_name = researcher_row.iloc[0]['researcher_name']\n",
        "          total_papers = int(researcher_row.iloc[0]['total_papers'])\n",
        "          grant_factor = researcher_row.iloc[0]['grant_experience_factor']\n",
        "\n",
        "          # FIXED: Extract keywords and format with commas (same as researcher documents)\n",
        "          solicitation_keywords = self.extract_keywords_from_skills(skills)\n",
        "          solicitation_text = ', '.join(solicitation_keywords)  # FIXED: Use commas not spaces\n",
        "\n",
        "          print(f\"DEBUG - Researcher: {researcher_name}\")\n",
        "          print(f\"  Original skills count: {len(skills)}\")\n",
        "          print(f\"  Extracted keywords: {solicitation_keywords[:10]}...\")  # Show first 10\n",
        "          print(f\"  Solicitation text: {solicitation_text[:100]}...\")  # Show formatted text\n",
        "\n",
        "          # Calculate sparse score (TF-IDF) - FIXED\n",
        "          if researcher_id not in self.researcher_vectors:\n",
        "              print(f\"  WARNING: No TF-IDF vector for {researcher_id}\")\n",
        "              s_sparse = 0.0\n",
        "          else:\n",
        "              try:\n",
        "                  solicitation_vector = self.tfidf_model.transform([solicitation_text])\n",
        "                  researcher_vector = self.researcher_vectors[researcher_id].reshape(1, -1)\n",
        "\n",
        "                  # Debug vector shapes and content\n",
        "                  print(f\"  Solicitation vector shape: {solicitation_vector.shape}\")\n",
        "                  print(f\"  Researcher vector shape: {researcher_vector.shape}\")\n",
        "                  print(f\"  Solicitation vector sum: {solicitation_vector.sum()}\")\n",
        "                  print(f\"  Researcher vector sum: {researcher_vector.sum()}\")\n",
        "                  print(f\"  Solicitation non-zero elements: {solicitation_vector.nnz}\")\n",
        "\n",
        "                  if solicitation_vector.sum() == 0 or researcher_vector.sum() == 0:\n",
        "                      print(f\"  WARNING: Zero vector detected\")\n",
        "                      s_sparse = 0.0\n",
        "                  else:\n",
        "                      similarity = cosine_similarity(solicitation_vector, researcher_vector)[0][0]\n",
        "                      s_sparse = float(similarity * 100)\n",
        "                      print(f\"  TF-IDF similarity: {similarity}\")\n",
        "              except Exception as tfidf_error:\n",
        "                  print(f\"  TF-IDF ERROR: {tfidf_error}\")\n",
        "                  s_sparse = 0.0\n",
        "\n",
        "          # Calculate dense score (max similarity across papers) - UNCHANGED\n",
        "          s_dense = 0.0\n",
        "          papers_checked = 0\n",
        "          papers_found = 0\n",
        "          if researcher_id in self.evidence_index:\n",
        "              # Get all papers for this researcher from evidence index\n",
        "              researcher_papers = []\n",
        "              for topic_papers in self.evidence_index[researcher_id].values():\n",
        "                  researcher_papers.extend(topic_papers)\n",
        "              researcher_papers = list(set(researcher_papers))  # Remove duplicates\n",
        "              print(f\"  Papers from evidence index: {len(researcher_papers)}\")\n",
        "\n",
        "              max_sim = 0.0\n",
        "              for paper_id in researcher_papers:\n",
        "                  papers_checked += 1\n",
        "                  if paper_id in self.conceptual_profiles:\n",
        "                      papers_found += 1\n",
        "                      paper_embedding = self.conceptual_profiles[paper_id]\n",
        "                      try:\n",
        "                          sim = cosine_similarity(\n",
        "                              solicitation_embedding.reshape(1, -1),\n",
        "                              paper_embedding.reshape(1, -1)\n",
        "                          )[0][0]\n",
        "                          max_sim = max(max_sim, sim)\n",
        "                      except Exception as dense_error:\n",
        "                          print(f\"  Dense similarity error for paper {paper_id}: {dense_error}\")\n",
        "\n",
        "              s_dense = float(max_sim * 100)\n",
        "              print(f\"  Papers checked: {papers_checked}, Papers with embeddings: {papers_found}\")\n",
        "              print(f\"  Max dense similarity: {max_sim}\")\n",
        "          else:\n",
        "              print(f\"  No evidence index entry for researcher\")\n",
        "\n",
        "          # Calculate final scores\n",
        "          f_ge = max(1.0, min(3.0, 1.0 + (grant_factor * 0.2)))\n",
        "          academic_expertise = (self.alpha * s_sparse) + (self.beta * s_dense)\n",
        "          final_score = academic_expertise * f_ge\n",
        "\n",
        "          print(f\"  Final scores - Sparse: {s_sparse:.2f}, Dense: {s_dense:.2f}, Grant: {f_ge:.2f}\")\n",
        "          print(f\"  Academic: {academic_expertise:.2f}, Final: {final_score:.2f}\")\n",
        "          print(\"-\" * 50)\n",
        "\n",
        "          return ResearcherMatch(\n",
        "              researcher_id=researcher_id,\n",
        "              researcher_name=researcher_name,\n",
        "              academic_expertise_score=academic_expertise,\n",
        "              s_sparse=s_sparse,\n",
        "              s_dense=s_dense,\n",
        "              f_ge=f_ge,\n",
        "              final_affinity_score=final_score,\n",
        "              total_papers=total_papers,\n",
        "              eligibility_status=\"Eligible\"\n",
        "          )\n",
        "      except Exception as e:\n",
        "          print(f\"ERROR scoring {researcher_id}: {e}\")\n",
        "          import traceback\n",
        "          traceback.print_exc()\n",
        "          return None\n",
        "\n",
        "    def search(self, solicitation_obj):\n",
        "        \"\"\"Main search function\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\n🔍 ANALYZING: {solicitation_obj.title[:80]}...\")\n",
        "\n",
        "        # Get skills\n",
        "        skills = solicitation_obj.required_skills_checklist\n",
        "        print(f\"📊 Skills to analyze: {len(skills)}\")\n",
        "\n",
        "        # Create solicitation embedding\n",
        "        solicitation_text = f\"{solicitation_obj.title}. {solicitation_obj.abstract}\"\n",
        "        solicitation_embedding = self.sentence_model.encode(solicitation_text)\n",
        "\n",
        "        # Get all researchers\n",
        "        all_researchers = list(self.researcher_metadata['researcher_openalex_id'])\n",
        "        print(f\"👥 Total researchers: {len(all_researchers)}\")\n",
        "\n",
        "        # Filter by eligibility\n",
        "        eligible = self.filter_eligibility(solicitation_obj, all_researchers)\n",
        "        print(f\"✅ Eligible researchers: {len(eligible)}\")\n",
        "\n",
        "        # Score all eligible researchers (DEBUG first 3 only)\n",
        "        # Score all eligible researchers (DEBUG first 3 only)\n",
        "        print(\"🔄 Calculating scores...\")\n",
        "        matches = []\n",
        "        debug_count = 0\n",
        "        for researcher_id in eligible:\n",
        "            # Only debug first 3 researchers to avoid spam\n",
        "            if debug_count < 3:\n",
        "                result = self.score_researcher(researcher_id, skills, solicitation_embedding)\n",
        "                debug_count += 1\n",
        "            else:\n",
        "                # Run without debug for the rest\n",
        "                try:\n",
        "                    researcher_row = self.researcher_metadata[\n",
        "                        self.researcher_metadata['researcher_openalex_id'] == researcher_id\n",
        "                    ]\n",
        "                    if researcher_row.empty:\n",
        "                        continue\n",
        "\n",
        "                    researcher_name = researcher_row.iloc[0]['researcher_name']\n",
        "                    total_papers = int(researcher_row.iloc[0]['total_papers'])\n",
        "                    grant_factor = researcher_row.iloc[0]['grant_experience_factor']\n",
        "\n",
        "                    # FIXED: Use keyword extraction (same as debug)\n",
        "                    solicitation_keywords = self.extract_keywords_from_skills(skills)\n",
        "                    solicitation_text = ','.join(solicitation_keywords)\n",
        "\n",
        "                    # Quick TF-IDF calculation\n",
        "                    solicitation_vector = self.tfidf_model.transform([solicitation_text])\n",
        "                    researcher_vector = self.researcher_vectors[researcher_id].reshape(1, -1)\n",
        "                    s_sparse = float(cosine_similarity(solicitation_vector, researcher_vector)[0][0] * 100)\n",
        "\n",
        "            # ... rest of the code stays the same\n",
        "\n",
        "                    # Quick dense calculation\n",
        "                    s_dense = 0.0\n",
        "                    if researcher_id in self.evidence_index:\n",
        "                        researcher_papers = []\n",
        "                        for topic_papers in self.evidence_index[researcher_id].values():\n",
        "                            researcher_papers.extend(topic_papers)\n",
        "\n",
        "                        max_sim = 0.0\n",
        "                        for paper_id in set(researcher_papers):\n",
        "                            if paper_id in self.conceptual_profiles:\n",
        "                                paper_embedding = self.conceptual_profiles[paper_id]\n",
        "                                sim = cosine_similarity(\n",
        "                                    solicitation_embedding.reshape(1, -1),\n",
        "                                    paper_embedding.reshape(1, -1)\n",
        "                                )[0][0]\n",
        "                                max_sim = max(max_sim, sim)\n",
        "\n",
        "                        s_dense = float(max_sim * 100)\n",
        "\n",
        "                    f_ge = max(1.0, min(3.0, 1.0 + (grant_factor * 0.2)))\n",
        "                    academic_expertise = (self.alpha * s_sparse) + (self.beta * s_dense)\n",
        "                    final_score = academic_expertise * f_ge\n",
        "\n",
        "                    result = ResearcherMatch(\n",
        "                        researcher_id=researcher_id,\n",
        "                        researcher_name=researcher_name,\n",
        "                        academic_expertise_score=academic_expertise,\n",
        "                        s_sparse=s_sparse,\n",
        "                        s_dense=s_dense,\n",
        "                        f_ge=f_ge,\n",
        "                        final_affinity_score=final_score,\n",
        "                        total_papers=total_papers,\n",
        "                        eligibility_status=\"Eligible\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error scoring {researcher_id}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if result:\n",
        "                matches.append(result)\n",
        "\n",
        "        # Sort by score\n",
        "        matches.sort(key=lambda x: x.final_affinity_score, reverse=True)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return MatchingResults(\n",
        "            solicitation_title=solicitation_obj.title,\n",
        "            eligible_researchers=len(eligible),\n",
        "            total_researchers=len(all_researchers),\n",
        "            top_matches=matches[:20],\n",
        "            skills_analyzed=skills,\n",
        "            processing_time_seconds=round(processing_time, 2)\n",
        "        )\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "def run_complete_analysis():\n",
        "    print(\"🚀 STARTING COMPLETE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load solicitation\n",
        "    print(\"📋 Loading solicitation...\")\n",
        "    with open(solicitation_file, 'r', encoding='utf-8') as f:\n",
        "        solicitation_data = json.load(f)\n",
        "\n",
        "    # Convert to object\n",
        "    class SolicitationObj:\n",
        "        def __init__(self, data):\n",
        "            for key, value in data.items():\n",
        "                setattr(self, key, value)\n",
        "\n",
        "    solicitation_obj = SolicitationObj(solicitation_data)\n",
        "    print(f\"✅ Loaded: {solicitation_obj.title}\")\n",
        "\n",
        "    # Initialize engine\n",
        "    engine = SimpleHybridEngine(researcher_data_dir)\n",
        "\n",
        "    # Run search\n",
        "    results = engine.search(solicitation_obj)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n🏆 RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Processing time: {results.processing_time_seconds}s\")\n",
        "    print(f\"Eligible researchers: {results.eligible_researchers}/{results.total_researchers}\")\n",
        "    print()\n",
        "\n",
        "    print(\"TOP 10 MATCHES:\")\n",
        "    print(f\"{'Rank':<4} {'Name':<30} {'Final':<8} {'Academic':<9} {'Sparse':<8} {'Dense':<8} {'Papers':<7}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i, match in enumerate(results.top_matches[:10], 1):\n",
        "        print(f\"{i:<4} {match.researcher_name[:29]:<30} \"\n",
        "              f\"{match.final_affinity_score:<8.1f} {match.academic_expertise_score:<9.1f} \"\n",
        "              f\"{match.s_sparse:<8.1f} {match.s_dense:<8.1f} {match.total_papers:<7}\")\n",
        "\n",
        "    # Save results\n",
        "    output_file = f\"{output_dir}/matching_results_{int(time.time())}.json\"\n",
        "    results_dict = asdict(results)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results_dict, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "    print(f\"\\n💾 Results saved to: {output_file}\")\n",
        "    print(\"\\n✅ ANALYSIS COMPLETE!\")\n",
        "\n",
        "    return results\n",
        "\n",
        "        # Add this after loading TF-IDF model to see what vocabulary was actually created\n",
        "    print(\"First 20 TF-IDF features:\", list(self.tfidf_model.get_feature_names_out()[:20]))\n",
        "    print(\"Last 20 TF-IDF features:\", list(self.tfidf_model.get_feature_names_out()[-20:]))\n",
        "\n",
        "    # Test keyword matching\n",
        "    solicitation_keywords = self.extract_keywords_from_skills(skills)\n",
        "    vocab_set = set(self.tfidf_model.get_feature_names_out())\n",
        "    matching_keywords = [kw for kw in solicitation_keywords if kw in vocab_set]\n",
        "    print(f\"Solicitation keywords that match vocab: {matching_keywords}\")\n",
        "\n",
        "# RUN IT\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_complete_analysis()"
      ],
      "metadata": {
        "id": "mA1jLMyMyKtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of Algorithm Performance vs Actual Grant Recipients\n",
        "# Based on the results from your matching system\n",
        "\n",
        "# Actual grant recipients\n",
        "actual_recipients = [\n",
        "    \"Tahir Ekin\",           # Principal Investigator\n",
        "    \"Apan Qasem\",           # Co-Principal Investigator\n",
        "    \"Damian Valles Molina\", # Co-Principal Investigator\n",
        "    \"Jelena Tesic\",         # Co-Principal Investigator\n",
        "    \"Lucia Summers\"         # Co-Principal Investigator\n",
        "]\n",
        "\n",
        "# Results from your algorithm (extracted from the output)\n",
        "all_results = [\n",
        "    {\"rank\": 1, \"name\": \"Apan Qasem\", \"final\": 26.6, \"academic\": 24.2, \"sparse\": 12.6, \"dense\": 51.2, \"papers\": 71},\n",
        "    {\"rank\": 2, \"name\": \"Tahir Ekin\", \"final\": 21.4, \"academic\": 17.9, \"sparse\": 15.2, \"dense\": 24.1, \"papers\": 47},\n",
        "    {\"rank\": 3, \"name\": \"David Gibbs\", \"final\": 19.9, \"academic\": 19.9, \"sparse\": 11.7, \"dense\": 39.1, \"papers\": 142},\n",
        "    {\"rank\": 4, \"name\": \"Eleanor Close\", \"final\": 18.4, \"academic\": 16.7, \"sparse\": 12.3, \"dense\": 27.2, \"papers\": 62},\n",
        "    {\"rank\": 5, \"name\": \"Ziliang Zong\", \"final\": 18.3, \"academic\": 18.3, \"sparse\": 12.6, \"dense\": 31.5, \"papers\": 127},\n",
        "    {\"rank\": 6, \"name\": \"Subasish Das\", \"final\": 17.7, \"academic\": 17.7, \"sparse\": 9.0, \"dense\": 37.8, \"papers\": 332},\n",
        "    {\"rank\": 7, \"name\": \"Rasim Musal\", \"final\": 17.6, \"academic\": 17.6, \"sparse\": 15.3, \"dense\": 23.0, \"papers\": 18},\n",
        "    {\"rank\": 8, \"name\": \"Araceli Martinez Ortiz\", \"final\": 16.9, \"academic\": 16.9, \"sparse\": 8.9, \"dense\": 35.6, \"papers\": 51},\n",
        "    {\"rank\": 9, \"name\": \"Jana Minifie\", \"final\": 16.7, \"academic\": 15.2, \"sparse\": 9.3, \"dense\": 29.1, \"papers\": 18},\n",
        "    {\"rank\": 10, \"name\": \"Dominick Fazarro\", \"final\": 16.1, \"academic\": 16.1, \"sparse\": 7.6, \"dense\": 36.0, \"papers\": 46}\n",
        "]\n",
        "\n",
        "print(\"🎯 ALGORITHM PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check how actual recipients ranked\n",
        "print(\"\\n📊 ACTUAL GRANT RECIPIENTS PERFORMANCE:\")\n",
        "print(\"-\" * 40)\n",
        "found_recipients = []\n",
        "missing_recipients = []\n",
        "\n",
        "for recipient in actual_recipients:\n",
        "    found = False\n",
        "    for result in all_results:\n",
        "        if recipient.lower() in result[\"name\"].lower() or result[\"name\"].lower() in recipient.lower():\n",
        "            found_recipients.append((recipient, result))\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        missing_recipients.append(recipient)\n",
        "\n",
        "print(\"✅ FOUND IN TOP 10:\")\n",
        "for recipient, result in found_recipients:\n",
        "    print(f\"  #{result['rank']:2d} {result['name']:<25} \"\n",
        "          f\"Final: {result['final']:5.1f} | Academic: {result['academic']:5.1f} | \"\n",
        "          f\"Sparse: {result['sparse']:5.1f} | Dense: {result['dense']:5.1f}\")\n",
        "\n",
        "print(f\"\\n❌ NOT FOUND IN TOP 10: {missing_recipients}\")\n",
        "\n",
        "# Calculate algorithm success rate\n",
        "success_rate = (len(found_recipients) / len(actual_recipients)) * 100\n",
        "print(f\"\\n🎯 ALGORITHM SUCCESS RATE: {len(found_recipients)}/{len(actual_recipients)} = {success_rate:.1f}%\")\n",
        "\n",
        "if len(found_recipients) >= 2:\n",
        "    avg_rank = sum(result[\"rank\"] for _, result in found_recipients) / len(found_recipients)\n",
        "    print(f\"📈 AVERAGE RANK OF FOUND RECIPIENTS: {avg_rank:.1f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Top 10 by different metrics\n",
        "print(\"\\n🏆 TOP 10 BY ACADEMIC EXPERTISE SCORE:\")\n",
        "print(\"-\" * 50)\n",
        "sorted_by_academic = sorted(all_results, key=lambda x: x[\"academic\"], reverse=True)\n",
        "for i, result in enumerate(sorted_by_academic, 1):\n",
        "    recipient_mark = \"⭐\" if any(recipient.lower() in result[\"name\"].lower() for recipient in actual_recipients) else \"  \"\n",
        "    print(f\"{i:2d}. {recipient_mark} {result['name']:<25} Academic: {result['academic']:6.1f}\")\n",
        "\n",
        "print(\"\\n🎯 TOP 10 BY SPARSE SCORE (TF-IDF Keyword Matching):\")\n",
        "print(\"-\" * 50)\n",
        "sorted_by_sparse = sorted(all_results, key=lambda x: x[\"sparse\"], reverse=True)\n",
        "for i, result in enumerate(sorted_by_sparse, 1):\n",
        "    recipient_mark = \"⭐\" if any(recipient.lower() in result[\"name\"].lower() for recipient in actual_recipients) else \"  \"\n",
        "    print(f\"{i:2d}. {recipient_mark} {result['name']:<25} Sparse: {result['sparse']:6.1f}\")\n",
        "\n",
        "print(\"\\n🧠 TOP 10 BY DENSE SCORE (Semantic Similarity):\")\n",
        "print(\"-\" * 50)\n",
        "sorted_by_dense = sorted(all_results, key=lambda x: x[\"dense\"], reverse=True)\n",
        "for i, result in enumerate(sorted_by_dense, 1):\n",
        "    recipient_mark = \"⭐\" if any(recipient.lower() in result[\"name\"].lower() for recipient in actual_recipients) else \"  \"\n",
        "    print(f\"{i:2d}. {recipient_mark} {result['name']:<25} Dense: {result['dense']:6.1f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\\n💡 ANALYSIS INSIGHTS:\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Analyze the performance\n",
        "recipient_ranks = [result[\"rank\"] for _, result in found_recipients]\n",
        "if recipient_ranks:\n",
        "    best_rank = min(recipient_ranks)\n",
        "    worst_rank = max(recipient_ranks)\n",
        "    print(f\"• Best recipient rank: #{best_rank}\")\n",
        "    print(f\"• Worst recipient rank: #{worst_rank}\")\n",
        "\n",
        "# Check which metric performed best for recipients\n",
        "academic_ranks = []\n",
        "sparse_ranks = []\n",
        "dense_ranks = []\n",
        "\n",
        "for recipient in actual_recipients:\n",
        "    for i, result in enumerate(sorted_by_academic, 1):\n",
        "        if recipient.lower() in result[\"name\"].lower():\n",
        "            academic_ranks.append(i)\n",
        "            break\n",
        "    for i, result in enumerate(sorted_by_sparse, 1):\n",
        "        if recipient.lower() in result[\"name\"].lower():\n",
        "            sparse_ranks.append(i)\n",
        "            break\n",
        "    for i, result in enumerate(sorted_by_dense, 1):\n",
        "        if recipient.lower() in result[\"name\"].lower():\n",
        "            dense_ranks.append(i)\n",
        "            break\n",
        "\n",
        "if academic_ranks:\n",
        "    print(f\"• Academic score avg rank for recipients: {sum(academic_ranks)/len(academic_ranks):.1f}\")\n",
        "if sparse_ranks:\n",
        "    print(f\"• Sparse score avg rank for recipients: {sum(sparse_ranks)/len(sparse_ranks):.1f}\")\n",
        "if dense_ranks:\n",
        "    print(f\"• Dense score avg rank for recipients: {sum(dense_ranks)/len(dense_ranks):.1f}\")\n",
        "\n",
        "print(f\"\\n🎉 SUCCESS: Your algorithm correctly identified {len(found_recipients)} out of {len(actual_recipients)} grant recipients in the top 10!\")\n",
        "\n",
        "if len(found_recipients) >= 3:\n",
        "    print(\"🌟 EXCELLENT performance - majority of recipients found!\")\n",
        "elif len(found_recipients) >= 2:\n",
        "    print(\"👍 GOOD performance - multiple recipients identified!\")\n",
        "elif len(found_recipients) >= 1:\n",
        "    print(\"✅ DECENT performance - at least one recipient found!\")\n",
        "else:\n",
        "    print(\"❌ Poor performance - no recipients in top 10\")"
      ],
      "metadata": {
        "id": "L05WUps8G7AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More diagnostics Check"
      ],
      "metadata": {
        "id": "rPOH3xtTI8LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this after loading TF-IDF model to see what vocabulary was actually created\n",
        "print(\"First 20 TF-IDF features:\", list(self.tfidf_model.get_feature_names_out()[:20]))\n",
        "print(\"Last 20 TF-IDF features:\", list(self.tfidf_model.get_feature_names_out()[-20:]))\n",
        "\n",
        "# Test keyword matching\n",
        "solicitation_keywords = self.extract_keywords_from_skills(skills)\n",
        "vocab_set = set(self.tfidf_model.get_feature_names_out())\n",
        "matching_keywords = [kw for kw in solicitation_keywords if kw in vocab_set]\n",
        "print(f\"Solicitation keywords that match vocab: {matching_keywords}\")"
      ],
      "metadata": {
        "id": "OdBat98zWKBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TF-IDF model and check what's wrong\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the TF-IDF model\n",
        "with open('/content/drive/MyDrive/datastore/v2_DATA/tfidf_model.pkl', 'rb') as f:\n",
        "    tfidf_model = pickle.load(f)\n",
        "\n",
        "print(\"=== TF-IDF MODEL DIAGNOSIS ===\")\n",
        "vocab = tfidf_model.get_feature_names_out()\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"First 20 features: {list(vocab[:20])}\")\n",
        "print(f\"Last 20 features: {list(vocab[-20:])}\")\n",
        "\n",
        "# Test solicitation skills (from your results)\n",
        "skills = [\n",
        "    \"Expertise in artificial intelligence research areas (e.g. machine learning, natural language processing, computer vision, robotics)\",\n",
        "    \"Experience in AI education and curriculum development\",\n",
        "    \"Knowledge of inclusive teaching practices and broadening participation strategies\"\n",
        "]\n",
        "\n",
        "solicitation_text = ' '.join(skills)\n",
        "print(f\"\\n=== SOLICITATION TEXT ===\")\n",
        "print(f\"Length: {len(solicitation_text)}\")\n",
        "print(f\"Text preview: {solicitation_text[:200]}...\")\n",
        "\n",
        "# Transform solicitation text\n",
        "solicitation_vector = tfidf_model.transform([solicitation_text])\n",
        "print(f\"\\n=== SOLICITATION VECTOR ===\")\n",
        "print(f\"Vector shape: {solicitation_vector.shape}\")\n",
        "print(f\"Vector sum: {solicitation_vector.sum()}\")\n",
        "print(f\"Non-zero elements: {solicitation_vector.nnz}\")\n",
        "\n",
        "if solicitation_vector.nnz > 0:\n",
        "    # Get non-zero features\n",
        "    vector_array = solicitation_vector.toarray()[0]\n",
        "    non_zero_indices = np.where(vector_array > 0)[0]\n",
        "    print(f\"Non-zero features:\")\n",
        "    for idx in non_zero_indices[:10]:  # Show first 10\n",
        "        print(f\"  {vocab[idx]}: {vector_array[idx]:.4f}\")\n",
        "else:\n",
        "    print(\"❌ PROBLEM: Solicitation vector is completely zero!\")\n",
        "\n",
        "    # Check if any solicitation words are in vocabulary\n",
        "    solicitation_words = solicitation_text.lower().split()\n",
        "    vocab_set = set(vocab)\n",
        "    matching_words = [word for word in solicitation_words if word in vocab_set]\n",
        "    print(f\"Words from solicitation found in vocabulary: {matching_words[:10]}\")"
      ],
      "metadata": {
        "id": "ESyu-SdGI7l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample researcher vector and compare\n",
        "researcher_data = np.load('/content/drive/MyDrive/datastore/v2_analysis/researcher_vectors.npz', allow_pickle=True)\n",
        "vectors = researcher_data['vectors']\n",
        "researcher_ids = researcher_data['researcher_ids']\n",
        "\n",
        "print(f\"\\n=== SAMPLE RESEARCHER VECTOR ===\")\n",
        "sample_vector = vectors[0]\n",
        "print(f\"Researcher vector shape: {sample_vector.shape}\")\n",
        "print(f\"Researcher vector sum: {sample_vector.sum()}\")\n",
        "print(f\"Non-zero elements: {np.count_nonzero(sample_vector)}\")\n",
        "\n",
        "# Try manual similarity calculation\n",
        "if solicitation_vector.sum() > 0 and sample_vector.sum() > 0:\n",
        "    similarity = cosine_similarity(solicitation_vector, sample_vector.reshape(1, -1))[0][0]\n",
        "    print(f\"Manual similarity calculation: {similarity}\")\n",
        "else:\n",
        "    print(\"❌ Cannot calculate similarity - one vector is zero\")"
      ],
      "metadata": {
        "id": "FcsXkXabzcoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_weighted_expertise_document(self, researcher_papers: pd.DataFrame) -> str:\n",
        "    \"\"\"Create weighted expertise document using extracted keywords from topic names.\"\"\"\n",
        "    import re\n",
        "\n",
        "    # Common stop words to remove\n",
        "    stop_words = {'and', 'in', 'of', 'for', 'the', 'a', 'an', 'to', 'with', 'on', 'at', 'by'}\n",
        "\n",
        "    topic_words = []\n",
        "\n",
        "    for _, paper in researcher_papers.iterrows():\n",
        "        topics = self.parse_curated_topics(paper['curated_topics'])\n",
        "        publication_year = paper['publication_year']\n",
        "        citations = paper['cited_by_count'] if pd.notna(paper['cited_by_count']) else 0\n",
        "\n",
        "        for topic in topics:\n",
        "            if topic.get('type') == 'topic':\n",
        "                topic_score = topic.get('score', 0)\n",
        "                topic_name = topic.get('name', '').strip()\n",
        "\n",
        "                if topic_name and topic_score > 0:\n",
        "                    # Extract keywords from topic name\n",
        "                    keywords = self.extract_keywords_from_topic(topic_name, stop_words)\n",
        "\n",
        "                    if keywords:\n",
        "                        repetition_count = self.calculate_topic_repetition_weight(\n",
        "                            topic_score, publication_year, citations\n",
        "                        )\n",
        "\n",
        "                        if repetition_count == 0:\n",
        "                            repetition_count = 1\n",
        "\n",
        "                        # Add keywords repeated according to weight\n",
        "                        for keyword in keywords:\n",
        "                            topic_words.extend([keyword] * repetition_count)\n",
        "\n",
        "    document = ' '.join(topic_words)\n",
        "\n",
        "    # Fallback if document is too short\n",
        "    if len(document.split()) < 5:\n",
        "        fallback_keywords = []\n",
        "        for _, paper in researcher_papers.iterrows():\n",
        "            topics = self.parse_curated_topics(paper['curated_topics'])\n",
        "            for topic in topics:\n",
        "                if topic.get('type') == 'topic':\n",
        "                    topic_name = topic.get('name', '').strip()\n",
        "                    if topic_name:\n",
        "                        keywords = self.extract_keywords_from_topic(topic_name, stop_words)\n",
        "                        fallback_keywords.extend(keywords)\n",
        "\n",
        "        if fallback_keywords:\n",
        "            document = ' '.join(list(set(fallback_keywords))[:20])\n",
        "\n",
        "    return document\n",
        "\n",
        "def extract_keywords_from_topic(self, topic_name: str, stop_words: set) -> List[str]:\n",
        "    \"\"\"Extract meaningful keywords from a topic name.\"\"\"\n",
        "    import re\n",
        "\n",
        "    # Clean and split the topic name\n",
        "    # Remove special characters and normalize\n",
        "    cleaned = re.sub(r'[^\\w\\s-]', ' ', topic_name.lower())\n",
        "    words = cleaned.split()\n",
        "\n",
        "    # Filter out stop words and very short words\n",
        "    keywords = []\n",
        "    for word in words:\n",
        "        word = word.strip('-')\n",
        "        if (len(word) >= 3 and\n",
        "            word not in stop_words and\n",
        "            not word.isdigit()):\n",
        "            keywords.append(word)\n",
        "\n",
        "    # Keep only meaningful keywords (max 5 per topic)\n",
        "    return keywords[:5]"
      ],
      "metadata": {
        "id": "J7o_6XsgJQ3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "id": "KUW2q2lmf838"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# COMPLETE UNIFIED RESEARCHER MATCHING & DREAM TEAM SYSTEM\n",
        "# ==============================================================================\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Try to import Groq API (optional)\n",
        "try:\n",
        "    from groq import Groq\n",
        "    GROQ_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GROQ_AVAILABLE = False\n",
        "    print(\"⚠️ Groq library not installed. GAP analysis will be basic.\")\n",
        "    print(\"   Install with: pip install groq\")\n",
        "\n",
        "# Try to import Google Colab userdata (optional)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    COLAB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    COLAB_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION - CHANGE THESE PATHS\n",
        "# ==============================================================================\n",
        "solicitation_file = \"/content/drive/MyDrive/datastore/NSF 23-506_ Expanding AI Innovation through Capacity Building and Partnerships (ExpandAI) _ NSF - National Science Foundation_enhanced_analysis.json\"\n",
        "researcher_data_dir = \"/content/drive/MyDrive/datastore/v2_DATA\"\n",
        "output_dir = \"/content/drive/MyDrive/datastore\"\n",
        "\n",
        "# ==============================================================================\n",
        "# GROQ API SETUP (OPTIONAL - FOR AI-POWERED STRATEGIC ANALYSIS)\n",
        "# ==============================================================================\n",
        "# 1. Get free API key at: https://console.groq.com/\n",
        "# 2. In Google Colab: Go to left sidebar > Secrets > Add new secret\n",
        "#    Name: GROQ_API_KEY, Value: your_api_key_here\n",
        "# 3. Or set environment variable: export GROQ_API_KEY=your_api_key_here\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# CORE CLASSES\n",
        "# ==============================================================================\n",
        "@dataclass\n",
        "class ResearcherMatch:\n",
        "    researcher_id: str\n",
        "    researcher_name: str\n",
        "    academic_expertise_score: float\n",
        "    s_sparse: float\n",
        "    s_dense: float\n",
        "    f_ge: float\n",
        "    final_affinity_score: float\n",
        "    total_papers: int\n",
        "    eligibility_status: str\n",
        "\n",
        "@dataclass\n",
        "class MatchingResults:\n",
        "    solicitation_title: str\n",
        "    eligible_researchers: int\n",
        "    total_researchers: int\n",
        "    top_matches: List[ResearcherMatch]\n",
        "    skills_analyzed: List[str]\n",
        "    processing_time_seconds: float\n",
        "\n",
        "@dataclass\n",
        "class DreamTeamReport:\n",
        "    team_members: List[Dict]\n",
        "    overall_coverage_score: float\n",
        "    skill_analysis: List[Dict]\n",
        "    strategic_analysis: str\n",
        "    selection_history: List[Dict]\n",
        "    generated_at: str\n",
        "\n",
        "class UnifiedResearcherSystem:\n",
        "    \"\"\"\n",
        "    Complete system that combines researcher matching with dream team assembly\n",
        "    and strategic GAP analysis using Claude API.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.alpha = 0.7  # TF-IDF weight\n",
        "        self.beta = 0.3   # Dense weight\n",
        "        self.groq_client = None\n",
        "        self.setup_groq_api()\n",
        "        self.load_data()\n",
        "\n",
        "    def setup_groq_api(self):\n",
        "        \"\"\"Initialize Groq API client if available.\"\"\"\n",
        "        if not GROQ_AVAILABLE:\n",
        "            print(\"⚠️ Groq API not available. Strategic analysis will be basic.\")\n",
        "            print(\"   Install with: pip install groq\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            if COLAB_AVAILABLE:\n",
        "                api_key = userdata.get('GROQ_API_KEY')\n",
        "            else:\n",
        "                api_key = os.environ.get('GROQ_API_KEY')\n",
        "\n",
        "            if api_key:\n",
        "                self.groq_client = Groq(api_key=api_key)\n",
        "                print(\"✅ Groq API client initialized for strategic analysis\")\n",
        "            else:\n",
        "                print(\"⚠️ Groq API key not found. Set GROQ_API_KEY.\")\n",
        "                print(\"   Get free API key at: https://console.groq.com/\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Groq API setup failed: {e}. Strategic analysis will be basic.\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load all preprocessed researcher data.\"\"\"\n",
        "        print(\"📂 Loading preprocessed data...\")\n",
        "\n",
        "        # Handle pickle loading issue - define dummy class if needed\n",
        "        try:\n",
        "            # Try to load TF-IDF model normally first\n",
        "            with open(self.data_dir / 'tfidf_model.pkl', 'rb') as f:\n",
        "                self.tfidf_model = pickle.load(f)\n",
        "        except AttributeError as e:\n",
        "            if \"ResearcherProfileProcessor\" in str(e):\n",
        "                print(\"🔧 Fixing pickle compatibility issue...\")\n",
        "                # Create a dummy class to handle the pickle loading\n",
        "                import sys\n",
        "                class ResearcherProfileProcessor:\n",
        "                    def comma_tokenizer(self, text: str):\n",
        "                        return [token.strip() for token in text.split(',') if token.strip()]\n",
        "\n",
        "                # Add the class to the current module\n",
        "                sys.modules[__name__].ResearcherProfileProcessor = ResearcherProfileProcessor\n",
        "\n",
        "                # Try loading again\n",
        "                with open(self.data_dir / 'tfidf_model.pkl', 'rb') as f:\n",
        "                    self.tfidf_model = pickle.load(f)\n",
        "                print(\"✅ TF-IDF model loaded successfully\")\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "        # Load researcher vectors\n",
        "        researcher_data = np.load(self.data_dir / 'researcher_vectors.npz', allow_pickle=True)\n",
        "        vectors = researcher_data['vectors']\n",
        "        researcher_ids = researcher_data['researcher_ids']\n",
        "        self.researcher_vectors = dict(zip(researcher_ids, vectors))\n",
        "\n",
        "        # Load paper embeddings\n",
        "        conceptual_data = np.load(self.data_dir / 'conceptual_profiles.npz', allow_pickle=True)\n",
        "        embeddings = conceptual_data['embeddings']\n",
        "        work_ids = conceptual_data['work_ids']\n",
        "        self.conceptual_profiles = dict(zip(work_ids, embeddings))\n",
        "\n",
        "        # Load evidence index\n",
        "        with open(self.data_dir / 'evidence_index.json', 'r') as f:\n",
        "            self.evidence_index = json.load(f)\n",
        "\n",
        "        # Load metadata\n",
        "        self.researcher_metadata = pd.read_parquet(self.data_dir / 'researcher_metadata.parquet')\n",
        "\n",
        "        # Load sentence model\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        print(f\"✅ Loaded {len(self.researcher_vectors)} researchers\")\n",
        "        self.diagnose_data_quality()\n",
        "\n",
        "    def diagnose_data_quality(self):\n",
        "        \"\"\"Diagnose potential data quality issues.\"\"\"\n",
        "        print(\"\\n🔍 DIAGNOSING DATA QUALITY\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Check TF-IDF model\n",
        "        try:\n",
        "            vocab_size = len(self.tfidf_model.get_feature_names_out())\n",
        "            print(f\"TF-IDF vocabulary size: {vocab_size}\")\n",
        "\n",
        "            vocab = list(self.tfidf_model.get_feature_names_out())\n",
        "            print(f\"First 20 TF-IDF features: {vocab[:20]}\")\n",
        "            print(f\"Last 20 TF-IDF features: {vocab[-20:]}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error accessing TF-IDF vocabulary: {e}\")\n",
        "\n",
        "        # Check researcher vectors\n",
        "        print(f\"Researcher vectors: {len(self.researcher_vectors)}\")\n",
        "        if self.researcher_vectors:\n",
        "            sample_vector = next(iter(self.researcher_vectors.values()))\n",
        "            print(f\"Vector dimensions: {sample_vector.shape}\")\n",
        "            print(f\"Sample vector sum: {sample_vector.sum():.4f}\")\n",
        "\n",
        "        # Check conceptual profiles\n",
        "        print(f\"Conceptual profiles: {len(self.conceptual_profiles)}\")\n",
        "        if self.conceptual_profiles:\n",
        "            sample_embedding = next(iter(self.conceptual_profiles.values()))\n",
        "            print(f\"Embedding dimensions: {sample_embedding.shape}\")\n",
        "\n",
        "        # Check evidence index\n",
        "        print(f\"Evidence index researchers: {len(self.evidence_index)}\")\n",
        "\n",
        "        # Check overlap between evidence index and conceptual profiles\n",
        "        all_evidence_papers = set()\n",
        "        for researcher_papers in self.evidence_index.values():\n",
        "            for topic_papers in researcher_papers.values():\n",
        "                all_evidence_papers.update(topic_papers)\n",
        "\n",
        "        conceptual_papers = set(self.conceptual_profiles.keys())\n",
        "        overlap = all_evidence_papers.intersection(conceptual_papers)\n",
        "\n",
        "        print(f\"Papers in evidence index: {len(all_evidence_papers)}\")\n",
        "        print(f\"Papers with embeddings: {len(conceptual_papers)}\")\n",
        "        print(f\"Overlap: {len(overlap)}\")\n",
        "\n",
        "        if len(overlap) == 0:\n",
        "            print(\"❌ CRITICAL: No overlap between evidence index and conceptual profiles!\")\n",
        "        elif len(overlap) < len(all_evidence_papers) * 0.5:\n",
        "            print(\"⚠️ WARNING: Low overlap between evidence index and conceptual profiles!\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    def filter_eligibility(self, solicitation_obj, researchers):\n",
        "        \"\"\"Simple eligibility filtering.\"\"\"\n",
        "        eligible = set(researchers)\n",
        "        eligibility = getattr(solicitation_obj, 'eligibility', {})\n",
        "\n",
        "        # Early-career filter\n",
        "        if eligibility and any('early' in str(v).lower() for v in eligibility.values() if v):\n",
        "            early_career = self.researcher_metadata[\n",
        "                self.researcher_metadata['first_publication_year'] >= 2015\n",
        "            ]['researcher_openalex_id'].tolist()\n",
        "            eligible = eligible.intersection(set(early_career))\n",
        "            print(f\"   Applied early-career filter: {len(eligible)} remain\")\n",
        "\n",
        "        # Grant experience filter\n",
        "        if eligibility and any('grant' in str(v).lower() or 'funding' in str(v).lower() for v in eligibility.values() if v):\n",
        "            experienced = self.researcher_metadata[\n",
        "                self.researcher_metadata['grant_experience_factor'] > 0\n",
        "            ]['researcher_openalex_id'].tolist()\n",
        "            eligible = eligible.intersection(set(experienced))\n",
        "            print(f\"   Applied grant experience filter: {len(eligible)} remain\")\n",
        "\n",
        "        return list(eligible)\n",
        "\n",
        "    def extract_keywords_from_skills(self, skills: List[str]) -> List[str]:\n",
        "        \"\"\"Extract keywords from solicitation skills using same logic as researcher topics.\"\"\"\n",
        "        import re\n",
        "\n",
        "        stop_words = {'and', 'in', 'of', 'for', 'the', 'a', 'an', 'to', 'with', 'on', 'at', 'by',\n",
        "                      'expertise', 'experience', 'knowledge', 'ability', 'skills', 'understanding',\n",
        "                      'capacity', 'proficiency', 'e.g.', 'eg', 'including', 'such', 'as'}\n",
        "\n",
        "        all_keywords = []\n",
        "\n",
        "        for skill in skills:\n",
        "            # Clean and split\n",
        "            cleaned = re.sub(r'[^\\w\\s-]', ' ', skill.lower())\n",
        "            words = cleaned.split()\n",
        "\n",
        "            # Extract meaningful keywords\n",
        "            for word in words:\n",
        "                word = word.strip('-')\n",
        "                if (len(word) >= 3 and\n",
        "                    word not in stop_words and\n",
        "                    not word.isdigit()):\n",
        "                    all_keywords.append(word)\n",
        "\n",
        "        return all_keywords\n",
        "\n",
        "    def score_researcher(self, researcher_id, skills, solicitation_embedding, debug_mode=False):\n",
        "        \"\"\"Score a single researcher with optional debug output.\"\"\"\n",
        "        try:\n",
        "            # Get metadata\n",
        "            researcher_row = self.researcher_metadata[\n",
        "                self.researcher_metadata['researcher_openalex_id'] == researcher_id\n",
        "            ]\n",
        "            if researcher_row.empty:\n",
        "                return None\n",
        "\n",
        "            researcher_name = researcher_row.iloc[0]['researcher_name']\n",
        "            total_papers = int(researcher_row.iloc[0]['total_papers'])\n",
        "            grant_factor = researcher_row.iloc[0]['grant_experience_factor']\n",
        "\n",
        "            # Extract keywords and format with commas (same as researcher documents)\n",
        "            solicitation_keywords = self.extract_keywords_from_skills(skills)\n",
        "            solicitation_text = ', '.join(solicitation_keywords)\n",
        "\n",
        "            if debug_mode:\n",
        "                print(f\"DEBUG - Researcher: {researcher_name}\")\n",
        "                print(f\"  Original skills count: {len(skills)}\")\n",
        "                print(f\"  Extracted keywords: {solicitation_keywords[:10]}...\")\n",
        "                print(f\"  Solicitation text: {solicitation_text[:100]}...\")\n",
        "\n",
        "            # Calculate sparse score (TF-IDF)\n",
        "            if researcher_id not in self.researcher_vectors:\n",
        "                if debug_mode:\n",
        "                    print(f\"  WARNING: No TF-IDF vector for {researcher_id}\")\n",
        "                s_sparse = 0.0\n",
        "            else:\n",
        "                try:\n",
        "                    solicitation_vector = self.tfidf_model.transform([solicitation_text])\n",
        "                    researcher_vector = self.researcher_vectors[researcher_id].reshape(1, -1)\n",
        "\n",
        "                    if debug_mode:\n",
        "                        print(f\"  Solicitation vector shape: {solicitation_vector.shape}\")\n",
        "                        print(f\"  Researcher vector shape: {researcher_vector.shape}\")\n",
        "                        print(f\"  Solicitation vector sum: {solicitation_vector.sum()}\")\n",
        "                        print(f\"  Researcher vector sum: {researcher_vector.sum()}\")\n",
        "                        if hasattr(solicitation_vector, 'nnz'):\n",
        "                             print(f\"  Solicitation non-zero elements: {solicitation_vector.nnz}\")\n",
        "\n",
        "\n",
        "                    if solicitation_vector.sum() == 0 or researcher_vector.sum() == 0:\n",
        "                        if debug_mode:\n",
        "                            print(f\"  WARNING: Zero vector detected\")\n",
        "                        s_sparse = 0.0\n",
        "                    else:\n",
        "                        similarity = cosine_similarity(solicitation_vector, researcher_vector)[0][0]\n",
        "                        s_sparse = float(similarity * 100)\n",
        "                        if debug_mode:\n",
        "                            print(f\"  TF-IDF similarity: {similarity}\")\n",
        "                except Exception as tfidf_error:\n",
        "                    if debug_mode:\n",
        "                        print(f\"  TF-IDF ERROR: {tfidf_error}\")\n",
        "                    s_sparse = 0.0\n",
        "\n",
        "            # Calculate dense score (max similarity across papers)\n",
        "            s_dense = 0.0\n",
        "            papers_checked = 0\n",
        "            papers_found = 0\n",
        "            if researcher_id in self.evidence_index:\n",
        "                # Get all papers for this researcher from evidence index\n",
        "                researcher_papers = []\n",
        "                for topic_papers in self.evidence_index[researcher_id].values():\n",
        "                    researcher_papers.extend(topic_papers)\n",
        "                researcher_papers = list(set(researcher_papers))  # Remove duplicates\n",
        "\n",
        "                if debug_mode:\n",
        "                    print(f\"  Papers from evidence index: {len(researcher_papers)}\")\n",
        "\n",
        "                max_sim = 0.0\n",
        "                for paper_id in researcher_papers:\n",
        "                    papers_checked += 1\n",
        "                    if paper_id in self.conceptual_profiles:\n",
        "                        papers_found += 1\n",
        "                        paper_embedding = self.conceptual_profiles[paper_id]\n",
        "                        try:\n",
        "                            sim = cosine_similarity(\n",
        "                                solicitation_embedding.reshape(1, -1),\n",
        "                                paper_embedding.reshape(1, -1)\n",
        "                            )[0][0]\n",
        "                            max_sim = max(max_sim, sim)\n",
        "                        except Exception as dense_error:\n",
        "                            if debug_mode:\n",
        "                                print(f\"  Dense similarity error for paper {paper_id}: {dense_error}\")\n",
        "\n",
        "                s_dense = float(max_sim * 100)\n",
        "                if debug_mode:\n",
        "                    print(f\"  Papers checked: {papers_checked}, Papers with embeddings: {papers_found}\")\n",
        "                    print(f\"  Max dense similarity: {max_sim}\")\n",
        "            else:\n",
        "                if debug_mode:\n",
        "                    print(f\"  No evidence index entry for researcher\")\n",
        "\n",
        "            # Calculate final scores\n",
        "            f_ge = max(1.0, min(3.0, 1.0 + (grant_factor * 0.2)))\n",
        "            academic_expertise = (self.alpha * s_sparse) + (self.beta * s_dense)\n",
        "            final_score = academic_expertise * f_ge\n",
        "\n",
        "            if debug_mode:\n",
        "                print(f\"  Final scores - Sparse: {s_sparse:.2f}, Dense: {s_dense:.2f}, Grant: {f_ge:.2f}\")\n",
        "                print(f\"  Academic: {academic_expertise:.2f}, Final: {final_score:.2f}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            return ResearcherMatch(\n",
        "                researcher_id=researcher_id,\n",
        "                researcher_name=researcher_name,\n",
        "                academic_expertise_score=academic_expertise,\n",
        "                s_sparse=s_sparse,\n",
        "                s_dense=s_dense,\n",
        "                f_ge=f_ge,\n",
        "                final_affinity_score=final_score,\n",
        "                total_papers=total_papers,\n",
        "                eligibility_status=\"Eligible\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            if debug_mode:\n",
        "                print(f\"ERROR scoring {researcher_id}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def search_researchers(self, solicitation_obj, debug_first_n=3):\n",
        "        \"\"\"Main search function to rank researchers.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\n🔍 ANALYZING: {solicitation_obj.title[:80]}...\")\n",
        "\n",
        "        # Get skills\n",
        "        skills = solicitation_obj.required_skills_checklist\n",
        "        print(f\"📊 Skills to analyze: {len(skills)}\")\n",
        "\n",
        "        # Create solicitation embedding\n",
        "        solicitation_text = f\"{solicitation_obj.title}. {solicitation_obj.abstract}\"\n",
        "        solicitation_embedding = self.sentence_model.encode(solicitation_text)\n",
        "\n",
        "        # Get all researchers\n",
        "        all_researchers = list(self.researcher_metadata['researcher_openalex_id'])\n",
        "        print(f\"👥 Total researchers: {len(all_researchers)}\")\n",
        "\n",
        "        # Filter by eligibility\n",
        "        eligible = self.filter_eligibility(solicitation_obj, all_researchers)\n",
        "        print(f\"✅ Eligible researchers: {len(eligible)}\")\n",
        "\n",
        "        # Score all eligible researchers\n",
        "        print(\"🔄 Calculating scores...\")\n",
        "        matches = []\n",
        "        debug_count = 0\n",
        "\n",
        "        for researcher_id in eligible:\n",
        "            # Debug first N researchers to avoid spam\n",
        "            debug_mode = debug_count < debug_first_n\n",
        "            if debug_mode:\n",
        "                debug_count += 1\n",
        "\n",
        "            result = self.score_researcher(researcher_id, skills, solicitation_embedding, debug_mode)\n",
        "\n",
        "            if result:\n",
        "                matches.append(result)\n",
        "\n",
        "        # Sort by score\n",
        "        matches.sort(key=lambda x: x.final_affinity_score, reverse=True)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return MatchingResults(\n",
        "            solicitation_title=solicitation_obj.title,\n",
        "            eligible_researchers=len(eligible),\n",
        "            total_researchers=len(all_researchers),\n",
        "            top_matches=matches,\n",
        "            skills_analyzed=skills,\n",
        "            processing_time_seconds=round(processing_time, 2)\n",
        "        )\n",
        "\n",
        "    def create_affinity_matrix(self, matching_results, top_n_researchers=20):\n",
        "        \"\"\"Create an affinity matrix from matching results.\"\"\"\n",
        "        print(f\"\\n📊 Creating affinity matrix for top {top_n_researchers} researchers...\")\n",
        "\n",
        "        # Get top researchers\n",
        "        top_matches = matching_results.top_matches[:top_n_researchers]\n",
        "        researcher_names = [match.researcher_name for match in top_matches]\n",
        "        skills = matching_results.skills_analyzed\n",
        "\n",
        "        # Create matrix: researchers × skills\n",
        "        affinity_matrix = np.zeros((len(top_matches), len(skills)))\n",
        "\n",
        "        for i, match in enumerate(top_matches):\n",
        "            # For each researcher, calculate their affinity to each skill\n",
        "            researcher_id = match.researcher_id\n",
        "\n",
        "            for j, skill in enumerate(skills):\n",
        "                # Calculate skill-specific affinity score\n",
        "                skill_keywords = self.extract_keywords_from_skills([skill])\n",
        "                skill_text = ', '.join(skill_keywords)\n",
        "\n",
        "                try:\n",
        "                    # TF-IDF similarity for this specific skill\n",
        "                    skill_vector = self.tfidf_model.transform([skill_text])\n",
        "                    researcher_vector = self.researcher_vectors[researcher_id].reshape(1, -1)\n",
        "                    sparse_sim = cosine_similarity(skill_vector, researcher_vector)[0][0] * 100\n",
        "\n",
        "                    # Dense similarity (using overall solicitation embedding as proxy)\n",
        "                    # In practice, you might want to create skill-specific embeddings\n",
        "                    # For simplicity, let's use the max dense score from the overall matching\n",
        "                    dense_sim = match.s_dense # Use the pre-calculated max dense score for the researcher\n",
        "\n",
        "\n",
        "                    # Combined affinity score for this skill\n",
        "                    skill_affinity = (self.alpha * sparse_sim) + (self.beta * dense_sim)\n",
        "                    affinity_matrix[i, j] = max(0, skill_affinity)  # Ensure non-negative\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Fallback: use overall academic score\n",
        "                    affinity_matrix[i, j] = match.academic_expertise_score\n",
        "\n",
        "        # Create DataFrame\n",
        "        affinity_df = pd.DataFrame(\n",
        "            affinity_matrix,\n",
        "            index=researcher_names,\n",
        "            columns=[f\"Skill_{i+1}: {skill}\" for i, skill in enumerate(skills)]\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Created affinity matrix: {affinity_df.shape[0]} researchers × {affinity_df.shape[1]} skills\")\n",
        "        return affinity_df\n",
        "\n",
        "    def calculate_team_coverage(self, affinity_df, team_indices):\n",
        "        \"\"\"Calculate team coverage scores for all skills.\"\"\"\n",
        "        if not team_indices:\n",
        "            return np.array([0.0] * affinity_df.shape[1]), 0.0\n",
        "        team_affinities = affinity_df.iloc[team_indices]\n",
        "        skill_coverages = team_affinities.max(axis=0).values\n",
        "        return skill_coverages, np.mean(skill_coverages)\n",
        "\n",
        "    def calculate_marginal_gain(self, affinity_df, current_team_indices, candidate_index):\n",
        "        \"\"\"Calculate the marginal gain of adding a candidate to the team.\"\"\"\n",
        "        _, current_coverage = self.calculate_team_coverage(affinity_df, current_team_indices)\n",
        "        _, new_coverage = self.calculate_team_coverage(affinity_df, current_team_indices + [candidate_index])\n",
        "        return new_coverage - current_coverage\n",
        "\n",
        "    def dream_team_hybrid_strategy(self, affinity_df, guaranteed_top_n=2, max_team_size=4):\n",
        "        \"\"\"\n",
        "        Hybrid approach: Lock in top N performers, then optimize coverage for remaining slots.\n",
        "        This ensures we get researchers with proven grant experience and keyword matching.\n",
        "        \"\"\"\n",
        "        print(f\"\\n🎯 Running Hybrid Dream Team Strategy\")\n",
        "        print(f\"   Step 1: Lock in top {guaranteed_top_n} performers\")\n",
        "        print(f\"   Step 2: Optimize coverage for remaining {max_team_size - guaranteed_top_n} slots\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Phase 1: Lock in top performers (they earned their ranking!)\n",
        "        researcher_averages = affinity_df.mean(axis=1).sort_values(ascending=False)\n",
        "        top_performers = researcher_averages.head(guaranteed_top_n)\n",
        "\n",
        "        selected_indices = []\n",
        "        selection_history = []\n",
        "\n",
        "        print(\"🔒 LOCKING IN TOP PERFORMERS:\")\n",
        "        for i, (name, avg_score) in enumerate(top_performers.items()):\n",
        "            idx = affinity_df.index.get_loc(name)\n",
        "            selected_indices.append(idx)\n",
        "            role = \"PI\" if i == 0 else f\"Co-PI {i}\"\n",
        "\n",
        "            selection_history.append({\n",
        "                'step': i + 1,\n",
        "                'action': f'Lock {role}',\n",
        "                'researcher_name': name,\n",
        "                'reason': f'Top {i+1} performer (avg: {avg_score:.2f}, proven track record)',\n",
        "                'team_coverage': 0  # Will calculate after\n",
        "            })\n",
        "            print(f\"   ✅ {name} ({role}) - Avg Score: {avg_score:.2f}\")\n",
        "\n",
        "        # Calculate coverage after locking in top performers\n",
        "        _, coverage_after_top = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "        print(f\"   📊 Coverage after top {guaranteed_top_n}: {coverage_after_top:.2f}\")\n",
        "\n",
        "        # Update coverage in history\n",
        "        for entry in selection_history:\n",
        "            entry['team_coverage'] = coverage_after_top\n",
        "\n",
        "        # Phase 2: Optimize remaining slots for coverage\n",
        "        print(f\"\\n🎯 OPTIMIZING REMAINING {max_team_size - guaranteed_top_n} SLOTS FOR COVERAGE:\")\n",
        "        n_researchers = len(affinity_df)\n",
        "\n",
        "        for step in range(guaranteed_top_n + 1, max_team_size + 1):\n",
        "            # Calculate marginal gains for all remaining researchers\n",
        "            gains = [(idx, self.calculate_marginal_gain(affinity_df, selected_indices, idx))\n",
        "                     for idx in range(n_researchers) if idx not in selected_indices]\n",
        "\n",
        "            if not gains:\n",
        "                print(f\"   ⚠️ No more candidates available\")\n",
        "                break\n",
        "\n",
        "            # Sort by marginal gain and show top candidates\n",
        "            top_candidates = sorted(gains, key=lambda x: x[1], reverse=True)[:5]\n",
        "            best_candidate_idx, best_marginal_gain = top_candidates[0]\n",
        "\n",
        "            print(f\"   📊 Top candidates for slot {step - guaranteed_top_n}:\")\n",
        "            for i, (idx, gain) in enumerate(top_candidates):\n",
        "                candidate_name = affinity_df.index[idx]\n",
        "                candidate_avg = affinity_df.iloc[idx].mean()\n",
        "                marker = \"👑\" if i == 0 else f\"  {i+1}.\"\n",
        "                print(f\"      {marker} {candidate_name} (Avg: {candidate_avg:.2f}, Coverage Gain: +{gain:.2f})\")\n",
        "\n",
        "            # Add the best candidate for coverage\n",
        "            if best_marginal_gain > 0.1:  # Lower threshold since we have strong foundation\n",
        "                selected_indices.append(best_candidate_idx)\n",
        "                _, new_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "\n",
        "                selection_history.append({\n",
        "                    'step': step,\n",
        "                    'action': 'Add for Coverage',\n",
        "                    'researcher_name': affinity_df.index[best_candidate_idx],\n",
        "                    'reason': f'Best coverage gain (+{best_marginal_gain:.2f})',\n",
        "                    'team_coverage': new_coverage\n",
        "                })\n",
        "                print(f\"   ✅ Added: {affinity_df.index[best_candidate_idx]} (New Coverage: {new_coverage:.2f})\")\n",
        "            else:\n",
        "                print(f\"   🛑 Stopping: Marginal gain {best_marginal_gain:.2f} too small\")\n",
        "                break\n",
        "\n",
        "        final_coverage = self.calculate_team_coverage(affinity_df, selected_indices)[1]\n",
        "        print(f\"\\n🎯 Final Hybrid Team ({len(selected_indices)} members) with {final_coverage:.2f} coverage\")\n",
        "        print(f\"   Strategy: Top {guaranteed_top_n} performers + coverage optimization\")\n",
        "\n",
        "        return selected_indices, selection_history\n",
        "\n",
        "    def dream_team_greedy_algorithm(self, affinity_df, min_team_size=2, max_team_size=4, marginal_threshold=0.25):\n",
        "        \"\"\"Implement the greedy algorithm to select the best team.\"\"\"\n",
        "        print(\"\\n🎯 Running Pure Greedy Algorithm...\")\n",
        "        print(\"=\" * 50)\n",
        "        n_researchers = len(affinity_df)\n",
        "        selected_indices = []\n",
        "        selection_history = []\n",
        "\n",
        "        # Step 1: Select the best overall researcher as PI\n",
        "        best_researcher_pos = affinity_df.mean(axis=1).idxmax()\n",
        "        best_researcher_loc = affinity_df.index.get_loc(best_researcher_pos)\n",
        "        selected_indices.append(best_researcher_loc)\n",
        "        _, initial_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "\n",
        "        selection_history.append({\n",
        "            'step': 1, 'action': 'Select PI',\n",
        "            'researcher_name': affinity_df.index[best_researcher_loc],\n",
        "            'reason': 'Highest average affinity score',\n",
        "            'team_coverage': initial_coverage\n",
        "        })\n",
        "        print(f\"🏆 Step 1 - PI Selection: {affinity_df.index[best_researcher_loc]} (Coverage: {initial_coverage:.2f})\")\n",
        "\n",
        "        # Step 2-N: Iteratively add members with the highest marginal gain\n",
        "        for step in range(2, max_team_size + 1):\n",
        "            gains = [(idx, self.calculate_marginal_gain(affinity_df, selected_indices, idx))\n",
        "                     for idx in range(n_researchers) if idx not in selected_indices]\n",
        "            if not gains:\n",
        "                break\n",
        "\n",
        "            best_candidate_idx, best_marginal_gain = max(gains, key=lambda item: item[1])\n",
        "\n",
        "            # Show top 3 candidates for transparency\n",
        "            top_candidates = sorted(gains, key=lambda x: x[1], reverse=True)[:3]\n",
        "            print(f\"   📊 Top candidates for Step {step}:\")\n",
        "            for i, (idx, gain) in enumerate(top_candidates):\n",
        "                candidate_name = affinity_df.index[idx]\n",
        "                print(f\"      {i+1}. {candidate_name} (Marginal gain: +{gain:.2f})\")\n",
        "\n",
        "            # More flexible stopping criteria\n",
        "            should_add = (\n",
        "                best_marginal_gain > marginal_threshold or  # Significant gain\n",
        "                len(selected_indices) < min_team_size or    # Haven't reached minimum\n",
        "                (step <= 4 and best_marginal_gain > 0.1)   # Force at least 4 if minimal gain\n",
        "            )\n",
        "\n",
        "            if should_add:\n",
        "                selected_indices.append(best_candidate_idx)\n",
        "                _, new_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "                selection_history.append({\n",
        "                    'step': step, 'action': 'Add Member',\n",
        "                    'researcher_name': affinity_df.index[best_candidate_idx],\n",
        "                    'reason': f'Maximum marginal gain (+{best_marginal_gain:.2f})',\n",
        "                    'team_coverage': new_coverage\n",
        "                })\n",
        "                print(f\"✅ Step {step} - Added: {affinity_df.index[best_candidate_idx]} (New Coverage: {new_coverage:.2f})\")\n",
        "            else:\n",
        "                print(f\"🛑 Step {step} - Stopping: Marginal gain {best_marginal_gain:.2f} below threshold {marginal_threshold}\")\n",
        "                break\n",
        "\n",
        "        final_coverage = self.calculate_team_coverage(affinity_df, selected_indices)[1]\n",
        "        print(f\"\\n🎯 Final Dream Team ({len(selected_indices)} members) with {final_coverage:.2f} coverage.\")\n",
        "        return selected_indices, selection_history\n",
        "\n",
        "    def generate_coverage_report(self, affinity_df, team_indices, skills_list):\n",
        "        \"\"\"Generate a detailed coverage report for the selected team.\"\"\"\n",
        "        skill_coverages, overall_coverage = self.calculate_team_coverage(affinity_df, team_indices)\n",
        "\n",
        "        team_members = []\n",
        "        for idx in team_indices:\n",
        "            scores = affinity_df.iloc[idx]\n",
        "            top_skills = [{'skill': skills_list[i], 'score': scores[i]}\n",
        "                         for i in scores.argsort()[-3:][::-1]]\n",
        "            team_members.append({\n",
        "                'name': affinity_df.index[idx],\n",
        "                'avg_affinity': scores.mean(),\n",
        "                'top_skills': top_skills\n",
        "            })\n",
        "\n",
        "        skill_analysis = []\n",
        "        for i, (skill, coverage) in enumerate(zip(skills_list, skill_coverages)):\n",
        "            team_scores = affinity_df.iloc[team_indices, i]\n",
        "            if not team_scores.empty: # Handle case where team is empty or skill has no coverage\n",
        "                best_member_name = team_scores.idxmax()\n",
        "                expert_score = team_scores.max()\n",
        "            else:\n",
        "                best_member_name = \"N/A\"\n",
        "                expert_score = 0.0\n",
        "\n",
        "\n",
        "            skill_analysis.append({\n",
        "                'skill': skill,\n",
        "                'coverage_score': coverage,\n",
        "                'level': 'High' if coverage >= 70 else 'Medium' if coverage >= 40 else 'Low',\n",
        "                'expert': best_member_name,\n",
        "                'expert_score': expert_score\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'overall_coverage_score': overall_coverage,\n",
        "            'team_members': team_members,\n",
        "            'skill_analysis': skill_analysis\n",
        "        }\n",
        "\n",
        "    def generate_strategic_analysis(self, coverage_report, skills_list, solicitation_data):\n",
        "        \"\"\"Generate AI-powered gap analysis using Groq API.\"\"\"\n",
        "        if not self.groq_client:\n",
        "            # Basic fallback analysis\n",
        "            low_skills = [s['skill'] for s in coverage_report['skill_analysis'] if s['level'] == 'Low']\n",
        "            basic_analysis = f\"\"\"\n",
        "BASIC STRATEGIC ANALYSIS (Groq API not available):\n",
        "\n",
        "Team Coverage Score: {coverage_report['overall_coverage_score']:.2f}/100\n",
        "\n",
        "STRENGTHS:\n",
        "- Team assembled using advanced affinity matching\n",
        "- {len(coverage_report['team_members'])} selected team members\n",
        "- Covers {len(skills_list)} required skill areas\n",
        "\n",
        "POTENTIAL GAPS:\n",
        "{f\"- Low coverage areas: {', '.join(low_skills)}\" if low_skills else \"- No significant gaps identified\"}\n",
        "\n",
        "RECOMMENDATIONS:\n",
        "- Review low-coverage skills for additional expertise\n",
        "- Consider collaboration with external partners if needed\n",
        "- Leverage team members' top skills for proposal strength\n",
        "\"\"\"\n",
        "            return basic_analysis.strip()\n",
        "\n",
        "        print(\"🤖 Generating strategic analysis with Groq API...\")\n",
        "\n",
        "        # Create detailed prompt for Groq\n",
        "        team_info = \"\\n\".join([f\"- {member['name']} (avg affinity: {member['avg_affinity']:.2f})\"\n",
        "                              for member in coverage_report['team_members']])\n",
        "\n",
        "        low_skills = [s for s in coverage_report['skill_analysis'] if s['level'] == 'Low']\n",
        "        medium_skills = [s for s in coverage_report['skill_analysis'] if s['level'] == 'Medium']\n",
        "        high_skills = [s for s in coverage_report['skill_analysis'] if s['level'] == 'High']\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Analyze this research team's fit for the NSF solicitation titled: \"{solicitation_data.get('title', 'N/A')}\"\n",
        "\n",
        "TEAM COMPOSITION:\n",
        "{team_info}\n",
        "\n",
        "COVERAGE ANALYSIS:\n",
        "- Overall team coverage score: {coverage_report['overall_coverage_score']:.2f}/100\n",
        "- High coverage skills ({len(high_skills)}): {', '.join([s['skill'] for s in high_skills[:5]])}\n",
        "- Medium coverage skills ({len(medium_skills)}): {', '.join([s['skill'] for s in medium_skills[:5]])}\n",
        "- Low coverage skills ({len(low_skills)}): {', '.join([s['skill'] for s in low_skills])}\n",
        "\n",
        "Please provide a strategic analysis covering:\n",
        "1. Team strengths and competitive advantages, and why\n",
        "2. Critical gaps and risks, and strategic ways to combat that. Be assertive\n",
        "3. Specific recommendations for proposal development, intellectual and based on solicitation\n",
        "4. Potential collaboration or recruitment strategies, also suggest which areas of faculty should be targeted\n",
        "5. Overall competitiveness assessment\n",
        "\n",
        "Keep the analysis practical and actionable for proposal development.\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.groq_client.chat.completions.create(\n",
        "                model=\"llama3-70b-8192\",  # Free Groq model\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=2000,\n",
        "                temperature=0.5\n",
        "            )\n",
        "            analysis = response.choices[0].message.content\n",
        "            print(\"✅ Strategic analysis generated with Groq API\")\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Groq API analysis failed: {e}\")\n",
        "            # Fallback to basic analysis\n",
        "            low_skills_list = [s['skill'] for s in low_skills]\n",
        "            return f\"Groq API failed. Basic analysis: Team coverage is {coverage_report['overall_coverage_score']:.2f}/100. Review low-coverage skills: {', '.join(low_skills_list) if low_skills_list else 'None identified'}.\"\n",
        "\n",
        "    def dream_team_by_rankings(self, affinity_df, team_size=4):\n",
        "        \"\"\"Alternative approach: Simply select top N researchers by overall ranking.\"\"\"\n",
        "        print(f\"\\n📊 ALTERNATIVE: Dream Team by Rankings (Top {team_size})\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Sort by average affinity (overall performance)\n",
        "        researcher_averages = affinity_df.mean(axis=1).sort_values(ascending=False)\n",
        "        top_researchers = researcher_averages.head(team_size)\n",
        "\n",
        "        team_indices = [affinity_df.index.get_loc(name) for name in top_researchers.index]\n",
        "        _, coverage = self.calculate_team_coverage(affinity_df, team_indices)\n",
        "\n",
        "        print(\"Selected team (by ranking):\")\n",
        "        for i, (name, avg_score) in enumerate(top_researchers.items()):\n",
        "            role = \"PI\" if i == 0 else f\"Co-I {i}\"\n",
        "            print(f\"   {i+1}. {name} ({role}) - Avg Score: {avg_score:.2f}\")\n",
        "\n",
        "        print(f\"Team coverage by rankings: {coverage:.2f}\")\n",
        "        return team_indices, coverage\n",
        "\n",
        "    def compare_team_strategies(self, affinity_df, skills_list):\n",
        "        \"\"\"Compare different team selection strategies.\"\"\"\n",
        "        print(\"\\n🔬 COMPARING TEAM SELECTION STRATEGIES\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Strategy 1: Hybrid approach (lock top 2 + coverage optimization) - PREFERRED\n",
        "        hybrid_indices, hybrid_history = self.dream_team_hybrid_strategy(affinity_df, guaranteed_top_n=2, max_team_size=4)\n",
        "        hybrid_coverage = self.calculate_team_coverage(affinity_df, hybrid_indices)[1]\n",
        "\n",
        "        # Strategy 2: Pure greedy algorithm (optimized coverage)\n",
        "        greedy_indices, greedy_history = self.dream_team_greedy_algorithm(affinity_df, marginal_threshold=0.25, max_team_size=4)\n",
        "        greedy_coverage = self.calculate_team_coverage(affinity_df, greedy_indices)[1]\n",
        "\n",
        "        # Strategy 3: Top performers by ranking\n",
        "        ranking_indices, ranking_coverage = self.dream_team_by_rankings(affinity_df, team_size=4)\n",
        "\n",
        "        print(f\"\\n📊 STRATEGY COMPARISON:\")\n",
        "        print(f\"🎯 Hybrid Strategy ({len(hybrid_indices)} members): {hybrid_coverage:.2f} coverage\")\n",
        "        print(f\"🤖 Pure Greedy ({len(greedy_indices)} members): {greedy_coverage:.2f} coverage\")\n",
        "        print(f\"📈 Top Rankings (4 members): {ranking_coverage:.2f} coverage\")\n",
        "\n",
        "        # Show detailed comparison\n",
        "        strategies = [\n",
        "            (\"HYBRID (Top 2 + Coverage)\", hybrid_indices, hybrid_coverage),\n",
        "            (\"PURE GREEDY\", greedy_indices, greedy_coverage),\n",
        "            (\"TOP RANKINGS\", ranking_indices, ranking_coverage)\n",
        "        ]\n",
        "\n",
        "        for strategy_name, indices, coverage in strategies:\n",
        "            print(f\"\\n{strategy_name}:\")\n",
        "            for i, idx in enumerate(indices):\n",
        "                name = affinity_df.index[idx]\n",
        "                avg_score = affinity_df.iloc[idx].mean()\n",
        "                role = \"PI\" if i == 0 else f\"Co-I {i}\"\n",
        "                print(f\"   {name} ({role}) - Avg Score: {avg_score:.2f}\")\n",
        "\n",
        "        # Determine which strategy to use - prioritize hybrid if competitive\n",
        "        best_coverage = max(hybrid_coverage, greedy_coverage, ranking_coverage)\n",
        "\n",
        "        if hybrid_coverage >= best_coverage - 1.0:  # Hybrid is within 1 point of best\n",
        "            print(f\"\\n🎯 RECOMMENDATION: Use Hybrid Strategy\")\n",
        "            print(f\"   ✅ Guarantees top performers (grant experience + keyword matching)\")\n",
        "            print(f\"   ✅ Optimizes coverage for remaining slots\")\n",
        "            print(f\"   📊 Coverage: {hybrid_coverage:.2f} (competitive with best: {best_coverage:.2f})\")\n",
        "            best_indices = hybrid_indices\n",
        "            best_strategy = \"hybrid\"\n",
        "        elif ranking_coverage > greedy_coverage:\n",
        "            print(f\"\\n📈 RECOMMENDATION: Use Rankings Team (+{ranking_coverage - greedy_coverage:.2f} better coverage)\")\n",
        "            best_indices = ranking_indices\n",
        "            best_strategy = \"rankings\"\n",
        "        else:\n",
        "            print(f\"\\n🤖 RECOMMENDATION: Use Greedy Team (+{greedy_coverage - ranking_coverage:.2f} better coverage)\")\n",
        "            best_indices = greedy_indices\n",
        "            best_strategy = \"greedy\"\n",
        "\n",
        "        return best_indices, best_strategy, {\n",
        "            \"hybrid\": hybrid_coverage,\n",
        "            \"greedy\": greedy_coverage,\n",
        "            \"rankings\": ranking_coverage\n",
        "        }\n",
        "\n",
        "    def get_team_evidence(self, affinity_df, team_indices, skills_list):\n",
        "        \"\"\"Collect supporting evidence (papers) for each skill covered by the team.\"\"\"\n",
        "        print(\"\\n📚 Collecting Supporting Evidence...\")\n",
        "        team_evidence = {} # skill -> list of paper IDs\n",
        "\n",
        "        for i, skill in enumerate(skills_list):\n",
        "            # Find the best researcher for this skill in the selected team\n",
        "            team_scores = affinity_df.iloc[team_indices, i]\n",
        "            if not team_scores.empty and team_scores.max() > 0:\n",
        "                best_member_name = team_scores.idxmax()\n",
        "                best_member_idx_in_affinity_df = affinity_df.index.get_loc(best_member_name)\n",
        "                best_member_id = None\n",
        "                # Find the OpenAlex ID for the best member\n",
        "                for match in self.researcher_metadata.itertuples():\n",
        "                    if match.researcher_name == best_member_name:\n",
        "                         best_member_id = match.researcher_openalex_id\n",
        "                         break\n",
        "\n",
        "                if best_member_id and best_member_id in self.evidence_index:\n",
        "                    # Get papers from evidence index for this researcher and skill\n",
        "                    skill_in_index = None\n",
        "                    # Find the closest matching skill name in the evidence index\n",
        "                    for index_skill in self.evidence_index[best_member_id].keys():\n",
        "                        if skill.lower() in index_skill.lower() or index_skill.lower() in skill.lower():\n",
        "                            skill_in_index = index_skill\n",
        "                            break\n",
        "\n",
        "                    if skill_in_index and self.evidence_index[best_member_id][skill_in_index]:\n",
        "                        # Select top 3 papers for this skill from the best member\n",
        "                        # Prioritize by citations or recency if available, otherwise just take first few\n",
        "                        papers = self.evidence_index[best_member_id][skill_in_index]\n",
        "                        # For simplicity, just take the first 3 papers\n",
        "                        team_evidence[skill] = list(set(papers))[:3]\n",
        "                    else:\n",
        "                        team_evidence[skill] = [] # No specific evidence found for this skill\n",
        "                else:\n",
        "                     team_evidence[skill] = [] # Researcher not found in evidence index\n",
        "\n",
        "            else:\n",
        "                 team_evidence[skill] = [] # No team member covers this skill well\n",
        "\n",
        "        print(f\"✅ Collected evidence for {len([k for k,v in team_evidence.items() if v])}/{len(skills_list)} skills\")\n",
        "        return team_evidence\n",
        "\n",
        "\n",
        "    def create_dream_team_report(self, affinity_df, skills_list, solicitation_data, max_team_size=4):\n",
        "        \"\"\"Create comprehensive dream team report with strategic analysis.\"\"\"\n",
        "        print(\"\\n🚀 CREATING DREAM TEAM STRATEGIC REPORT\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Run team selection algorithm (using the hybrid strategy as recommended)\n",
        "        team_indices, selection_history = self.dream_team_hybrid_strategy(affinity_df, guaranteed_top_n=2, max_team_size=max_team_size)\n",
        "\n",
        "        # Generate coverage analysis\n",
        "        coverage_report = self.generate_coverage_report(affinity_df, team_indices, skills_list)\n",
        "\n",
        "        # Collect evidence\n",
        "        team_evidence = self.get_team_evidence(affinity_df, team_indices, skills_list)\n",
        "\n",
        "\n",
        "        # Generate strategic analysis\n",
        "        strategic_analysis = self.generate_strategic_analysis(coverage_report, skills_list, solicitation_data)\n",
        "\n",
        "        return DreamTeamReport(\n",
        "            team_members=coverage_report['team_members'],\n",
        "            overall_coverage_score=coverage_report['overall_coverage_score'],\n",
        "            skill_analysis=coverage_report['skill_analysis'],\n",
        "            strategic_analysis=strategic_analysis,\n",
        "            selection_history=selection_history,\n",
        "            generated_at=datetime.now().isoformat()\n",
        "        ), team_evidence\n",
        "\n",
        "    def format_markdown_report(self, dream_team_report, solicitation_title, team_evidence):\n",
        "        \"\"\"Format the strategic report as a human-readable Markdown file.\"\"\"\n",
        "        report = f\"# NSF Dream Team Strategic Report\\n\\n\"\n",
        "        report += f\"**Solicitation:** {solicitation_title}\\n\"\n",
        "        report += f\"**Generated:** {dream_team_report.generated_at}\\n\"\n",
        "        report += f\"**Overall Team Coverage Score:** **`{dream_team_report.overall_coverage_score:.2f} / 100`**\\n\\n\"\n",
        "\n",
        "        # Team Summary Table\n",
        "        report += f\"## 🏆 Recommended Dream Team\\n\\n\"\n",
        "        report += \"| Role | Researcher | Avg. Affinity | Top Expertise Areas |\\n\"\n",
        "        report += \"|:---|:---|:---:|:---|\\n\"\n",
        "        for i, member in enumerate(dream_team_report.team_members):\n",
        "            role = \"**Principal Investigator (PI)**\" if i == 0 else f\"Co-Investigator {i}\"\n",
        "            top_skills = \", \".join([s['skill'] for s in member['top_skills']])\n",
        "            report += f\"| {role} | {member['name']} | `{member['avg_affinity']:.2f}` | {top_skills} |\\n\"\n",
        "\n",
        "        # Coverage Analysis Table\n",
        "        report += f\"\\n## 📊 Skills Coverage Analysis\\n\\n\"\n",
        "        report += \"| Skill / Expertise Area | Coverage | Level | Primary Expert | Supporting Evidence |\\n\" # Added Evidence column\n",
        "        report += \"|:---|:---:|:---|:---|:---|\\n\" # Updated table format\n",
        "        for skill in sorted(dream_team_report.skill_analysis, key=lambda x: x['coverage_score'], reverse=True):\n",
        "            level_emoji = \"🟢\" if skill['level'] == 'High' else \"🟡\" if skill['level'] == 'Medium' else \"🔴\"\n",
        "            evidence_links = []\n",
        "            if skill['skill'] in team_evidence and team_evidence[skill['skill']]:\n",
        "                 evidence_links = [f\"[Paper {j+1}]({paper_id})\" for j, paper_id in enumerate(team_evidence[skill['skill']])]\n",
        "\n",
        "            report += f\"| {skill['skill']} | `{skill['coverage_score']:.2f}` | {level_emoji} {skill['level']} | {skill['expert']} | {', '.join(evidence_links) if evidence_links else 'None'} |\\n\" # Added evidence links\n",
        "\n",
        "\n",
        "        # Strategic Analysis\n",
        "        report += f\"\\n## 🧠 AI-Powered Strategic Analysis (via Groq)\\n\\n\"\n",
        "        report += dream_team_report.strategic_analysis + \"\\n\"\n",
        "\n",
        "        # Selection History\n",
        "        report += f\"\\n## 🎯 Team Selection Process\\n\\n\"\n",
        "        for entry in dream_team_report.selection_history:\n",
        "            report += f\"**Step {entry['step']}:** {entry['action']} - {entry['researcher_name']}\\n\"\n",
        "            report += f\"- Reason: {entry['reason']}\\n\"\n",
        "            report += f\"- Team Coverage: {entry['team_coverage']:.2f}\\n\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def run_complete_analysis(self, solicitation_obj, top_n_researchers=20, max_team_size=4):\n",
        "        \"\"\"Run the complete analysis pipeline: matching → affinity matrix → dream team → report.\"\"\"\n",
        "        print(\"🚀 STARTING COMPLETE UNIFIED ANALYSIS PIPELINE\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Phase 1: Researcher Matching\n",
        "        print(\"\\n📍 PHASE 1: RESEARCHER MATCHING & RANKING\")\n",
        "        matching_results = self.search_researchers(solicitation_obj)\n",
        "\n",
        "        # Display top matches\n",
        "        print(\"\\n🏆 TOP 10 RESEARCHER MATCHES:\")\n",
        "        print(f\"{'Rank':<4} {'Name':<30} {'Final':<8} {'Academic':<9} {'Sparse':<8} {'Dense':<8} {'Papers':<7}\")\n",
        "        print(\"-\" * 80)\n",
        "        for i, match in enumerate(matching_results.top_matches[:10], 1):\n",
        "            print(f\"{i:<4} {match.researcher_name[:29]:<30} \"\n",
        "                  f\"{match.final_affinity_score:<8.1f} {match.academic_expertise_score:<9.1f} \"\n",
        "                  f\"{match.s_sparse:<8.1f} {match.s_dense:<8.1f} {match.total_papers:<7}\")\n",
        "\n",
        "        # Phase 2: Affinity Matrix Creation\n",
        "        print(f\"\\n📍 PHASE 2: AFFINITY MATRIX CREATION\")\n",
        "        affinity_df = self.create_affinity_matrix(matching_results, top_n_researchers)\n",
        "\n",
        "        # Phase 3: Dream Team Assembly\n",
        "        print(f\"\\n📍 PHASE 3: DREAM TEAM ASSEMBLY & STRATEGIC ANALYSIS\")\n",
        "        skills_list = matching_results.skills_analyzed\n",
        "        solicitation_data = {'title': solicitation_obj.title}\n",
        "        dream_team_report, team_evidence = self.create_dream_team_report(affinity_df, skills_list, solicitation_data, max_team_size)\n",
        "\n",
        "\n",
        "        # Phase 4: Generate Reports\n",
        "        print(f\"\\n📍 PHASE 4: REPORT GENERATION\")\n",
        "        markdown_report = self.format_markdown_report(dream_team_report, solicitation_obj.title, team_evidence)\n",
        "\n",
        "        # Save all results\n",
        "        timestamp = int(time.time())\n",
        "        base_filename = f\"unified_analysis_{timestamp}\"\n",
        "\n",
        "        # Save matching results\n",
        "        matching_file = f\"{output_dir}/{base_filename}_matching_results.json\"\n",
        "        with open(matching_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(asdict(matching_results), f, indent=2, ensure_ascii=False, default=str)\n",
        "        print(f\"💾 Saved matching results: {matching_file}\")\n",
        "\n",
        "        # Save affinity matrix\n",
        "        affinity_file = f\"{output_dir}/{base_filename}_affinity_matrix.csv\"\n",
        "        affinity_df.to_csv(affinity_file)\n",
        "        print(f\"💾 Saved affinity matrix: {affinity_file}\")\n",
        "\n",
        "        # Save dream team report\n",
        "        report_file = f\"{output_dir}/{base_filename}_dream_team_report.json\"\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(asdict(dream_team_report), f, indent=2, ensure_ascii=False, default=str)\n",
        "        print(f\"💾 Saved dream team report: {report_file}\")\n",
        "\n",
        "        # Save markdown report\n",
        "        markdown_file = f\"{output_dir}/{base_filename}_strategic_report.md\"\n",
        "        with open(markdown_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_report)\n",
        "        print(f\"💾 Saved markdown report: {markdown_file}\")\n",
        "\n",
        "        # Save evidence data\n",
        "        evidence_file = f\"{output_dir}/{base_filename}_team_evidence.json\"\n",
        "        with open(evidence_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(team_evidence, f, indent=2, ensure_ascii=False, default=str)\n",
        "        print(f\"💾 Saved team evidence: {evidence_file}\")\n",
        "\n",
        "        # Display final summary\n",
        "        self.display_final_summary(matching_results, dream_team_report, team_evidence)\n",
        "\n",
        "        return {\n",
        "            'matching_results': matching_results,\n",
        "            'affinity_matrix': affinity_df,\n",
        "            'dream_team_report': dream_team_report,\n",
        "            'team_evidence': team_evidence,\n",
        "            'markdown_report': markdown_report,\n",
        "            'files_saved': {\n",
        "                'matching': matching_file,\n",
        "                'affinity': affinity_file,\n",
        "                'report': report_file,\n",
        "                'markdown': markdown_file,\n",
        "                'evidence': evidence_file\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def display_final_summary(self, matching_results, dream_team_report, team_evidence):\n",
        "        \"\"\"Display a comprehensive summary of the analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"📋 UNIFIED ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"🔍 RESEARCHER MATCHING:\")\n",
        "        print(f\"   • Analyzed {matching_results.total_researchers} total researchers\")\n",
        "        print(f\"   • {matching_results.eligible_researchers} eligible researchers\")\n",
        "        print(f\"   • Processing time: {matching_results.processing_time_seconds}s\")\n",
        "\n",
        "        print(f\"\\n🎯 DREAM TEAM ASSEMBLY:\")\n",
        "        print(f\"   • Selected {len(dream_team_report.team_members)} optimal team members\")\n",
        "        print(f\"   • Strategy: Hybrid (Top 2 performers + coverage optimization)\")\n",
        "        print(f\"   • Overall coverage score: {dream_team_report.overall_coverage_score:.2f}/100\")\n",
        "\n",
        "        print(f\"\\n👥 RECOMMENDED TEAM:\")\n",
        "        for i, member in enumerate(dream_team_report.team_members):\n",
        "            role = \"PI\" if i == 0 else f\"Co-I {i}\"\n",
        "            print(f\"   • {member['name']} ({role}) - Affinity: {member['avg_affinity']:.2f}\")\n",
        "\n",
        "        # Check for gaps\n",
        "        low_skills = [s for s in dream_team_report.skill_analysis if s['level'] == 'Low']\n",
        "        medium_skills = [s for s in dream_team_report.skill_analysis if s['level'] == 'Medium']\n",
        "        high_skills = [s for s in dream_team_report.skill_analysis if s['level'] == 'High']\n",
        "\n",
        "        print(f\"\\n📊 SKILL COVERAGE BREAKDOWN:\")\n",
        "        print(f\"   🟢 High Coverage: {len(high_skills)} skills\")\n",
        "        print(f\"   🟡 Medium Coverage: {len(medium_skills)} skills\")\n",
        "        print(f\"   🔴 Low Coverage: {len(low_skills)} skills\")\n",
        "\n",
        "        if low_skills:\n",
        "            print(f\"\\n🔴 TOP GAPS TO ADDRESS:\")\n",
        "            for skill in sorted(low_skills, key=lambda x: x['coverage_score'])[:3]:\n",
        "                print(f\"   • {skill['skill']} (Coverage: {skill['coverage_score']:.1f})\")\n",
        "            if len(low_skills) > 3:\n",
        "                print(f\"   • ... and {len(low_skills) - 3} more (see full report)\")\n",
        "        else:\n",
        "            print(f\"\\n🟢 EXCELLENT COVERAGE: No significant skill gaps identified\")\n",
        "\n",
        "        print(f\"\\n✅ ANALYSIS COMPLETE! Check saved files for detailed reports.\")\n",
        "        print(f\"📚 EVIDENCE: {sum(len(evidence) for evidence in team_evidence.values())} supporting papers collected\")\n",
        "        print(f\"💡 TIP: Review the markdown report for AI-powered strategic recommendations and evidence.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "def run_unified_analysis():\n",
        "    \"\"\"Main function to run the complete unified analysis.\"\"\"\n",
        "\n",
        "    # Load solicitation\n",
        "    print(\"📋 Loading solicitation...\")\n",
        "    with open(solicitation_file, 'r', encoding='utf-8') as f:\n",
        "        solicitation_data = json.load(f)\n",
        "\n",
        "    # Convert to object\n",
        "    class SolicitationObj:\n",
        "        def __init__(self, data):\n",
        "            for key, value in data.items():\n",
        "                setattr(self, key, value)\n",
        "\n",
        "    solicitation_obj = SolicitationObj(solicitation_data)\n",
        "    print(f\"✅ Loaded: {solicitation_obj.title}\")\n",
        "\n",
        "    # Initialize unified system\n",
        "    system = UnifiedResearcherSystem(researcher_data_dir)\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = system.run_complete_analysis(\n",
        "        solicitation_obj,\n",
        "        top_n_researchers=20,  # Adjust as needed\n",
        "        max_team_size=4       # Adjust as needed\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "# RUN THE UNIFIED SYSTEM\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_unified_analysis()"
      ],
      "metadata": {
        "id": "089Nf2gCJpd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# COMPLETE UNIFIED RESEARCHER MATCHING & DREAM TEAM SYSTEM\n",
        "# ==============================================================================\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Try to import Groq API (optional)\n",
        "try:\n",
        "    from groq import Groq\n",
        "    GROQ_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GROQ_AVAILABLE = False\n",
        "    print(\"⚠️ Groq library not installed. GAP analysis will be basic.\")\n",
        "    print(\"   Install with: pip install groq\")\n",
        "\n",
        "# Try to import Google Colab userdata (optional)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    COLAB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    COLAB_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION - CHANGE THESE PATHS\n",
        "# ==============================================================================\n",
        "solicitation_file = \"/content/drive/MyDrive/datastore/NSF 23-506_ Expanding AI Innovation through Capacity Building and Partnerships (ExpandAI) _ NSF - National Science Foundation_enhanced_analysis.json\"\n",
        "researcher_data_dir = \"/content/drive/MyDrive/datastore/v2_DATA\"\n",
        "output_dir = \"/content/drive/MyDrive/datastore\"\n",
        "\n",
        "# ==============================================================================\n",
        "# GROQ API SETUP (OPTIONAL - FOR AI-POWERED STRATEGIC ANALYSIS)\n",
        "# ==============================================================================\n",
        "# 1. Get free API key at: https://console.groq.com/\n",
        "# 2. In Google Colab: Go to left sidebar > Secrets > Add new secret\n",
        "#    Name: GROQ_API_KEY, Value: your_api_key_here\n",
        "# 3. Or set environment variable: export GROQ_API_KEY=your_api_key_here\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# CORE CLASSES\n",
        "# ==============================================================================\n",
        "@dataclass\n",
        "class ResearcherMatch:\n",
        "    researcher_id: str\n",
        "    researcher_name: str\n",
        "    academic_expertise_score: float\n",
        "    s_sparse: float\n",
        "    s_dense: float\n",
        "    f_ge: float\n",
        "    final_affinity_score: float\n",
        "    total_papers: int\n",
        "    eligibility_status: str\n",
        "\n",
        "@dataclass\n",
        "class MatchingResults:\n",
        "    solicitation_title: str\n",
        "    eligible_researchers: int\n",
        "    total_researchers: int\n",
        "    top_matches: List[ResearcherMatch]\n",
        "    skills_analyzed: List[str]\n",
        "    processing_time_seconds: float\n",
        "\n",
        "@dataclass\n",
        "class DreamTeamReport:\n",
        "    team_members: List[Dict]\n",
        "    overall_coverage_score: float\n",
        "    skill_analysis: List[Dict]\n",
        "    strategic_analysis: str\n",
        "    selection_history: List[Dict]\n",
        "    generated_at: str\n",
        "\n",
        "class UnifiedResearcherSystem:\n",
        "    \"\"\"\n",
        "    Complete system that combines researcher matching with dream team assembly\n",
        "    and strategic GAP analysis using Claude API.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.alpha = 0.7  # TF-IDF weight\n",
        "        self.beta = 0.3   # Dense weight\n",
        "        self.groq_client = None\n",
        "        self.setup_groq_api()\n",
        "        self.load_data()\n",
        "\n",
        "    def setup_groq_api(self):\n",
        "        \"\"\"Initialize Groq API client if available.\"\"\"\n",
        "        if not GROQ_AVAILABLE:\n",
        "            print(\"⚠️ Groq API not available. Strategic analysis will be basic.\")\n",
        "            print(\"   Install with: pip install groq\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            if COLAB_AVAILABLE:\n",
        "                api_key = userdata.get('GROQ_API_KEY')\n",
        "            else:\n",
        "                api_key = os.environ.get('GROQ_API_KEY')\n",
        "\n",
        "            if api_key:\n",
        "                self.groq_client = Groq(api_key=api_key)\n",
        "                print(\"✅ Groq API client initialized for strategic analysis\")\n",
        "            else:\n",
        "                print(\"⚠️ Groq API key not found. Set GROQ_API_KEY.\")\n",
        "                print(\"   Get free API key at: https://console.groq.com/\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Groq API setup failed: {e}. Strategic analysis will be basic.\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load all preprocessed researcher data.\"\"\"\n",
        "        print(\"📂 Loading preprocessed data...\")\n",
        "\n",
        "        # Handle pickle loading issue - define dummy class if needed\n",
        "        try:\n",
        "            # Try to load TF-IDF model normally first\n",
        "            with open(self.data_dir / 'tfidf_model.pkl', 'rb') as f:\n",
        "                self.tfidf_model = pickle.load(f)\n",
        "        except AttributeError as e:\n",
        "            if \"ResearcherProfileProcessor\" in str(e):\n",
        "                print(\"🔧 Fixing pickle compatibility issue...\")\n",
        "                # Create a dummy class to handle the pickle loading\n",
        "                import sys\n",
        "                class ResearcherProfileProcessor:\n",
        "                    def comma_tokenizer(self, text: str):\n",
        "                        return [token.strip() for token in text.split(',') if token.strip()]\n",
        "\n",
        "                # Add the class to the current module\n",
        "                sys.modules[__name__].ResearcherProfileProcessor = ResearcherProfileProcessor\n",
        "\n",
        "                # Try loading again\n",
        "                with open(self.data_dir / 'tfidf_model.pkl', 'rb') as f:\n",
        "                    self.tfidf_model = pickle.load(f)\n",
        "                print(\"✅ TF-IDF model loaded successfully\")\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "        # Load researcher vectors\n",
        "        researcher_data = np.load(self.data_dir / 'researcher_vectors.npz', allow_pickle=True)\n",
        "        vectors = researcher_data['vectors']\n",
        "        researcher_ids = researcher_data['researcher_ids']\n",
        "        self.researcher_vectors = dict(zip(researcher_ids, vectors))\n",
        "\n",
        "        # Load paper embeddings\n",
        "        conceptual_data = np.load(self.data_dir / 'conceptual_profiles.npz', allow_pickle=True)\n",
        "        embeddings = conceptual_data['embeddings']\n",
        "        work_ids = conceptual_data['work_ids']\n",
        "        self.conceptual_profiles = dict(zip(work_ids, embeddings))\n",
        "\n",
        "        # Load evidence index\n",
        "        with open(self.data_dir / 'evidence_index.json', 'r') as f:\n",
        "            self.evidence_index = json.load(f)\n",
        "\n",
        "        # Load metadata\n",
        "        self.researcher_metadata = pd.read_parquet(self.data_dir / 'researcher_metadata.parquet')\n",
        "\n",
        "        # Load sentence model\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        print(f\"✅ Loaded {len(self.researcher_vectors)} researchers\")\n",
        "        self.diagnose_data_quality()\n",
        "\n",
        "    def diagnose_data_quality(self):\n",
        "        \"\"\"Diagnose potential data quality issues.\"\"\"\n",
        "        print(\"\\n🔍 DIAGNOSING DATA QUALITY\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Check TF-IDF model\n",
        "        try:\n",
        "            vocab_size = len(self.tfidf_model.get_feature_names_out())\n",
        "            print(f\"TF-IDF vocabulary size: {vocab_size}\")\n",
        "\n",
        "            vocab = list(self.tfidf_model.get_feature_names_out())\n",
        "            print(f\"First 20 TF-IDF features: {vocab[:20]}\")\n",
        "            print(f\"Last 20 TF-IDF features: {vocab[-20:]}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error accessing TF-IDF vocabulary: {e}\")\n",
        "\n",
        "        # Check researcher vectors\n",
        "        print(f\"Researcher vectors: {len(self.researcher_vectors)}\")\n",
        "        if self.researcher_vectors:\n",
        "            sample_vector = next(iter(self.researcher_vectors.values()))\n",
        "            print(f\"Vector dimensions: {sample_vector.shape}\")\n",
        "            print(f\"Sample vector sum: {sample_vector.sum():.4f}\")\n",
        "\n",
        "        # Check conceptual profiles\n",
        "        print(f\"Conceptual profiles: {len(self.conceptual_profiles)}\")\n",
        "        if self.conceptual_profiles:\n",
        "            sample_embedding = next(iter(self.conceptual_profiles.values()))\n",
        "            print(f\"Embedding dimensions: {sample_embedding.shape}\")\n",
        "\n",
        "        # Check evidence index\n",
        "        print(f\"Evidence index researchers: {len(self.evidence_index)}\")\n",
        "\n",
        "        # Check overlap between evidence index and conceptual profiles\n",
        "        all_evidence_papers = set()\n",
        "        for researcher_papers in self.evidence_index.values():\n",
        "            for topic_papers in researcher_papers.values():\n",
        "                all_evidence_papers.update(topic_papers)\n",
        "\n",
        "        conceptual_papers = set(self.conceptual_profiles.keys())\n",
        "        overlap = all_evidence_papers.intersection(conceptual_papers)\n",
        "\n",
        "        print(f\"Papers in evidence index: {len(all_evidence_papers)}\")\n",
        "        print(f\"Papers with embeddings: {len(conceptual_papers)}\")\n",
        "        print(f\"Overlap: {len(overlap)}\")\n",
        "\n",
        "        if len(overlap) == 0:\n",
        "            print(\"❌ CRITICAL: No overlap between evidence index and conceptual profiles!\")\n",
        "        elif len(overlap) < len(all_evidence_papers) * 0.5:\n",
        "            print(\"⚠️ WARNING: Low overlap between evidence index and conceptual profiles!\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    def filter_eligibility(self, solicitation_obj, researchers):\n",
        "        \"\"\"Simple eligibility filtering.\"\"\"\n",
        "        eligible = set(researchers)\n",
        "        eligibility = getattr(solicitation_obj, 'eligibility', {})\n",
        "\n",
        "        # Early-career filter\n",
        "        if eligibility and any('early' in str(v).lower() for v in eligibility.values() if v):\n",
        "            early_career = self.researcher_metadata[\n",
        "                self.researcher_metadata['first_publication_year'] >= 2015\n",
        "            ]['researcher_openalex_id'].tolist()\n",
        "            eligible = eligible.intersection(set(early_career))\n",
        "            print(f\"   Applied early-career filter: {len(eligible)} remain\")\n",
        "\n",
        "        # Grant experience filter\n",
        "        if eligibility and any('grant' in str(v).lower() or 'funding' in str(v).lower() for v in eligibility.values() if v):\n",
        "            experienced = self.researcher_metadata[\n",
        "                self.researcher_metadata['grant_experience_factor'] > 0\n",
        "            ]['researcher_openalex_id'].tolist()\n",
        "            eligible = eligible.intersection(set(experienced))\n",
        "            print(f\"   Applied grant experience filter: {len(eligible)} remain\")\n",
        "\n",
        "        return list(eligible)\n",
        "\n",
        "    def extract_keywords_from_skills(self, skills: List[str]) -> List[str]:\n",
        "        \"\"\"Extract keywords from solicitation skills using same logic as researcher topics.\"\"\"\n",
        "        import re\n",
        "\n",
        "        stop_words = {'and', 'in', 'of', 'for', 'the', 'a', 'an', 'to', 'with', 'on', 'at', 'by',\n",
        "                      'expertise', 'experience', 'knowledge', 'ability', 'skills', 'understanding',\n",
        "                      'capacity', 'proficiency', 'e.g.', 'eg', 'including', 'such', 'as'}\n",
        "\n",
        "        all_keywords = []\n",
        "\n",
        "        for skill in skills:\n",
        "            # Clean and split\n",
        "            cleaned = re.sub(r'[^\\w\\s-]', ' ', skill.lower())\n",
        "            words = cleaned.split()\n",
        "\n",
        "            # Extract meaningful keywords\n",
        "            for word in words:\n",
        "                word = word.strip('-')\n",
        "                if (len(word) >= 3 and\n",
        "                    word not in stop_words and\n",
        "                    not word.isdigit()):\n",
        "                    all_keywords.append(word)\n",
        "\n",
        "        return all_keywords\n",
        "\n",
        "    def score_researcher(self, researcher_id, skills, solicitation_embedding, debug_mode=False):\n",
        "        \"\"\"Score a single researcher with optional debug output.\"\"\"\n",
        "        try:\n",
        "            # Get metadata\n",
        "            researcher_row = self.researcher_metadata[\n",
        "                self.researcher_metadata['researcher_openalex_id'] == researcher_id\n",
        "            ]\n",
        "            if researcher_row.empty:\n",
        "                return None\n",
        "\n",
        "            researcher_name = researcher_row.iloc[0]['researcher_name']\n",
        "            total_papers = int(researcher_row.iloc[0]['total_papers'])\n",
        "            grant_factor = researcher_row.iloc[0]['grant_experience_factor']\n",
        "\n",
        "            # Extract keywords and format with commas (same as researcher documents)\n",
        "            solicitation_keywords = self.extract_keywords_from_skills(skills)\n",
        "            solicitation_text = ', '.join(solicitation_keywords)\n",
        "\n",
        "            if debug_mode:\n",
        "                print(f\"DEBUG - Researcher: {researcher_name}\")\n",
        "                print(f\"  Original skills count: {len(skills)}\")\n",
        "                print(f\"  Extracted keywords: {solicitation_keywords[:10]}...\")\n",
        "                print(f\"  Solicitation text: {solicitation_text[:100]}...\")\n",
        "\n",
        "            # Calculate sparse score (TF-IDF)\n",
        "            if researcher_id not in self.researcher_vectors:\n",
        "                if debug_mode:\n",
        "                    print(f\"  WARNING: No TF-IDF vector for {researcher_id}\")\n",
        "                s_sparse = 0.0\n",
        "            else:\n",
        "                try:\n",
        "                    solicitation_vector = self.tfidf_model.transform([solicitation_text])\n",
        "                    researcher_vector = self.researcher_vectors[researcher_id].reshape(1, -1)\n",
        "\n",
        "                    if debug_mode:\n",
        "                        print(f\"  Solicitation vector shape: {solicitation_vector.shape}\")\n",
        "                        print(f\"  Researcher vector shape: {researcher_vector.shape}\")\n",
        "                        print(f\"  Solicitation vector sum: {solicitation_vector.sum()}\")\n",
        "                        print(f\"  Researcher vector sum: {researcher_vector.sum()}\")\n",
        "                        if hasattr(solicitation_vector, 'nnz'):\n",
        "                             print(f\"  Solicitation non-zero elements: {solicitation_vector.nnz}\")\n",
        "\n",
        "\n",
        "                    if solicitation_vector.sum() == 0 or researcher_vector.sum() == 0:\n",
        "                        if debug_mode:\n",
        "                            print(f\"  WARNING: Zero vector detected\")\n",
        "                        s_sparse = 0.0\n",
        "                    else:\n",
        "                        similarity = cosine_similarity(solicitation_vector, researcher_vector)[0][0]\n",
        "                        s_sparse = float(similarity * 100)\n",
        "                        if debug_mode:\n",
        "                            print(f\"  TF-IDF similarity: {similarity}\")\n",
        "                except Exception as tfidf_error:\n",
        "                    if debug_mode:\n",
        "                        print(f\"  TF-IDF ERROR: {tfidf_error}\")\n",
        "                    s_sparse = 0.0\n",
        "\n",
        "            # Calculate dense score (max similarity across papers)\n",
        "            s_dense = 0.0\n",
        "            papers_checked = 0\n",
        "            papers_found = 0\n",
        "            if researcher_id in self.evidence_index:\n",
        "                # Get all papers for this researcher from evidence index\n",
        "                researcher_papers = []\n",
        "                for topic_papers in self.evidence_index[researcher_id].values():\n",
        "                    researcher_papers.extend(topic_papers)\n",
        "                researcher_papers = list(set(researcher_papers))  # Remove duplicates\n",
        "\n",
        "                if debug_mode:\n",
        "                    print(f\"  Papers from evidence index: {len(researcher_papers)}\")\n",
        "\n",
        "                max_sim = 0.0\n",
        "                for paper_id in researcher_papers:\n",
        "                    papers_checked += 1\n",
        "                    if paper_id in self.conceptual_profiles:\n",
        "                        papers_found += 1\n",
        "                        paper_embedding = self.conceptual_profiles[paper_id]\n",
        "                        try:\n",
        "                            sim = cosine_similarity(\n",
        "                                solicitation_embedding.reshape(1, -1),\n",
        "                                paper_embedding.reshape(1, -1)\n",
        "                            )[0][0]\n",
        "                            max_sim = max(max_sim, sim)\n",
        "                        except Exception as dense_error:\n",
        "                            if debug_mode:\n",
        "                                print(f\"  Dense similarity error for paper {paper_id}: {dense_error}\")\n",
        "\n",
        "                s_dense = float(max_sim * 100)\n",
        "                if debug_mode:\n",
        "                    print(f\"  Papers checked: {papers_checked}, Papers with embeddings: {papers_found}\")\n",
        "                    print(f\"  Max dense similarity: {max_sim}\")\n",
        "            else:\n",
        "                if debug_mode:\n",
        "                    print(f\"  No evidence index entry for researcher\")\n",
        "\n",
        "            # Calculate final scores\n",
        "            f_ge = max(1.0, min(3.0, 1.0 + (grant_factor * 0.2)))\n",
        "            academic_expertise = (self.alpha * s_sparse) + (self.beta * s_dense)\n",
        "            final_score = academic_expertise * f_ge\n",
        "\n",
        "            if debug_mode:\n",
        "                print(f\"  Final scores - Sparse: {s_sparse:.2f}, Dense: {s_dense:.2f}, Grant: {f_ge:.2f}\")\n",
        "                print(f\"  Academic: {academic_expertise:.2f}, Final: {final_score:.2f}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            return ResearcherMatch(\n",
        "                researcher_id=researcher_id,\n",
        "                researcher_name=researcher_name,\n",
        "                academic_expertise_score=academic_expertise,\n",
        "                s_sparse=s_sparse,\n",
        "                s_dense=s_dense,\n",
        "                f_ge=f_ge,\n",
        "                final_affinity_score=final_score,\n",
        "                total_papers=total_papers,\n",
        "                eligibility_status=\"Eligible\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            if debug_mode:\n",
        "                print(f\"ERROR scoring {researcher_id}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def search_researchers(self, solicitation_obj, debug_first_n=3):\n",
        "        \"\"\"Main search function to rank researchers.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\n🔍 ANALYZING: {solicitation_obj.title[:80]}...\")\n",
        "\n",
        "        # Get skills\n",
        "        skills = solicitation_obj.required_skills_checklist\n",
        "        print(f\"📊 Skills to analyze: {len(skills)}\")\n",
        "\n",
        "        # Create solicitation embedding\n",
        "        solicitation_text = f\"{solicitation_obj.title}. {solicitation_obj.abstract}\"\n",
        "        solicitation_embedding = self.sentence_model.encode(solicitation_text)\n",
        "\n",
        "        # Get all researchers\n",
        "        all_researchers = list(self.researcher_metadata['researcher_openalex_id'])\n",
        "        print(f\"👥 Total researchers: {len(all_researchers)}\")\n",
        "\n",
        "        # Filter by eligibility\n",
        "        eligible = self.filter_eligibility(solicitation_obj, all_researchers)\n",
        "        print(f\"✅ Eligible researchers: {len(eligible)}\")\n",
        "\n",
        "        # Score all eligible researchers\n",
        "        print(\"🔄 Calculating scores...\")\n",
        "        matches = []\n",
        "        debug_count = 0\n",
        "\n",
        "        for researcher_id in eligible:\n",
        "            # Debug first N researchers to avoid spam\n",
        "            debug_mode = debug_count < debug_first_n\n",
        "            if debug_mode:\n",
        "                debug_count += 1\n",
        "\n",
        "            result = self.score_researcher(researcher_id, skills, solicitation_embedding, debug_mode)\n",
        "\n",
        "            if result:\n",
        "                matches.append(result)\n",
        "\n",
        "        # Sort by score\n",
        "        matches.sort(key=lambda x: x.final_affinity_score, reverse=True)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return MatchingResults(\n",
        "            solicitation_title=solicitation_obj.title,\n",
        "            eligible_researchers=len(eligible),\n",
        "            total_researchers=len(all_researchers),\n",
        "            top_matches=matches,\n",
        "            skills_analyzed=skills,\n",
        "            processing_time_seconds=round(processing_time, 2)\n",
        "        )\n",
        "\n",
        "    def create_affinity_matrix(self, matching_results, top_n_researchers=20):\n",
        "        \"\"\"Create an affinity matrix from matching results.\"\"\"\n",
        "        print(f\"\\n📊 Creating affinity matrix for top {top_n_researchers} researchers...\")\n",
        "\n",
        "        # Get top researchers\n",
        "        top_matches = matching_results.top_matches[:top_n_researchers]\n",
        "        researcher_names = [match.researcher_name for match in top_matches]\n",
        "        skills = matching_results.skills_analyzed\n",
        "\n",
        "        # Create matrix: researchers × skills\n",
        "        affinity_matrix = np.zeros((len(top_matches), len(skills)))\n",
        "\n",
        "        for i, match in enumerate(top_matches):\n",
        "            # For each researcher, calculate their affinity to each skill\n",
        "            researcher_id = match.researcher_id\n",
        "\n",
        "            for j, skill in enumerate(skills):\n",
        "                # Calculate skill-specific affinity score\n",
        "                skill_keywords = self.extract_keywords_from_skills([skill])\n",
        "                skill_text = ', '.join(skill_keywords)\n",
        "\n",
        "                try:\n",
        "                    # TF-IDF similarity for this specific skill\n",
        "                    skill_vector = self.tfidf_model.transform([skill_text])\n",
        "                    researcher_vector = self.researcher_vectors[researcher_id].reshape(1, -1)\n",
        "                    sparse_sim = cosine_similarity(skill_vector, researcher_vector)[0][0] * 100\n",
        "\n",
        "                    # Dense similarity (using overall solicitation embedding as proxy)\n",
        "                    # In practice, you might want to create skill-specific embeddings\n",
        "                    # For simplicity, let's use the max dense score from the overall matching\n",
        "                    dense_sim = match.s_dense # Use the pre-calculated max dense score for the researcher\n",
        "\n",
        "\n",
        "                    # Combined affinity score for this skill\n",
        "                    skill_affinity = (self.alpha * sparse_sim) + (self.beta * dense_sim)\n",
        "                    affinity_matrix[i, j] = max(0, skill_affinity)  # Ensure non-negative\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Fallback: use overall academic score\n",
        "                    affinity_matrix[i, j] = match.academic_expertise_score\n",
        "\n",
        "        # Create DataFrame\n",
        "        affinity_df = pd.DataFrame(\n",
        "            affinity_matrix,\n",
        "            index=researcher_names,\n",
        "            columns=[f\"Skill_{i+1}: {skill}\" for i, skill in enumerate(skills)]\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Created affinity matrix: {affinity_df.shape[0]} researchers × {affinity_df.shape[1]} skills\")\n",
        "        return affinity_df\n",
        "\n",
        "    def calculate_team_coverage(self, affinity_df, team_indices):\n",
        "        \"\"\"Calculate team coverage scores for all skills.\"\"\"\n",
        "        if not team_indices:\n",
        "            return np.array([0.0] * affinity_df.shape[1]), 0.0\n",
        "        team_affinities = affinity_df.iloc[team_indices]\n",
        "        skill_coverages = team_affinities.max(axis=0).values\n",
        "        return skill_coverages, np.mean(skill_coverages)\n",
        "\n",
        "    def calculate_marginal_gain(self, affinity_df, current_team_indices, candidate_index):\n",
        "        \"\"\"Calculate the marginal gain of adding a candidate to the team.\"\"\"\n",
        "        _, current_coverage = self.calculate_team_coverage(affinity_df, current_team_indices)\n",
        "        _, new_coverage = self.calculate_team_coverage(affinity_df, current_team_indices + [candidate_index])\n",
        "        return new_coverage - current_coverage\n",
        "\n",
        "    def dream_team_hybrid_strategy(self, affinity_df, guaranteed_top_n=2, max_team_size=4):\n",
        "        \"\"\"\n",
        "        Hybrid approach: Lock in top N performers, then optimize coverage for remaining slots.\n",
        "        This ensures we get researchers with proven grant experience and keyword matching.\n",
        "        \"\"\"\n",
        "        print(f\"\\n🎯 Running Hybrid Dream Team Strategy\")\n",
        "        print(f\"   Step 1: Lock in top {guaranteed_top_n} performers\")\n",
        "        print(f\"   Step 2: Optimize coverage for remaining {max_team_size - guaranteed_top_n} slots\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Phase 1: Lock in top performers (they earned their ranking!)\n",
        "        researcher_averages = affinity_df.mean(axis=1).sort_values(ascending=False)\n",
        "        top_performers = researcher_averages.head(guaranteed_top_n)\n",
        "\n",
        "        selected_indices = []\n",
        "        selection_history = []\n",
        "\n",
        "        print(\"🔒 LOCKING IN TOP PERFORMERS:\")\n",
        "        for i, (name, avg_score) in enumerate(top_performers.items()):\n",
        "            idx = affinity_df.index.get_loc(name)\n",
        "            selected_indices.append(idx)\n",
        "            role = \"PI\" if i == 0 else f\"Co-PI {i}\"\n",
        "\n",
        "            selection_history.append({\n",
        "                'step': i + 1,\n",
        "                'action': f'Lock {role}',\n",
        "                'researcher_name': name,\n",
        "                'reason': f'Top {i+1} performer (avg: {avg_score:.2f}, proven track record)',\n",
        "                'team_coverage': 0  # Will calculate after\n",
        "            })\n",
        "            print(f\"   ✅ {name} ({role}) - Avg Score: {avg_score:.2f}\")\n",
        "\n",
        "        # Calculate coverage after locking in top performers\n",
        "        _, coverage_after_top = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "        print(f\"   📊 Coverage after top {guaranteed_top_n}: {coverage_after_top:.2f}\")\n",
        "\n",
        "        # Update coverage in history\n",
        "        for entry in selection_history:\n",
        "            entry['team_coverage'] = coverage_after_top\n",
        "\n",
        "        # Phase 2: Optimize remaining slots for coverage\n",
        "        print(f\"\\n🎯 OPTIMIZING REMAINING {max_team_size - guaranteed_top_n} SLOTS FOR COVERAGE:\")\n",
        "        n_researchers = len(affinity_df)\n",
        "\n",
        "        for step in range(guaranteed_top_n + 1, max_team_size + 1):\n",
        "            # Calculate marginal gains for all remaining researchers\n",
        "            gains = [(idx, self.calculate_marginal_gain(affinity_df, selected_indices, idx))\n",
        "                     for idx in range(n_researchers) if idx not in selected_indices]\n",
        "\n",
        "            if not gains:\n",
        "                print(f\"   ⚠️ No more candidates available\")\n",
        "                break\n",
        "\n",
        "            # Sort by marginal gain and show top candidates\n",
        "            top_candidates = sorted(gains, key=lambda x: x[1], reverse=True)[:5]\n",
        "            best_candidate_idx, best_marginal_gain = top_candidates[0]\n",
        "\n",
        "            print(f\"   📊 Top candidates for slot {step - guaranteed_top_n}:\")\n",
        "            for i, (idx, gain) in enumerate(top_candidates):\n",
        "                candidate_name = affinity_df.index[idx]\n",
        "                candidate_avg = affinity_df.iloc[idx].mean()\n",
        "                marker = \"👑\" if i == 0 else f\"  {i+1}.\"\n",
        "                print(f\"      {marker} {candidate_name} (Avg: {candidate_avg:.2f}, Coverage Gain: +{gain:.2f})\")\n",
        "\n",
        "            # Add the best candidate for coverage\n",
        "            if best_marginal_gain > 0.1:  # Lower threshold since we have strong foundation\n",
        "                selected_indices.append(best_candidate_idx)\n",
        "                _, new_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "\n",
        "                selection_history.append({\n",
        "                    'step': step,\n",
        "                    'action': 'Add for Coverage',\n",
        "                    'researcher_name': affinity_df.index[best_candidate_idx],\n",
        "                    'reason': f'Best coverage gain (+{best_marginal_gain:.2f})',\n",
        "                    'team_coverage': new_coverage\n",
        "                })\n",
        "                print(f\"   ✅ Added: {affinity_df.index[best_candidate_idx]} (New Coverage: {new_coverage:.2f})\")\n",
        "            else:\n",
        "                print(f\"   🛑 Stopping: Marginal gain {best_marginal_gain:.2f} too small\")\n",
        "                break\n",
        "\n",
        "        final_coverage = self.calculate_team_coverage(affinity_df, selected_indices)[1]\n",
        "        print(f\"\\n🎯 Final Hybrid Team ({len(selected_indices)} members) with {final_coverage:.2f} coverage\")\n",
        "        print(f\"   Strategy: Top {guaranteed_top_n} performers + coverage optimization\")\n",
        "\n",
        "        return selected_indices, selection_history\n",
        "\n",
        "    def dream_team_greedy_algorithm(self, affinity_df, min_team_size=2, max_team_size=4, marginal_threshold=0.25):\n",
        "        \"\"\"Implement the greedy algorithm to select the best team.\"\"\"\n",
        "        print(\"\\n🎯 Running Pure Greedy Algorithm...\")\n",
        "        print(\"=\" * 50)\n",
        "        n_researchers = len(affinity_df)\n",
        "        selected_indices = []\n",
        "        selection_history = []\n",
        "\n",
        "        # Step 1: Select the best overall researcher as PI\n",
        "        best_researcher_pos = affinity_df.mean(axis=1).idxmax()\n",
        "        best_researcher_loc = affinity_df.index.get_loc(best_researcher_pos)\n",
        "        selected_indices.append(best_researcher_loc)\n",
        "        _, initial_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "\n",
        "        selection_history.append({\n",
        "            'step': 1, 'action': 'Select PI',\n",
        "            'researcher_name': affinity_df.index[best_researcher_loc],\n",
        "            'reason': 'Highest average affinity score',\n",
        "            'team_coverage': initial_coverage\n",
        "        })\n",
        "        print(f\"🏆 Step 1 - PI Selection: {affinity_df.index[best_researcher_loc]} (Coverage: {initial_coverage:.2f})\")\n",
        "\n",
        "        # Step 2-N: Iteratively add members with the highest marginal gain\n",
        "        for step in range(2, max_team_size + 1):\n",
        "            gains = [(idx, self.calculate_marginal_gain(affinity_df, selected_indices, idx))\n",
        "                     for idx in range(n_researchers) if idx not in selected_indices]\n",
        "            if not gains:\n",
        "                break\n",
        "\n",
        "            best_candidate_idx, best_marginal_gain = max(gains, key=lambda item: item[1])\n",
        "\n",
        "            # Show top 3 candidates for transparency\n",
        "            top_candidates = sorted(gains, key=lambda x: x[1], reverse=True)[:3]\n",
        "            print(f\"   📊 Top candidates for Step {step}:\")\n",
        "            for i, (idx, gain) in enumerate(top_candidates):\n",
        "                candidate_name = affinity_df.index[idx]\n",
        "                print(f\"      {i+1}. {candidate_name} (Marginal gain: +{gain:.2f})\")\n",
        "\n",
        "            # More flexible stopping criteria\n",
        "            should_add = (\n",
        "                best_marginal_gain > marginal_threshold or  # Significant gain\n",
        "                len(selected_indices) < min_team_size or    # Haven't reached minimum\n",
        "                (step <= 4 and best_marginal_gain > 0.1)   # Force at least 4 if minimal gain\n",
        "            )\n",
        "\n",
        "            if should_add:\n",
        "                selected_indices.append(best_candidate_idx)\n",
        "                _, new_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "                selection_history.append({\n",
        "                    'step': step, 'action': 'Add Member',\n",
        "                    'researcher_name': affinity_df.index[best_candidate_idx],\n",
        "                    'reason': f'Maximum marginal gain (+{best_marginal_gain:.2f})',\n",
        "                    'team_coverage': new_coverage\n",
        "                })\n",
        "                print(f\"✅ Step {step} - Added: {affinity_df.index[best_candidate_idx]} (New Coverage: {new_coverage:.2f})\")\n",
        "            else:\n",
        "                print(f\"🛑 Step {step} - Stopping: Marginal gain {best_marginal_gain:.2f} below threshold {marginal_threshold}\")\n",
        "                break\n",
        "\n",
        "        final_coverage = self.calculate_team_coverage(affinity_df, selected_indices)[1]\n",
        "        print(f\"\\n🎯 Final Dream Team ({len(selected_indices)} members) with {final_coverage:.2f} coverage.\")\n",
        "        return selected_indices, selection_history\n",
        "\n",
        "    def generate_coverage_report(self, affinity_df, team_indices, skills_list):\n",
        "        \"\"\"Generate a detailed coverage report for the selected team.\"\"\"\n",
        "        skill_coverages, overall_coverage = self.calculate_team_coverage(affinity_df, team_indices)\n",
        "\n",
        "        team_members = []\n",
        "        for idx in team_indices:\n",
        "            scores = affinity_df.iloc[idx]\n",
        "            top_skills = [{'skill': skills_list[i], 'score': scores[i]}\n",
        "                         for i in scores.argsort()[-3:][::-1]]\n",
        "            team_members.append({\n",
        "                'name': affinity_df.index[idx],\n",
        "                'avg_affinity': scores.mean(),\n",
        "                'top_skills': top_skills\n",
        "            })\n",
        "\n",
        "        skill_analysis = []\n",
        "        for i, (skill, coverage) in enumerate(zip(skills_list, skill_coverages)):\n",
        "            team_scores = affinity_df.iloc[team_indices, i]\n",
        "            if not team_scores.empty: # Handle case where team is empty or skill has no coverage\n",
        "                best_member_name = team_scores.idxmax()\n",
        "                expert_score = team_scores.max()\n",
        "            else:\n",
        "                best_member_name = \"N/A\"\n",
        "                expert_score = 0.0\n",
        "\n",
        "\n",
        "            skill_analysis.append({\n",
        "                'skill': skill,\n",
        "                'coverage_score': coverage,\n",
        "                'level': 'High' if coverage >= 70 else 'Medium' if coverage >= 40 else 'Low',\n",
        "                'expert': best_member_name,\n",
        "                'expert_score': expert_score\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'overall_coverage_score': overall_coverage,\n",
        "            'team_members': team_members,\n",
        "            'skill_analysis': skill_analysis\n",
        "        }\n",
        "\n",
        "    def generate_strategic_analysis(self, coverage_report, skills_list, solicitation_data):\n",
        "        \"\"\"Generate AI-powered gap analysis using Groq API.\"\"\"\n",
        "        if not self.groq_client:\n",
        "            # Basic fallback analysis\n",
        "            low_skills = [s['skill'] for s in coverage_report['skill_analysis'] if s['level'] == 'Low']\n",
        "            basic_analysis = f\"\"\"\n",
        "BASIC STRATEGIC ANALYSIS (Groq API not available):\n",
        "\n",
        "Team Coverage Score: {coverage_report['overall_coverage_score']:.2f}/100\n",
        "\n",
        "STRENGTHS:\n",
        "- Team assembled using advanced affinity matching\n",
        "- {len(coverage_report['team_members'])} selected team members\n",
        "- Covers {len(skills_list)} required skill areas\n",
        "\n",
        "POTENTIAL GAPS:\n",
        "{f\"- Low coverage areas: {', '.join(low_skills)}\" if low_skills else \"- No significant gaps identified\"}\n",
        "\n",
        "RECOMMENDATIONS:\n",
        "- Review low-coverage skills for additional expertise\n",
        "- Consider collaboration with external partners if needed\n",
        "- Leverage team members' top skills for proposal strength\n",
        "\"\"\"\n",
        "            return basic_analysis.strip()\n",
        "\n",
        "        print(\"🤖 Generating strategic analysis with Groq API...\")\n",
        "\n",
        "        # Create detailed prompt for Groq\n",
        "        team_info = \"\\n\".join([f\"- {member['name']} (avg affinity: {member['avg_affinity']:.2f})\"\n",
        "                              for member in coverage_report['team_members']])\n",
        "\n",
        "        low_skills = [s for s in coverage_report['skill_analysis'] if s['level'] == 'Low']\n",
        "        medium_skills = [s for s in coverage_report['skill_analysis'] if s['level'] == 'Medium']\n",
        "        high_skills = [s for s in coverage_report['skill_analysis'] if s['level'] == 'High']\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Analyze this research team's fit for the NSF solicitation titled: \"{solicitation_data.get('title', 'N/A')}\"\n",
        "\n",
        "TEAM COMPOSITION:\n",
        "{team_info}\n",
        "\n",
        "COVERAGE ANALYSIS:\n",
        "- Overall team coverage score: {coverage_report['overall_coverage_score']:.2f}/100\n",
        "- High coverage skills ({len(high_skills)}): {', '.join([s['skill'] for s in high_skills[:5]])}\n",
        "- Medium coverage skills ({len(medium_skills)}): {', '.join([s['skill'] for s in medium_skills[:5]])}\n",
        "- Low coverage skills ({len(low_skills)}): {', '.join([s['skill'] for s in low_skills])}\n",
        "\n",
        "Please provide a strategic analysis covering:\n",
        "1. Team strengths and competitive advantages\n",
        "2. Critical gaps and risks\n",
        "3. Specific recommendations for proposal development\n",
        "4. Potential collaboration or recruitment strategies\n",
        "5. Overall competitiveness assessment\n",
        "\n",
        "Keep the analysis practical and actionable for proposal development.\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.groq_client.chat.completions.create(\n",
        "                model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Free Groq model\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=2000,\n",
        "                temperature=0.5\n",
        "            )\n",
        "            analysis = response.choices[0].message.content\n",
        "            print(\"✅ Strategic analysis generated with Groq API\")\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Groq API analysis failed: {e}\")\n",
        "            # Fallback to basic analysis\n",
        "            low_skills_list = [s['skill'] for s in low_skills]\n",
        "            return f\"Groq API failed. Basic analysis: Team coverage is {coverage_report['overall_coverage_score']:.2f}/100. Review low-coverage skills: {', '.join(low_skills_list) if low_skills_list else 'None identified'}.\"\n",
        "\n",
        "    def dream_team_by_rankings(self, affinity_df, team_size=4):\n",
        "        \"\"\"Alternative approach: Simply select top N researchers by overall ranking.\"\"\"\n",
        "        print(f\"\\n📊 ALTERNATIVE: Dream Team by Rankings (Top {team_size})\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Sort by average affinity (overall performance)\n",
        "        researcher_averages = affinity_df.mean(axis=1).sort_values(ascending=False)\n",
        "        top_researchers = researcher_averages.head(team_size)\n",
        "\n",
        "        team_indices = [affinity_df.index.get_loc(name) for name in top_researchers.index]\n",
        "        _, coverage = self.calculate_team_coverage(affinity_df, team_indices)\n",
        "\n",
        "        print(\"Selected team (by ranking):\")\n",
        "        for i, (name, avg_score) in enumerate(top_researchers.items()):\n",
        "            role = \"PI\" if i == 0 else f\"Co-I {i}\"\n",
        "            print(f\"   {i+1}. {name} ({role}) - Avg Score: {avg_score:.2f}\")\n",
        "\n",
        "        print(f\"Team coverage by rankings: {coverage:.2f}\")\n",
        "        return team_indices, coverage\n",
        "\n",
        "    def compare_team_strategies(self, affinity_df, skills_list):\n",
        "        \"\"\"Compare different team selection strategies.\"\"\"\n",
        "        print(\"\\n🔬 COMPARING TEAM SELECTION STRATEGIES\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Strategy 1: Hybrid approach (lock top 2 + coverage optimization) - PREFERRED\n",
        "        hybrid_indices, hybrid_history = self.dream_team_hybrid_strategy(affinity_df, guaranteed_top_n=2, max_team_size=4)\n",
        "        hybrid_coverage = self.calculate_team_coverage(affinity_df, hybrid_indices)[1]\n",
        "\n",
        "        # Strategy 2: Pure greedy algorithm (optimized coverage)\n",
        "        greedy_indices, greedy_history = self.dream_team_greedy_algorithm(affinity_df, marginal_threshold=0.25, max_team_size=4)\n",
        "        greedy_coverage = self.calculate_team_coverage(affinity_df, greedy_indices)[1]\n",
        "\n",
        "        # Strategy 3: Top performers by ranking\n",
        "        ranking_indices, ranking_coverage = self.dream_team_by_rankings(affinity_df, team_size=4)\n",
        "\n",
        "        print(f\"\\n📊 STRATEGY COMPARISON:\")\n",
        "        print(f\"🎯 Hybrid Strategy ({len(hybrid_indices)} members): {hybrid_coverage:.2f} coverage\")\n",
        "        print(f\"🤖 Pure Greedy ({len(greedy_indices)} members): {greedy_coverage:.2f} coverage\")\n",
        "        print(f\"📈 Top Rankings (4 members): {ranking_coverage:.2f} coverage\")\n",
        "\n",
        "        # Show detailed comparison\n",
        "        strategies = [\n",
        "            (\"HYBRID (Top 2 + Coverage)\", hybrid_indices, hybrid_coverage),\n",
        "            (\"PURE GREEDY\", greedy_indices, greedy_coverage),\n",
        "            (\"TOP RANKINGS\", ranking_indices, ranking_coverage)\n",
        "        ]\n",
        "\n",
        "        for strategy_name, indices, coverage in strategies:\n",
        "            print(f\"\\n{strategy_name}:\")\n",
        "            for i, idx in enumerate(indices):\n",
        "                name = affinity_df.index[idx]\n",
        "                avg_score = affinity_df.iloc[idx].mean()\n",
        "                role = \"PI\" if i == 0 else f\"Co-I {i}\"\n",
        "                print(f\"   {name} ({role}) - Avg Score: {avg_score:.2f}\")\n",
        "\n",
        "        # Determine which strategy to use - prioritize hybrid if competitive\n",
        "        best_coverage = max(hybrid_coverage, greedy_coverage, ranking_coverage)\n",
        "\n",
        "        if hybrid_coverage >= best_coverage - 1.0:  # Hybrid is within 1 point of best\n",
        "            print(f\"\\n🎯 RECOMMENDATION: Use Hybrid Strategy\")\n",
        "            print(f\"   ✅ Guarantees top performers (grant experience + keyword matching)\")\n",
        "            print(f\"   ✅ Optimizes coverage for remaining slots\")\n",
        "            print(f\"   📊 Coverage: {hybrid_coverage:.2f} (competitive with best: {best_coverage:.2f})\")\n",
        "            best_indices = hybrid_indices\n",
        "            best_strategy = \"hybrid\"\n",
        "        elif ranking_coverage > greedy_coverage:\n",
        "            print(f\"\\n📈 RECOMMENDATION: Use Rankings Team (+{ranking_coverage - greedy_coverage:.2f} better coverage)\")\n",
        "            best_indices = ranking_indices\n",
        "            best_strategy = \"rankings\"\n",
        "        else:\n",
        "            print(f\"\\n🤖 RECOMMENDATION: Use Greedy Team (+{greedy_coverage - ranking_coverage:.2f} better coverage)\")\n",
        "            best_indices = greedy_indices\n",
        "            best_strategy = \"greedy\"\n",
        "\n",
        "        return best_indices, best_strategy, {\n",
        "            \"hybrid\": hybrid_coverage,\n",
        "            \"greedy\": greedy_coverage,\n",
        "            \"rankings\": ranking_coverage\n",
        "        }\n",
        "\n",
        "    def get_team_evidence(self, affinity_df, team_indices, skills_list):\n",
        "        \"\"\"Collect supporting evidence (papers) for each skill covered by the team.\"\"\"\n",
        "        print(\"\\n📚 Collecting Supporting Evidence...\")\n",
        "        team_evidence = {} # skill -> list of paper IDs\n",
        "\n",
        "        for i, skill in enumerate(skills_list):\n",
        "            # Find the best researcher for this skill in the selected team\n",
        "            team_scores = affinity_df.iloc[team_indices, i]\n",
        "            if not team_scores.empty and team_scores.max() > 0:\n",
        "                best_member_name = team_scores.idxmax()\n",
        "                best_member_idx_in_affinity_df = affinity_df.index.get_loc(best_member_name)\n",
        "                best_member_id = None\n",
        "                # Find the OpenAlex ID for the best member\n",
        "                for match in self.researcher_metadata.itertuples():\n",
        "                    if match.researcher_name == best_member_name:\n",
        "                         best_member_id = match.researcher_openalex_id\n",
        "                         break\n",
        "\n",
        "                if best_member_id and best_member_id in self.evidence_index:\n",
        "                    # Get papers from evidence index for this researcher and skill\n",
        "                    skill_in_index = None\n",
        "                    # Find the closest matching skill name in the evidence index\n",
        "                    for index_skill in self.evidence_index[best_member_id].keys():\n",
        "                        if skill.lower() in index_skill.lower() or index_skill.lower() in skill.lower():\n",
        "                            skill_in_index = index_skill\n",
        "                            break\n",
        "\n",
        "                    if skill_in_index and self.evidence_index[best_member_id][skill_in_index]:\n",
        "                        # Select top 3 papers for this skill from the best member\n",
        "                        # Prioritize by citations or recency if available, otherwise just take first few\n",
        "                        papers = self.evidence_index[best_member_id][skill_in_index]\n",
        "                        # For simplicity, just take the first 3 papers\n",
        "                        team_evidence[skill] = list(set(papers))[:3]\n",
        "                    else:\n",
        "                        team_evidence[skill] = [] # No specific evidence found for this skill\n",
        "                else:\n",
        "                     team_evidence[skill] = [] # Researcher not found in evidence index\n",
        "\n",
        "            else:\n",
        "                 team_evidence[skill] = [] # No team member covers this skill well\n",
        "\n",
        "        print(f\"✅ Collected evidence for {len([k for k,v in team_evidence.items() if v])}/{len(skills_list)} skills\")\n",
        "        return team_evidence\n",
        "\n",
        "\n",
        "    def create_dream_team_report(self, affinity_df, skills_list, solicitation_data, max_team_size=4):\n",
        "        \"\"\"Create comprehensive dream team report with strategic analysis.\"\"\"\n",
        "        print(\"\\n🚀 CREATING DREAM TEAM STRATEGIC REPORT\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Run team selection algorithm (using the hybrid strategy as recommended)\n",
        "        team_indices, selection_history = self.dream_team_hybrid_strategy(affinity_df, guaranteed_top_n=2, max_team_size=max_team_size)\n",
        "\n",
        "        # Generate coverage analysis\n",
        "        coverage_report = self.generate_coverage_report(affinity_df, team_indices, skills_list)\n",
        "\n",
        "        # Collect evidence\n",
        "        team_evidence = self.get_team_evidence(affinity_df, team_indices, skills_list)\n",
        "\n",
        "\n",
        "        # Generate strategic analysis\n",
        "        strategic_analysis = self.generate_strategic_analysis(coverage_report, skills_list, solicitation_data)\n",
        "\n",
        "        return DreamTeamReport(\n",
        "            team_members=coverage_report['team_members'],\n",
        "            overall_coverage_score=coverage_report['overall_coverage_score'],\n",
        "            skill_analysis=coverage_report['skill_analysis'],\n",
        "            strategic_analysis=strategic_analysis,\n",
        "            selection_history=selection_history,\n",
        "            generated_at=datetime.now().isoformat()\n",
        "        ), team_evidence\n",
        "\n",
        "    def format_markdown_report(self, dream_team_report, solicitation_title, team_evidence):\n",
        "        \"\"\"Format the strategic report as a human-readable Markdown file.\"\"\"\n",
        "        report = f\"# NSF Dream Team Strategic Report\\n\\n\"\n",
        "        report += f\"**Solicitation:** {solicitation_title}\\n\"\n",
        "        report += f\"**Generated:** {dream_team_report.generated_at}\\n\"\n",
        "        report += f\"**Overall Team Coverage Score:** **`{dream_team_report.overall_coverage_score:.2f} / 100`**\\n\\n\"\n",
        "\n",
        "        # Team Summary Table\n",
        "        report += f\"## 🏆 Recommended Dream Team\\n\\n\"\n",
        "        report += \"| Role | Researcher | Avg. Affinity | Top Expertise Areas |\\n\"\n",
        "        report += \"|:---|:---|:---:|:---|\\n\"\n",
        "        for i, member in enumerate(dream_team_report.team_members):\n",
        "            role = \"**Principal Investigator (PI)**\" if i == 0 else f\"Co-Investigator {i}\"\n",
        "            top_skills = \", \".join([s['skill'] for s in member['top_skills']])\n",
        "            report += f\"| {role} | {member['name']} | `{member['avg_affinity']:.2f}` | {top_skills} |\\n\"\n",
        "\n",
        "        # Coverage Analysis Table\n",
        "        report += f\"\\n## 📊 Skills Coverage Analysis\\n\\n\"\n",
        "        report += \"| Skill / Expertise Area | Coverage | Level | Primary Expert | Supporting Evidence |\\n\" # Added Evidence column\n",
        "        report += \"|:---|:---:|:---|:---|:---|\\n\" # Updated table format\n",
        "        for skill in sorted(dream_team_report.skill_analysis, key=lambda x: x['coverage_score'], reverse=True):\n",
        "            level_emoji = \"🟢\" if skill['level'] == 'High' else \"🟡\" if skill['level'] == 'Medium' else \"🔴\"\n",
        "            evidence_links = []\n",
        "            if skill['skill'] in team_evidence and team_evidence[skill['skill']]:\n",
        "                 evidence_links = [f\"[Paper {j+1}]({paper_id})\" for j, paper_id in enumerate(team_evidence[skill['skill']])]\n",
        "\n",
        "            report += f\"| {skill['skill']} | `{skill['coverage_score']:.2f}` | {level_emoji} {skill['level']} | {skill['expert']} | {', '.join(evidence_links) if evidence_links else 'None'} |\\n\" # Added evidence links\n",
        "\n",
        "\n",
        "        # Strategic Analysis\n",
        "        report += f\"\\n## 🧠 AI-Powered Strategic Analysis (via Groq)\\n\\n\"\n",
        "        report += dream_team_report.strategic_analysis + \"\\n\"\n",
        "\n",
        "        # Selection History\n",
        "        report += f\"\\n## 🎯 Team Selection Process\\n\\n\"\n",
        "        for entry in dream_team_report.selection_history:\n",
        "            report += f\"**Step {entry['step']}:** {entry['action']} - {entry['researcher_name']}\\n\"\n",
        "            report += f\"- Reason: {entry['reason']}\\n\"\n",
        "            report += f\"- Team Coverage: {entry['team_coverage']:.2f}\\n\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def run_complete_analysis(self, solicitation_obj, top_n_researchers=20, max_team_size=4):\n",
        "        \"\"\"Run the complete analysis pipeline: matching → affinity matrix → dream team → report.\"\"\"\n",
        "        print(\"🚀 STARTING COMPLETE UNIFIED ANALYSIS PIPELINE\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Phase 1: Researcher Matching\n",
        "        print(\"\\n📍 PHASE 1: RESEARCHER MATCHING & RANKING\")\n",
        "        matching_results = self.search_researchers(solicitation_obj)\n",
        "\n",
        "        # Display top matches\n",
        "        print(\"\\n🏆 TOP 10 RESEARCHER MATCHES:\")\n",
        "        print(f\"{'Rank':<4} {'Name':<30} {'Final':<8} {'Academic':<9} {'Sparse':<8} {'Dense':<8} {'Papers':<7}\")\n",
        "        print(\"-\" * 80)\n",
        "        for i, match in enumerate(matching_results.top_matches[:10], 1):\n",
        "            print(f\"{i:<4} {match.researcher_name[:29]:<30} \"\n",
        "                  f\"{match.final_affinity_score:<8.1f} {match.academic_expertise_score:<9.1f} \"\n",
        "                  f\"{match.s_sparse:<8.1f} {match.s_dense:<8.1f} {match.total_papers:<7}\")\n",
        "\n",
        "        # Phase 2: Affinity Matrix Creation\n",
        "        print(f\"\\n📍 PHASE 2: AFFINITY MATRIX CREATION\")\n",
        "        affinity_df = self.create_affinity_matrix(matching_results, top_n_researchers)\n",
        "\n",
        "        # Phase 3: Dream Team Assembly\n",
        "        print(f\"\\n📍 PHASE 3: DREAM TEAM ASSEMBLY & STRATEGIC ANALYSIS\")\n",
        "        skills_list = matching_results.skills_analyzed\n",
        "        solicitation_data = {'title': solicitation_obj.title}\n",
        "        dream_team_report, team_evidence = self.create_dream_team_report(affinity_df, skills_list, solicitation_data, max_team_size)\n",
        "\n",
        "\n",
        "        # Phase 4: Generate Reports\n",
        "        print(f\"\\n📍 PHASE 4: REPORT GENERATION\")\n",
        "        markdown_report = self.format_markdown_report(dream_team_report, solicitation_obj.title, team_evidence)\n",
        "\n",
        "        # Save all results\n",
        "        timestamp = int(time.time())\n",
        "        base_filename = f\"unified_analysis_{timestamp}\"\n",
        "\n",
        "        # Save matching results\n",
        "        matching_file = f\"{output_dir}/{base_filename}_matching_results.json\"\n",
        "        with open(matching_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(asdict(matching_results), f, indent=2, ensure_ascii=False, default=str)\n",
        "        print(f\"💾 Saved matching results: {matching_file}\")\n",
        "\n",
        "        # Save affinity matrix\n",
        "        affinity_file = f\"{output_dir}/{base_filename}_affinity_matrix.csv\"\n",
        "        affinity_df.to_csv(affinity_file)\n",
        "        print(f\"💾 Saved affinity matrix: {affinity_file}\")\n",
        "\n",
        "        # Save dream team report\n",
        "        report_file = f\"{output_dir}/{base_filename}_dream_team_report.json\"\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(asdict(dream_team_report), f, indent=2, ensure_ascii=False, default=str)\n",
        "        print(f\"💾 Saved dream team report: {report_file}\")\n",
        "\n",
        "        # Save markdown report\n",
        "        markdown_file = f\"{output_dir}/{base_filename}_strategic_report.md\"\n",
        "        with open(markdown_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_report)\n",
        "        print(f\"💾 Saved markdown report: {markdown_file}\")\n",
        "\n",
        "        # Save evidence data\n",
        "        evidence_file = f\"{output_dir}/{base_filename}_team_evidence.json\"\n",
        "        with open(evidence_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(team_evidence, f, indent=2, ensure_ascii=False, default=str)\n",
        "        print(f\"💾 Saved team evidence: {evidence_file}\")\n",
        "\n",
        "        # Display final summary\n",
        "        self.display_final_summary(matching_results, dream_team_report, team_evidence)\n",
        "\n",
        "        return {\n",
        "            'matching_results': matching_results,\n",
        "            'affinity_matrix': affinity_df,\n",
        "            'dream_team_report': dream_team_report,\n",
        "            'team_evidence': team_evidence,\n",
        "            'markdown_report': markdown_report,\n",
        "            'files_saved': {\n",
        "                'matching': matching_file,\n",
        "                'affinity': affinity_file,\n",
        "                'report': report_file,\n",
        "                'markdown': markdown_file,\n",
        "                'evidence': evidence_file\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def display_final_summary(self, matching_results, dream_team_report, team_evidence):\n",
        "        \"\"\"Display a comprehensive summary of the analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"📋 UNIFIED ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"🔍 RESEARCHER MATCHING:\")\n",
        "        print(f\"   • Analyzed {matching_results.total_researchers} total researchers\")\n",
        "        print(f\"   • {matching_results.eligible_researchers} eligible researchers\")\n",
        "        print(f\"   • Processing time: {matching_results.processing_time_seconds}s\")\n",
        "\n",
        "        print(f\"\\n🎯 DREAM TEAM ASSEMBLY:\")\n",
        "        print(f\"   • Selected {len(dream_team_report.team_members)} optimal team members\")\n",
        "        print(f\"   • Strategy: Hybrid (Top 2 performers + coverage optimization)\")\n",
        "        print(f\"   • Overall coverage score: {dream_team_report.overall_coverage_score:.2f}/100\")\n",
        "\n",
        "        print(f\"\\n👥 RECOMMENDED TEAM:\")\n",
        "        for i, member in enumerate(dream_team_report.team_members):\n",
        "            role = \"PI\" if i == 0 else f\"Co-I {i}\"\n",
        "            print(f\"   • {member['name']} ({role}) - Affinity: {member['avg_affinity']:.2f}\")\n",
        "\n",
        "        # Check for gaps\n",
        "        low_skills = [s for s in dream_team_report.skill_analysis if s['level'] == 'Low']\n",
        "        medium_skills = [s for s in dream_team_report.skill_analysis if s['level'] == 'Medium']\n",
        "        high_skills = [s for s in dream_team_report.skill_analysis if s['level'] == 'High']\n",
        "\n",
        "        print(f\"\\n📊 SKILL COVERAGE BREAKDOWN:\")\n",
        "        print(f\"   🟢 High Coverage: {len(high_skills)} skills\")\n",
        "        print(f\"   🟡 Medium Coverage: {len(medium_skills)} skills\")\n",
        "        print(f\"   🔴 Low Coverage: {len(low_skills)} skills\")\n",
        "\n",
        "        if low_skills:\n",
        "            print(f\"\\n🔴 TOP GAPS TO ADDRESS:\")\n",
        "            for skill in sorted(low_skills, key=lambda x: x['coverage_score'])[:3]:\n",
        "                print(f\"   • {skill['skill']} (Coverage: {skill['coverage_score']:.1f})\")\n",
        "            if len(low_skills) > 3:\n",
        "                print(f\"   • ... and {len(low_skills) - 3} more (see full report)\")\n",
        "        else:\n",
        "            print(f\"\\n🟢 EXCELLENT COVERAGE: No significant skill gaps identified\")\n",
        "\n",
        "        print(f\"\\n✅ ANALYSIS COMPLETE! Check saved files for detailed reports.\")\n",
        "        print(f\"📚 EVIDENCE: {sum(len(evidence) for evidence in team_evidence.values())} supporting papers collected\")\n",
        "        print(f\"💡 TIP: Review the markdown report for AI-powered strategic recommendations and evidence.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "def run_unified_analysis():\n",
        "    \"\"\"Main function to run the complete unified analysis.\"\"\"\n",
        "\n",
        "    # Load solicitation\n",
        "    print(\"📋 Loading solicitation...\")\n",
        "    with open(solicitation_file, 'r', encoding='utf-8') as f:\n",
        "        solicitation_data = json.load(f)\n",
        "\n",
        "    # Convert to object\n",
        "    class SolicitationObj:\n",
        "        def __init__(self, data):\n",
        "            for key, value in data.items():\n",
        "                setattr(self, key, value)\n",
        "\n",
        "    solicitation_obj = SolicitationObj(solicitation_data)\n",
        "    print(f\"✅ Loaded: {solicitation_obj.title}\")\n",
        "\n",
        "    # Initialize unified system\n",
        "    system = UnifiedResearcherSystem(researcher_data_dir)\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = system.run_complete_analysis(\n",
        "        solicitation_obj,\n",
        "        top_n_researchers=20,  # Adjust as needed\n",
        "        max_team_size=4       # Adjust as needed\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "# RUN THE UNIFIED SYSTEM\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_unified_analysis()"
      ],
      "metadata": {
        "id": "dSp94AniiI-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pEO60HCxlB7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1ns9Xpay-us_M17AlaVFyb6MPw2_kAe5C",
      "authorship_tag": "ABX9TyMpjKQfsP++5hTICHzrmvYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}