{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtTFQ5Y0w/eodiySWEfmlD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f608812de00149439800cd96bae8413b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e250c7aa89e841a1917a48d3fc91f071",
              "IPY_MODEL_d9dbbfb521cb4933b7a0725353ee87e8",
              "IPY_MODEL_30fd9f446cd14d2b8fbd6723b86ab9e6"
            ],
            "layout": "IPY_MODEL_c2fa4e85674d4d2e940016c5e2222ac6"
          }
        },
        "e250c7aa89e841a1917a48d3fc91f071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6264b4591aac436894e22e065b942078",
            "placeholder": "​",
            "style": "IPY_MODEL_17e771b7406c420c946a34d46662d4fd",
            "value": "Batches: 100%"
          }
        },
        "d9dbbfb521cb4933b7a0725353ee87e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cbe2085e634483cbc33e084120f74f5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d30ee13178a340ffa9219f4e804b73a8",
            "value": 1
          }
        },
        "30fd9f446cd14d2b8fbd6723b86ab9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61be64e437984538b298e69c59a572f9",
            "placeholder": "​",
            "style": "IPY_MODEL_0df02a4e1714404e8dc59fc637fed4a7",
            "value": " 1/1 [00:00&lt;00:00,  1.43it/s]"
          }
        },
        "c2fa4e85674d4d2e940016c5e2222ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6264b4591aac436894e22e065b942078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e771b7406c420c946a34d46662d4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cbe2085e634483cbc33e084120f74f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d30ee13178a340ffa9219f4e804b73a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61be64e437984538b298e69c59a572f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df02a4e1714404e8dc59fc637fed4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7d16e5d0ce48959e2b981129bf083a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b37a5c038d584b199bec0886ccf913a4",
              "IPY_MODEL_cf4eca5b08ea4f28a6db3e82ba655c62",
              "IPY_MODEL_b5adde69c950473bacb94696c2ef87e6"
            ],
            "layout": "IPY_MODEL_989acf8eb8d141a5b05a450a0f5c3857"
          }
        },
        "b37a5c038d584b199bec0886ccf913a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50226896f9864f36a6cacc4a41504c97",
            "placeholder": "​",
            "style": "IPY_MODEL_0c3a67c070a547c1a5b0e5b1682365ef",
            "value": "Batches: 100%"
          }
        },
        "cf4eca5b08ea4f28a6db3e82ba655c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6641ff34b3d04af9be1f7f49046b4097",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bd0121d80444127a0ffc8a54bba8b00",
            "value": 1
          }
        },
        "b5adde69c950473bacb94696c2ef87e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a58e44c63eb40ad8995ef499243f0e2",
            "placeholder": "​",
            "style": "IPY_MODEL_1bb9e01dc16545d5bc0090820fe051ad",
            "value": " 1/1 [00:00&lt;00:00,  6.23it/s]"
          }
        },
        "989acf8eb8d141a5b05a450a0f5c3857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50226896f9864f36a6cacc4a41504c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c3a67c070a547c1a5b0e5b1682365ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6641ff34b3d04af9be1f7f49046b4097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bd0121d80444127a0ffc8a54bba8b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a58e44c63eb40ad8995ef499243f0e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb9e01dc16545d5bc0090820fe051ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tar-ive/Dashboard/blob/main/brahman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the System Architecture docs here: https://www.overleaf.com/read/ffzgqryyrgzm#22fb04"
      ],
      "metadata": {
        "id": "cY1zflRdqDg0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8Ov4DHAi41i",
        "outputId": "3031e877-b202-4279-9cd5-ca18defef2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texas State University Researchers Works Extractor\n",
            "==================================================\n",
            "This script will:\n",
            "1. Get top 50 cited researchers from Texas State University\n",
            "2. Include Tahir Ekin even if not in top 50\n",
            "3. Extract all works data for each researcher\n",
            "4. Save combined data to CSV file\n",
            "==================================================\n",
            "IMPORTANT: Please update the EMAIL variable with your actual email address!\n",
            "Current email: your_email@example.com\n",
            "======================================================================\n",
            "Getting Texas State University ID...\n",
            "Found Texas State University ID: https://openalex.org/I13511017\n",
            "\n",
            "Fetching top 50 cited researchers from Texas State University...\n",
            "Found 50 researchers\n",
            "\n",
            "Tahir Ekin (ID: A5088154684) not in top 50, adding him...\n",
            "Tahir Ekin added successfully\n",
            "\n",
            "Processing 51 researchers and their works...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:   0%|          | 0/51 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: Manfred Schartl (https://openalex.org/A5031215616)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:   2%|▏         | 1/51 [00:09<07:37,  9.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 667 works for Manfred Schartl\n",
            "Extracted data for 667 works\n",
            "\n",
            "Processing: Larry R. Price (https://openalex.org/A5046299069)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:   4%|▍         | 2/51 [00:12<04:27,  5.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 155 works for Larry R. Price\n",
            "Extracted data for 155 works\n",
            "\n",
            "Processing: Michael A. Huston (https://openalex.org/A5039371296)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:   6%|▌         | 3/51 [00:14<03:12,  4.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 93 works for Michael A. Huston\n",
            "Extracted data for 93 works\n",
            "\n",
            "Processing: Marcus Felson (https://openalex.org/A5070163403)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:   8%|▊         | 4/51 [00:16<02:33,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 162 works for Marcus Felson\n",
            "Extracted data for 162 works\n",
            "\n",
            "Processing: Togay Ozbakkaloglu (https://openalex.org/A5017593645)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  10%|▉         | 5/51 [00:20<02:46,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 353 works for Togay Ozbakkaloglu\n",
            "Extracted data for 353 works\n",
            "\n",
            "Processing: Brady T. West (https://openalex.org/A5010637807)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  12%|█▏        | 6/51 [00:25<02:59,  3.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 342 works for Brady T. West\n",
            "Extracted data for 342 works\n",
            "\n",
            "Processing: Eric Kirby (https://openalex.org/A5048215687)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  14%|█▎        | 7/51 [00:28<02:49,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 280 works for Eric Kirby\n",
            "Extracted data for 280 works\n",
            "\n",
            "Processing: GEORGE W. LATIMER (https://openalex.org/A5061773538)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  16%|█▌        | 8/51 [00:30<02:17,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 61 works for GEORGE W. LATIMER\n",
            "Extracted data for 61 works\n",
            "\n",
            "Processing: James P. LeSage (https://openalex.org/A5053526422)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  18%|█▊        | 9/51 [00:32<02:01,  2.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 181 works for James P. LeSage\n",
            "Extracted data for 181 works\n",
            "\n",
            "Processing: William J. Brittain (https://openalex.org/A5004843671)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  20%|█▉        | 10/51 [00:35<01:52,  2.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 170 works for William J. Brittain\n",
            "Extracted data for 170 works\n",
            "\n",
            "Processing: Raymond P. Fisk (https://openalex.org/A5090189953)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  22%|██▏       | 11/51 [00:37<01:42,  2.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 139 works for Raymond P. Fisk\n",
            "Extracted data for 139 works\n",
            "\n",
            "Processing: Robert A. Giacalone (https://openalex.org/A5088313921)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  24%|██▎       | 12/51 [00:39<01:35,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 160 works for Robert A. Giacalone\n",
            "Extracted data for 160 works\n",
            "\n",
            "Processing: Soe W. Myint (https://openalex.org/A5076178372)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  25%|██▌       | 13/51 [00:42<01:37,  2.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 174 works for Soe W. Myint\n",
            "Extracted data for 174 works\n",
            "\n",
            "Processing: Anne H. H. Ngu (https://openalex.org/A5016020974)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  27%|██▋       | 14/51 [00:44<01:29,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 119 works for Anne H. H. Ngu\n",
            "Extracted data for 119 works\n",
            "\n",
            "Processing: Russell Lang (https://openalex.org/A5112296295)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  29%|██▉       | 15/51 [00:47<01:28,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 179 works for Russell Lang\n",
            "Extracted data for 179 works\n",
            "\n",
            "Processing: Clemens Scott Kruse (https://openalex.org/A5040317882)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  31%|███▏      | 16/51 [00:49<01:23,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 works for Clemens Scott Kruse\n",
            "Extracted data for 100 works\n",
            "\n",
            "Processing: K.M. Kent (https://openalex.org/A5082637722)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  33%|███▎      | 17/51 [00:51<01:18,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 works for K.M. Kent\n",
            "Extracted data for 100 works\n",
            "\n",
            "Processing: Subasish Das (https://openalex.org/A5053621729)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  35%|███▌      | 18/51 [00:56<01:38,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 331 works for Subasish Das\n",
            "Extracted data for 331 works\n",
            "\n",
            "Processing: Francisco X. Barrios (https://openalex.org/A5111778985)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  37%|███▋      | 19/51 [00:57<01:24,  2.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 61 works for Francisco X. Barrios\n",
            "Extracted data for 61 works\n",
            "\n",
            "Processing: Dittmar Hahn (https://openalex.org/A5080638929)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  39%|███▉      | 20/51 [01:00<01:20,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 183 works for Dittmar Hahn\n",
            "Extracted data for 183 works\n",
            "\n",
            "Processing: David L. Mills (https://openalex.org/A5073857984)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  41%|████      | 21/51 [01:02<01:14,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 150 works for David L. Mills\n",
            "Extracted data for 150 works\n",
            "\n",
            "Processing: James B. Reeves (https://openalex.org/A5045635837)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  43%|████▎     | 22/51 [01:04<01:09,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 176 works for James B. Reeves\n",
            "Extracted data for 176 works\n",
            "\n",
            "Processing: James P. LeSage (https://openalex.org/A5111463834)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  45%|████▌     | 23/51 [01:06<01:02,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 62 works for James P. LeSage\n",
            "Extracted data for 62 works\n",
            "\n",
            "Processing: Susanne Schwinning (https://openalex.org/A5068455420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  47%|████▋     | 24/51 [01:08<01:00,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 68 works for Susanne Schwinning\n",
            "Extracted data for 68 works\n",
            "\n",
            "Processing: Megan L. Rogers (https://openalex.org/A5054025836)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  49%|████▉     | 25/51 [01:13<01:12,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 221 works for Megan L. Rogers\n",
            "Extracted data for 221 works\n",
            "\n",
            "Processing: Robert McLean (https://openalex.org/A5033667907)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  51%|█████     | 26/51 [01:16<01:16,  3.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 266 works for Robert McLean\n",
            "Extracted data for 266 works\n",
            "\n",
            "Processing: Alexander Kornienko (https://openalex.org/A5009000319)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  53%|█████▎    | 27/51 [01:20<01:20,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 203 works for Alexander Kornienko\n",
            "Extracted data for 203 works\n",
            "\n",
            "Processing: Walter E. Rudzinski (https://openalex.org/A5058978080)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  55%|█████▍    | 28/51 [01:22<01:08,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 97 works for Walter E. Rudzinski\n",
            "Extracted data for 97 works\n",
            "\n",
            "Processing: Lee Friedman (https://openalex.org/A5010220844)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  57%|█████▋    | 29/51 [01:25<01:02,  2.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 184 works for Lee Friedman\n",
            "Extracted data for 184 works\n",
            "\n",
            "Processing: John Smyth (https://openalex.org/A5103402630)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  59%|█████▉    | 30/51 [01:28<01:02,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 240 works for John Smyth\n",
            "Extracted data for 240 works\n",
            "\n",
            "Processing: Nihal Dharmasiri (https://openalex.org/A5022776280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  61%|██████    | 31/51 [01:30<00:50,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 24 works for Nihal Dharmasiri\n",
            "Extracted data for 24 works\n",
            "\n",
            "Processing: R. W. Dixon (https://openalex.org/A5010334621)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  63%|██████▎   | 32/51 [01:32<00:46,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 158 works for R. W. Dixon\n",
            "Extracted data for 158 works\n",
            "\n",
            "Processing: John H. Walker (https://openalex.org/A5111404583)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  65%|██████▍   | 33/51 [01:36<00:51,  2.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 187 works for John H. Walker\n",
            "Extracted data for 187 works\n",
            "\n",
            "Processing: Karl Stephan (https://openalex.org/A5022648834)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  67%|██████▋   | 34/51 [01:41<01:00,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 482 works for Karl Stephan\n",
            "Extracted data for 482 works\n",
            "\n",
            "Processing: Floyd W. Weckerly (https://openalex.org/A5048251671)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  69%|██████▊   | 35/51 [01:43<00:50,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 116 works for Floyd W. Weckerly\n",
            "Extracted data for 116 works\n",
            "\n",
            "Processing: Gary W. Beall (https://openalex.org/A5078294602)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  71%|███████   | 36/51 [01:46<00:44,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 179 works for Gary W. Beall\n",
            "Extracted data for 179 works\n",
            "\n",
            "Processing: Chris C. Nice (https://openalex.org/A5046140996)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  73%|███████▎  | 37/51 [01:48<00:39,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 121 works for Chris C. Nice\n",
            "Extracted data for 121 works\n",
            "\n",
            "Processing: Joseph A. Veech (https://openalex.org/A5003558378)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  75%|███████▍  | 38/51 [01:50<00:33,  2.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 130 works for Joseph A. Veech\n",
            "Extracted data for 130 works\n",
            "\n",
            "Processing: Sunethra Dharmasiri (https://openalex.org/A5027219393)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  76%|███████▋  | 39/51 [01:52<00:28,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 24 works for Sunethra Dharmasiri\n",
            "Extracted data for 24 works\n",
            "\n",
            "Processing: M. Holtz (https://openalex.org/A5083363281)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  78%|███████▊  | 40/51 [01:56<00:31,  2.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 248 works for M. Holtz\n",
            "Extracted data for 248 works\n",
            "\n",
            "Processing: Yuan Lü (https://openalex.org/A5100672889)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  80%|████████  | 41/51 [01:59<00:28,  2.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 136 works for Yuan Lü\n",
            "Extracted data for 136 works\n",
            "\n",
            "Processing: Sean M. Kerwin (https://openalex.org/A5001476601)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  82%|████████▏ | 42/51 [02:01<00:24,  2.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 153 works for Sean M. Kerwin\n",
            "Extracted data for 153 works\n",
            "\n",
            "Processing: William W. L. Glenn (https://openalex.org/A5110507225)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  84%|████████▍ | 43/51 [02:03<00:19,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 164 works for William W. L. Glenn\n",
            "Extracted data for 164 works\n",
            "\n",
            "Processing: John P. Tiefenbacher (https://openalex.org/A5062128630)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  86%|████████▋ | 44/51 [02:05<00:16,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 158 works for John P. Tiefenbacher\n",
            "Extracted data for 158 works\n",
            "\n",
            "Processing: E. L. Piner (https://openalex.org/A5063322161)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  88%|████████▊ | 45/51 [02:08<00:14,  2.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 193 works for E. L. Piner\n",
            "Extracted data for 193 works\n",
            "\n",
            "Processing: Randy L. Diehl (https://openalex.org/A5077670401)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  90%|█████████ | 46/51 [02:10<00:11,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 154 works for Randy L. Diehl\n",
            "Extracted data for 154 works\n",
            "\n",
            "Processing: Martin Burtscher (https://openalex.org/A5103125276)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  92%|█████████▏| 47/51 [02:11<00:08,  2.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 146 works for Martin Burtscher\n",
            "Extracted data for 146 works\n",
            "\n",
            "Processing: Madan M. Dey (https://openalex.org/A5039339664)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  94%|█████████▍| 48/51 [02:14<00:06,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 188 works for Madan M. Dey\n",
            "Extracted data for 188 works\n",
            "\n",
            "Processing: Ravi Droopad (https://openalex.org/A5019347685)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  96%|█████████▌| 49/51 [02:17<00:04,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 273 works for Ravi Droopad\n",
            "Extracted data for 273 works\n",
            "\n",
            "Processing: Jill D. Pruetz (https://openalex.org/A5016263840)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing researchers:  98%|█████████▊| 50/51 [02:19<00:02,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 128 works for Jill D. Pruetz\n",
            "Extracted data for 128 works\n",
            "\n",
            "Processing: Tahir Ekin (https://openalex.org/A5088154684)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing researchers: 100%|██████████| 51/51 [02:20<00:00,  2.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 47 works for Tahir Ekin\n",
            "Extracted data for 47 works\n",
            "\n",
            "Creating DataFrame with 9086 total works...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data saved to: texas_state_top_50_researchers_works.csv\n",
            "\n",
            "==================================================\n",
            "SUMMARY STATISTICS\n",
            "==================================================\n",
            "Total researchers processed: 51\n",
            "Total works: 9086\n",
            "Works with abstracts: 5139\n",
            "Works without abstracts: 3947\n",
            "Total citations: 394,702\n",
            "Average citations per work: 43.4\n",
            "Open access works: 2532\n",
            "Publication year range: 1896 - 2025\n",
            "\n",
            "Top 10 researchers by number of works:\n",
            "  Manfred Schartl: 667 works\n",
            "  Karl Stephan: 482 works\n",
            "  Togay Ozbakkaloglu: 353 works\n",
            "  Brady T. West: 342 works\n",
            "  Subasish Das: 331 works\n",
            "  Eric Kirby: 280 works\n",
            "  Ravi Droopad: 273 works\n",
            "  Robert McLean: 266 works\n",
            "  M. Holtz: 248 works\n",
            "  James P. LeSage: 243 works\n",
            "\n",
            "Tahir Ekin's works: 47\n",
            "\n",
            "DataFrame shape: (9086, 17)\n",
            "Columns: ['researcher_id', 'researcher_name', 'work_id', 'title', 'abstract', 'topics', 'topic_scores', 'concepts', 'publication_year', 'work_type', 'doi', 'citations', 'source_name', 'is_open_access', 'oa_status', 'has_abstract', 'num_topics']\n",
            "\n",
            "✅ Success! Data extracted and saved.\n",
            "📊 9086 total works from 51 researchers\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "import json\n",
        "\n",
        "# Configuration\n",
        "TAHIR_EKIN_ID = \"A5088154684\"  # Tahir Ekin's researcher ID\n",
        "EMAIL = \"your_email@example.com\"  # Replace with your actual email\n",
        "MAX_RESEARCHERS = 50\n",
        "\n",
        "def call_openalex_api(endpoint, params=None):\n",
        "    \"\"\"Make API calls with rate limiting and error handling\"\"\"\n",
        "    base_url = f\"https://api.openalex.org/{endpoint}\"\n",
        "    headers = {'User-Agent': f'mailto:{EMAIL}'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        time.sleep(0.2)  # Rate limiting\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling {endpoint} API: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def get_texas_state_id():\n",
        "    \"\"\"Get Texas State University's OpenAlex ID.\"\"\"\n",
        "    params = {\n",
        "        'filter': 'display_name.search:texas state university',\n",
        "        'per-page': 1\n",
        "    }\n",
        "    response = call_openalex_api('institutions', params)\n",
        "    if response and 'results' in response and response['results']:\n",
        "        return response['results'][0]['id']\n",
        "    return None\n",
        "\n",
        "def fetch_top_researchers(institution_id, max_researchers=50):\n",
        "    \"\"\"Fetch the top cited researchers affiliated with an institution.\"\"\"\n",
        "    all_researchers = []\n",
        "    cursor = '*'\n",
        "\n",
        "    while cursor and len(all_researchers) < max_researchers:\n",
        "        try:\n",
        "            params = {\n",
        "                'filter': f'last_known_institutions.id:{institution_id}',\n",
        "                'per-page': min(100, max_researchers - len(all_researchers)),\n",
        "                'sort': 'cited_by_count:desc',\n",
        "                'cursor': cursor\n",
        "            }\n",
        "\n",
        "            response = call_openalex_api('authors', params)\n",
        "\n",
        "            if not response or 'results' not in response:\n",
        "                break\n",
        "\n",
        "            researchers = response['results']\n",
        "            if not researchers:\n",
        "                break\n",
        "\n",
        "            all_researchers.extend(researchers)\n",
        "\n",
        "            if len(all_researchers) >= max_researchers:\n",
        "                all_researchers = all_researchers[:max_researchers]\n",
        "                break\n",
        "\n",
        "            cursor = response.get('meta', {}).get('next_cursor')\n",
        "\n",
        "            if not cursor:\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching researchers: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    return all_researchers\n",
        "\n",
        "def get_researcher_by_id(researcher_id):\n",
        "    \"\"\"Get a specific researcher by ID\"\"\"\n",
        "    try:\n",
        "        clean_id = researcher_id.split('/')[-1] if '/' in researcher_id else researcher_id\n",
        "        response = call_openalex_api(f'authors/{clean_id}')\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching researcher {researcher_id}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def reconstruct_abstract(inverted_index: Dict) -> str:\n",
        "    \"\"\"Reconstruct abstract text from OpenAlex inverted index format.\"\"\"\n",
        "    if not inverted_index:\n",
        "        return \"\"\n",
        "\n",
        "    word_positions = []\n",
        "    for word, positions in inverted_index.items():\n",
        "        for pos in positions:\n",
        "            word_positions.append((pos, word))\n",
        "\n",
        "    word_positions.sort(key=lambda x: x[0])\n",
        "    words = [word for _, word in word_positions]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "def get_researcher_works(researcher_id):\n",
        "    \"\"\"Get all works for a researcher using cursor pagination\"\"\"\n",
        "    clean_id = researcher_id.split('/')[-1] if '/' in researcher_id else researcher_id\n",
        "\n",
        "    base_params = {\n",
        "        'filter': f'author.id:{clean_id}',\n",
        "        'per-page': 200,\n",
        "        'sort': 'cited_by_count:desc'\n",
        "    }\n",
        "\n",
        "    all_works = []\n",
        "    cursor = '*'\n",
        "\n",
        "    while cursor:\n",
        "        try:\n",
        "            params = base_params.copy()\n",
        "            params['cursor'] = cursor\n",
        "\n",
        "            response = call_openalex_api('works', params)\n",
        "\n",
        "            if not response or 'results' not in response:\n",
        "                break\n",
        "\n",
        "            works = response['results']\n",
        "            if not works:\n",
        "                break\n",
        "\n",
        "            all_works.extend(works)\n",
        "\n",
        "            cursor = response.get('meta', {}).get('next_cursor')\n",
        "\n",
        "            if not cursor:\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching works for researcher {clean_id}: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    return all_works\n",
        "\n",
        "def extract_work_data(works: List[Dict], researcher_id: str, researcher_name: str) -> List[Dict]:\n",
        "    \"\"\"Extract work data including abstracts, topics, titles, and publication years.\"\"\"\n",
        "    extracted_data = []\n",
        "\n",
        "    for work in works:\n",
        "        # Extract basic work information\n",
        "        work_id = work.get('id', '')\n",
        "        title = work.get('display_name', '')\n",
        "        publication_year = work.get('publication_year', None)\n",
        "        doi = work.get('doi', '')\n",
        "        citations = work.get('cited_by_count', 0) or 0\n",
        "\n",
        "        # Extract and reconstruct abstract\n",
        "        abstract_inverted = work.get('abstract_inverted_index', {})\n",
        "        abstract = reconstruct_abstract(abstract_inverted)\n",
        "\n",
        "        # Extract topics\n",
        "        topics = work.get('topics', [])\n",
        "        topic_names = []\n",
        "        topic_scores = []\n",
        "\n",
        "        if topics and isinstance(topics, list):\n",
        "            for topic in topics[:5]:  # Limit to top 5 topics\n",
        "                if topic and isinstance(topic, dict):\n",
        "                    topic_names.append(topic.get('display_name', ''))\n",
        "                    topic_scores.append(topic.get('score', 0))\n",
        "\n",
        "        # Join topics with semicolon separator\n",
        "        topics_str = '; '.join(topic_names) if topic_names else ''\n",
        "        topic_scores_str = '; '.join([str(score) for score in topic_scores]) if topic_scores else ''\n",
        "\n",
        "        # Extract source information\n",
        "        primary_location = work.get('primary_location')\n",
        "        source_name = ''\n",
        "        if primary_location and isinstance(primary_location, dict):\n",
        "            source = primary_location.get('source')\n",
        "            if source and isinstance(source, dict):\n",
        "                source_name = source.get('display_name', '')\n",
        "\n",
        "        # Extract open access information\n",
        "        oa_info = work.get('open_access', {})\n",
        "        is_oa = False\n",
        "        oa_status = ''\n",
        "        if oa_info and isinstance(oa_info, dict):\n",
        "            is_oa = oa_info.get('is_oa', False)\n",
        "            oa_status = oa_info.get('oa_status', '')\n",
        "\n",
        "        # Extract work type\n",
        "        work_type = work.get('type', '')\n",
        "\n",
        "        # Extract concepts (different from topics)\n",
        "        concepts = work.get('concepts', [])\n",
        "        concept_names = []\n",
        "        if concepts and isinstance(concepts, list):\n",
        "            for concept in concepts[:5]:  # Limit to top 5 concepts\n",
        "                if concept and isinstance(concept, dict):\n",
        "                    concept_names.append(concept.get('display_name', ''))\n",
        "\n",
        "        concepts_str = '; '.join(concept_names) if concept_names else ''\n",
        "\n",
        "        extracted_data.append({\n",
        "            'researcher_id': researcher_id,\n",
        "            'researcher_name': researcher_name,\n",
        "            'work_id': work_id,\n",
        "            'title': title,\n",
        "            'abstract': abstract,\n",
        "            'topics': topics_str,\n",
        "            'topic_scores': topic_scores_str,\n",
        "            'concepts': concepts_str,\n",
        "            'publication_year': publication_year,\n",
        "            'work_type': work_type,\n",
        "            'doi': doi,\n",
        "            'citations': citations,\n",
        "            'source_name': source_name,\n",
        "            'is_open_access': is_oa,\n",
        "            'oa_status': oa_status,\n",
        "            'has_abstract': bool(abstract),\n",
        "            'num_topics': len(topic_names)\n",
        "        })\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to extract works data for top Texas State University researchers\"\"\"\n",
        "\n",
        "    print(\"IMPORTANT: Please update the EMAIL variable with your actual email address!\")\n",
        "    print(f\"Current email: {EMAIL}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Step 1: Get Texas State University ID\n",
        "    print(\"Getting Texas State University ID...\")\n",
        "    texas_state_id = get_texas_state_id()\n",
        "    if not texas_state_id:\n",
        "        print(\"Could not find Texas State University ID\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found Texas State University ID: {texas_state_id}\")\n",
        "\n",
        "    # Step 2: Fetch top researchers\n",
        "    print(f\"\\nFetching top {MAX_RESEARCHERS} cited researchers from Texas State University...\")\n",
        "    researchers = fetch_top_researchers(texas_state_id, MAX_RESEARCHERS)\n",
        "\n",
        "    if not researchers:\n",
        "        print(\"Failed to fetch researchers\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(researchers)} researchers\")\n",
        "\n",
        "    # Step 3: Check if Tahir Ekin is in the list, if not add him\n",
        "    researcher_ids = [r['id'] for r in researchers]\n",
        "    tahir_ekin_included = any(TAHIR_EKIN_ID in rid for rid in researcher_ids)\n",
        "\n",
        "    if not tahir_ekin_included:\n",
        "        print(f\"\\nTahir Ekin (ID: {TAHIR_EKIN_ID}) not in top {MAX_RESEARCHERS}, adding him...\")\n",
        "        tahir_ekin_data = get_researcher_by_id(TAHIR_EKIN_ID)\n",
        "        if tahir_ekin_data:\n",
        "            researchers.append(tahir_ekin_data)\n",
        "            print(\"Tahir Ekin added successfully\")\n",
        "        else:\n",
        "            print(\"Failed to fetch Tahir Ekin's data\")\n",
        "    else:\n",
        "        print(f\"Tahir Ekin is already in the top {MAX_RESEARCHERS} researchers\")\n",
        "\n",
        "    # Step 4: Process each researcher and get their works\n",
        "    all_works_data = []\n",
        "\n",
        "    print(f\"\\nProcessing {len(researchers)} researchers and their works...\")\n",
        "\n",
        "    for i, researcher in enumerate(tqdm(researchers, desc=\"Processing researchers\")):\n",
        "        try:\n",
        "            researcher_id = researcher['id']\n",
        "            researcher_name = researcher['display_name']\n",
        "\n",
        "            print(f\"\\nProcessing: {researcher_name} ({researcher_id})\")\n",
        "\n",
        "            # Get all works for this researcher\n",
        "            works = get_researcher_works(researcher_id)\n",
        "\n",
        "            if works:\n",
        "                print(f\"Found {len(works)} works for {researcher_name}\")\n",
        "\n",
        "                # Extract work data\n",
        "                work_data = extract_work_data(works, researcher_id, researcher_name)\n",
        "                all_works_data.extend(work_data)\n",
        "\n",
        "                print(f\"Extracted data for {len(work_data)} works\")\n",
        "            else:\n",
        "                print(f\"No works found for {researcher_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing researcher {researcher.get('display_name', 'Unknown')}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Step 5: Create DataFrame and save to CSV\n",
        "    if all_works_data:\n",
        "        print(f\"\\nCreating DataFrame with {len(all_works_data)} total works...\")\n",
        "        df = pd.DataFrame(all_works_data)\n",
        "\n",
        "        # Save to CSV\n",
        "        filename = f\"texas_state_top_{MAX_RESEARCHERS}_researchers_works.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "        print(f\"\\nData saved to: {filename}\")\n",
        "\n",
        "        # Display summary statistics\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SUMMARY STATISTICS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        print(f\"Total researchers processed: {df['researcher_id'].nunique()}\")\n",
        "        print(f\"Total works: {len(df)}\")\n",
        "        print(f\"Works with abstracts: {df['has_abstract'].sum()}\")\n",
        "        print(f\"Works without abstracts: {(~df['has_abstract']).sum()}\")\n",
        "        print(f\"Total citations: {df['citations'].sum():,}\")\n",
        "        print(f\"Average citations per work: {df['citations'].mean():.1f}\")\n",
        "        print(f\"Open access works: {df['is_open_access'].sum()}\")\n",
        "\n",
        "        # Year distribution\n",
        "        valid_years = df[df['publication_year'].notna() & (df['publication_year'] > 0)]\n",
        "        if not valid_years.empty:\n",
        "            print(f\"Publication year range: {valid_years['publication_year'].min():.0f} - {valid_years['publication_year'].max():.0f}\")\n",
        "\n",
        "        # Top researchers by total works\n",
        "        print(f\"\\nTop 10 researchers by number of works:\")\n",
        "        researcher_counts = df['researcher_name'].value_counts().head(10)\n",
        "        for name, count in researcher_counts.items():\n",
        "            print(f\"  {name}: {count} works\")\n",
        "\n",
        "        # Check if Tahir Ekin is included\n",
        "        tahir_works = df[df['researcher_name'].str.contains('Tahir', case=False, na=False)]\n",
        "        if not tahir_works.empty:\n",
        "            print(f\"\\nTahir Ekin's works: {len(tahir_works)}\")\n",
        "\n",
        "        print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    else:\n",
        "        print(\"No works data extracted\")\n",
        "        return None\n",
        "\n",
        "# Run the extraction\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Texas State University Researchers Works Extractor\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"This script will:\")\n",
        "    print(f\"1. Get top {MAX_RESEARCHERS} cited researchers from Texas State University\")\n",
        "    print(\"2. Include Tahir Ekin even if not in top 50\")\n",
        "    print(\"3. Extract all works data for each researcher\")\n",
        "    print(\"4. Save combined data to CSV file\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    df = main()\n",
        "\n",
        "    if df is not None:\n",
        "        print(f\"\\n✅ Success! Data extracted and saved.\")\n",
        "        print(f\"📊 {len(df)} total works from {df['researcher_id'].nunique()} researchers\")\n",
        "    else:\n",
        "        print(\"\\n❌ Failed to extract data\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufwmnxTAj_V7",
        "outputId": "b7d794a7-7165-428c-86e5-326c71314c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['researcher_id', 'researcher_name', 'work_id', 'title', 'abstract',\n",
              "       'topics', 'topic_scores', 'concepts', 'publication_year', 'work_type',\n",
              "       'doi', 'citations', 'source_name', 'is_open_access', 'oa_status',\n",
              "       'has_abstract', 'num_topics'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This CSV file contains comprehensive data about every published work (research papers, articles, etc.) from the top 50 most-cited researchers at Texas State University, plus Tahir Ekin's works. Each row represents a single publication with details including the work's title, reconstructed abstract, research topics with scores, publication year, citation count, source journal/venue, and open access status, along with the researcher's name and ID who authored it.\n",
        "\n",
        "(9086, 17)-> Shape\n",
        "\n",
        "Index(['researcher_id', 'researcher_name', 'work_id', 'title', 'abstract',\n",
        "       'topics', 'topic_scores', 'concepts', 'publication_year', 'work_type',\n",
        "       'doi', 'citations', 'source_name', 'is_open_access', 'oa_status',\n",
        "       'has_abstract', 'num_topics'],\n",
        "      dtype='object') -> columns"
      ],
      "metadata": {
        "id": "Yd0V5lkfkDsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import joblib\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class ResearcherProfileDatastore:\n",
        "    \"\"\"\n",
        "    A class to create and manage researcher profile embeddings and metadata.\n",
        "\n",
        "    Storage Structure:\n",
        "    ├── researcher_profiles_metadata.parquet     # Main metadata with recency weights\n",
        "    ├── researcher_embeddings.npy               # All embeddings as numpy array (N x 384)\n",
        "    ├── embedding_index.json                    # Maps work_id -> array position\n",
        "    ├── researcher_index.json                   # Maps researcher_id -> list of work_ids\n",
        "    └── datastore_info.json                     # Metadata about the datastore\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drive_path=\"/content/drive/My Drive/datastore\"):\n",
        "        self.drive_path = drive_path\n",
        "        self.model = None\n",
        "        self.current_year = datetime.now().year\n",
        "\n",
        "    def load_model(self, model_name='all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Load the sentence transformer model.\n",
        "        Using 'all-MiniLM-L6-v2': 384 dimensions, good balance of speed and quality.\n",
        "        \"\"\"\n",
        "        print(f\"Loading sentence transformer model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        print(f\"Model loaded. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "        return self.model\n",
        "\n",
        "    def calculate_recency_weight(self, publication_year):\n",
        "        \"\"\"\n",
        "        Calculate recency weight: Wt = max(0, 1 - (CurrentYear - PublicationYear) / 10)\n",
        "\n",
        "        Args:\n",
        "            publication_year (int): Year of publication\n",
        "\n",
        "        Returns:\n",
        "            float: Recency weight between 0 and 1\n",
        "        \"\"\"\n",
        "        if pd.isna(publication_year) or publication_year == 0:\n",
        "            return 0.0\n",
        "\n",
        "        weight = max(0, 1 - (self.current_year - publication_year) / 10)\n",
        "        return round(weight, 4)\n",
        "\n",
        "    def create_text_for_embedding(self, title, abstract):\n",
        "        \"\"\"\n",
        "        Combine title and abstract for embedding.\n",
        "\n",
        "        Args:\n",
        "            title (str): Paper title\n",
        "            abstract (str): Paper abstract\n",
        "\n",
        "        Returns:\n",
        "            str: Combined text for embedding\n",
        "        \"\"\"\n",
        "        title = str(title) if pd.notna(title) else \"\"\n",
        "        abstract = str(abstract) if pd.notna(abstract) else \"\"\n",
        "\n",
        "        # Combine title and abstract with separator\n",
        "        if abstract:\n",
        "            return f\"{title}. {abstract}\"\n",
        "        else:\n",
        "            return title\n",
        "\n",
        "    def process_papers(self, csv_file_path):\n",
        "        \"\"\"\n",
        "        Process all papers from CSV file to create embeddings and metadata.\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (str): Path to the CSV file with researcher works\n",
        "\n",
        "        Returns:\n",
        "            tuple: (metadata_df, embeddings_array, embedding_index, researcher_index)\n",
        "        \"\"\"\n",
        "        print(\"Loading data from CSV...\")\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        print(f\"Loaded {len(df)} papers from {df['researcher_id'].nunique()} researchers\")\n",
        "\n",
        "        # Calculate recency weights\n",
        "        print(\"Calculating recency weights...\")\n",
        "        df['recency_weight'] = df['publication_year'].apply(self.calculate_recency_weight)\n",
        "\n",
        "        # Prepare text for embeddings\n",
        "        print(\"Preparing text for embeddings...\")\n",
        "        df['embedding_text'] = df.apply(\n",
        "            lambda row: self.create_text_for_embedding(row['title'], row['abstract']),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Filter out papers with no text\n",
        "        valid_papers = df[df['embedding_text'].str.len() > 0].copy()\n",
        "        print(f\"Processing {len(valid_papers)} papers with valid text\")\n",
        "\n",
        "        if len(valid_papers) == 0:\n",
        "            raise ValueError(\"No papers with valid text found!\")\n",
        "\n",
        "        # Load model if not already loaded\n",
        "        if self.model is None:\n",
        "            self.load_model()\n",
        "\n",
        "        # Generate embeddings\n",
        "        print(\"Generating embeddings...\")\n",
        "        texts = valid_papers['embedding_text'].tolist()\n",
        "\n",
        "        # Process in batches to avoid memory issues\n",
        "        batch_size = 100\n",
        "        all_embeddings = []\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating embeddings\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=False)\n",
        "            all_embeddings.append(batch_embeddings)\n",
        "\n",
        "        # Combine all embeddings\n",
        "        embeddings_array = np.vstack(all_embeddings)\n",
        "        print(f\"Generated embeddings shape: {embeddings_array.shape}\")\n",
        "\n",
        "        # Create embedding index (work_id -> array position)\n",
        "        embedding_index = {}\n",
        "        researcher_index = {}\n",
        "\n",
        "        for idx, (_, row) in enumerate(valid_papers.iterrows()):\n",
        "            work_id = row['work_id']\n",
        "            researcher_id = row['researcher_id']\n",
        "\n",
        "            # Map work_id to embedding position\n",
        "            embedding_index[work_id] = idx\n",
        "\n",
        "            # Group by researcher_id\n",
        "            if researcher_id not in researcher_index:\n",
        "                researcher_index[researcher_id] = []\n",
        "            researcher_index[researcher_id].append(work_id)\n",
        "\n",
        "        print(f\"Created embedding index for {len(embedding_index)} papers\")\n",
        "        print(f\"Created researcher index for {len(researcher_index)} researchers\")\n",
        "\n",
        "        return valid_papers, embeddings_array, embedding_index, researcher_index\n",
        "\n",
        "    def save_datastore(self, metadata_df, embeddings_array, embedding_index, researcher_index):\n",
        "        \"\"\"\n",
        "        Save all components of the researcher profile datastore.\n",
        "\n",
        "        Args:\n",
        "            metadata_df (pd.DataFrame): Paper metadata with recency weights\n",
        "            embeddings_array (np.ndarray): All embeddings\n",
        "            embedding_index (dict): work_id -> array position mapping\n",
        "            researcher_index (dict): researcher_id -> list of work_ids mapping\n",
        "        \"\"\"\n",
        "        print(\"Saving researcher profile datastore...\")\n",
        "\n",
        "        # Ensure the datastore directory exists\n",
        "        os.makedirs(self.drive_path, exist_ok=True)\n",
        "\n",
        "        # Save metadata as Parquet (much faster than CSV)\n",
        "        metadata_path = os.path.join(self.drive_path, \"researcher_profiles_metadata.parquet\")\n",
        "        metadata_df.to_parquet(metadata_path, index=False)\n",
        "        print(f\"✅ Saved metadata: {metadata_path}\")\n",
        "\n",
        "        # Save embeddings as numpy array\n",
        "        embeddings_path = os.path.join(self.drive_path, \"researcher_embeddings.npy\")\n",
        "        np.save(embeddings_path, embeddings_array)\n",
        "        print(f\"✅ Saved embeddings: {embeddings_path}\")\n",
        "\n",
        "        # Save embedding index\n",
        "        embedding_index_path = os.path.join(self.drive_path, \"embedding_index.json\")\n",
        "        with open(embedding_index_path, 'w') as f:\n",
        "            json.dump(embedding_index, f, indent=2)\n",
        "        print(f\"✅ Saved embedding index: {embedding_index_path}\")\n",
        "\n",
        "        # Save researcher index\n",
        "        researcher_index_path = os.path.join(self.drive_path, \"researcher_index.json\")\n",
        "        with open(researcher_index_path, 'w') as f:\n",
        "            json.dump(researcher_index, f, indent=2)\n",
        "        print(f\"✅ Saved researcher index: {researcher_index_path}\")\n",
        "\n",
        "        # Save datastore info\n",
        "        datastore_info = {\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"total_papers\": len(metadata_df),\n",
        "            \"total_researchers\": metadata_df['researcher_id'].nunique(),\n",
        "            \"embedding_dimensions\": embeddings_array.shape[1],\n",
        "            \"model_used\": \"all-MiniLM-L6-v2\",\n",
        "            \"current_year_for_recency\": self.current_year,\n",
        "            \"files\": {\n",
        "                \"metadata\": \"researcher_profiles_metadata.parquet\",\n",
        "                \"embeddings\": \"researcher_embeddings.npy\",\n",
        "                \"embedding_index\": \"embedding_index.json\",\n",
        "                \"researcher_index\": \"researcher_index.json\"\n",
        "            },\n",
        "            \"usage_instructions\": {\n",
        "                \"load_metadata\": f\"pd.read_parquet('{self.drive_path}/researcher_profiles_metadata.parquet')\",\n",
        "                \"load_embeddings\": f\"np.load('{self.drive_path}/researcher_embeddings.npy')\",\n",
        "                \"load_indices\": f\"json.load(open('{self.drive_path}/embedding_index.json'))\",\n",
        "                \"get_embedding_by_work_id\": \"embeddings[embedding_index[work_id]]\",\n",
        "                \"get_researcher_papers\": \"researcher_index[researcher_id]\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        info_path = os.path.join(self.drive_path, \"datastore_info.json\")\n",
        "        with open(info_path, 'w') as f:\n",
        "            json.dump(datastore_info, f, indent=2)\n",
        "        print(f\"✅ Saved datastore info: {info_path}\")\n",
        "\n",
        "        return datastore_info\n",
        "\n",
        "    def load_datastore(self):\n",
        "        \"\"\"\n",
        "        Load the complete datastore for future use.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (metadata_df, embeddings_array, embedding_index, researcher_index, datastore_info)\n",
        "        \"\"\"\n",
        "        print(\"Loading researcher profile datastore...\")\n",
        "\n",
        "        # Load metadata\n",
        "        metadata_path = os.path.join(self.drive_path, \"researcher_profiles_metadata.parquet\")\n",
        "        metadata_df = pd.read_parquet(metadata_path)\n",
        "\n",
        "        # Load embeddings\n",
        "        embeddings_path = os.path.join(self.drive_path, \"researcher_embeddings.npy\")\n",
        "        embeddings_array = np.load(embeddings_path)\n",
        "\n",
        "        # Load indices\n",
        "        embedding_index_path = os.path.join(self.drive_path, \"embedding_index.json\")\n",
        "        with open(embedding_index_path, 'r') as f:\n",
        "            embedding_index = json.load(f)\n",
        "\n",
        "        researcher_index_path = os.path.join(self.drive_path, \"researcher_index.json\")\n",
        "        with open(researcher_index_path, 'r') as f:\n",
        "            researcher_index = json.load(f)\n",
        "\n",
        "        # Load info\n",
        "        info_path = os.path.join(self.drive_path, \"datastore_info.json\")\n",
        "        with open(info_path, 'r') as f:\n",
        "            datastore_info = json.load(f)\n",
        "\n",
        "        print(f\"✅ Loaded datastore with {len(metadata_df)} papers and {embeddings_array.shape} embeddings\")\n",
        "\n",
        "        return metadata_df, embeddings_array, embedding_index, researcher_index, datastore_info\n",
        "\n",
        "    def get_researcher_embeddings(self, researcher_id, embedding_index, researcher_index, embeddings_array):\n",
        "        \"\"\"\n",
        "        Get all embeddings for a specific researcher.\n",
        "\n",
        "        Args:\n",
        "            researcher_id (str): The researcher ID\n",
        "            embedding_index (dict): work_id -> position mapping\n",
        "            researcher_index (dict): researcher_id -> work_ids mapping\n",
        "            embeddings_array (np.ndarray): All embeddings\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Embeddings for the researcher's papers\n",
        "        \"\"\"\n",
        "        if researcher_id not in researcher_index:\n",
        "            return np.array([])\n",
        "\n",
        "        work_ids = researcher_index[researcher_id]\n",
        "        positions = [embedding_index[work_id] for work_id in work_ids if work_id in embedding_index]\n",
        "\n",
        "        return embeddings_array[positions]\n",
        "\n",
        "    def display_summary(self, metadata_df, datastore_info):\n",
        "        \"\"\"Display summary statistics of the created datastore.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RESEARCHER PROFILE DATASTORE SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"📊 Total papers: {len(metadata_df):,}\")\n",
        "        print(f\"👥 Total researchers: {metadata_df['researcher_id'].nunique()}\")\n",
        "        print(f\"🧠 Embedding dimensions: {datastore_info['embedding_dimensions']}\")\n",
        "        print(f\"📅 Current year (for recency): {datastore_info['current_year_for_recency']}\")\n",
        "\n",
        "        print(f\"\\n📝 Papers with abstracts: {metadata_df['has_abstract'].sum():,}\")\n",
        "        print(f\"📄 Papers without abstracts: {(~metadata_df['has_abstract']).sum():,}\")\n",
        "\n",
        "        print(f\"\\n⚡ Avg recency weight: {metadata_df['recency_weight'].mean():.3f}\")\n",
        "        print(f\"📊 Recency weight distribution:\")\n",
        "        print(f\"   High (>0.8): {(metadata_df['recency_weight'] > 0.8).sum():,} papers\")\n",
        "        print(f\"   Medium (0.4-0.8): {((metadata_df['recency_weight'] > 0.4) & (metadata_df['recency_weight'] <= 0.8)).sum():,} papers\")\n",
        "        print(f\"   Low (0-0.4): {(metadata_df['recency_weight'] <= 0.4).sum():,} papers\")\n",
        "\n",
        "        # Top researchers by number of papers\n",
        "        top_researchers = metadata_df['researcher_name'].value_counts().head(5)\n",
        "        print(f\"\\n🔬 Top 5 researchers by paper count:\")\n",
        "        for name, count in top_researchers.items():\n",
        "            print(f\"   {name}: {count} papers\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to create the researcher profile datastore.\"\"\"\n",
        "\n",
        "    # Initialize the datastore\n",
        "    datastore = ResearcherProfileDatastore()\n",
        "\n",
        "    # Specify your CSV file path (update this!)\n",
        "    csv_file_path = \"/content/texas_state_top_50_researchers_works.csv\"\n",
        "\n",
        "    print(\"🚀 Creating Researcher Profile Datastore\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # Process papers and create embeddings\n",
        "        metadata_df, embeddings_array, embedding_index, researcher_index = datastore.process_papers(csv_file_path)\n",
        "\n",
        "        # Save everything\n",
        "        datastore_info = datastore.save_datastore(metadata_df, embeddings_array, embedding_index, researcher_index)\n",
        "\n",
        "        # Display summary\n",
        "        datastore.display_summary(metadata_df, datastore_info)\n",
        "\n",
        "        print(f\"\\n✅ SUCCESS! Researcher Profile Datastore created.\")\n",
        "        print(f\"📁 Files saved to: {datastore.drive_path}\")\n",
        "\n",
        "        return datastore_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage functions\n",
        "def example_usage():\n",
        "    \"\"\"Show how to use the datastore after it's created.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXAMPLE: HOW TO USE THE DATASTORE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    code_examples = \"\"\"\n",
        "# 1. Load the complete datastore\n",
        "datastore = ResearcherProfileDatastore()\n",
        "metadata_df, embeddings, embedding_index, researcher_index, info = datastore.load_datastore()\n",
        "\n",
        "# 2. Get embedding for a specific paper\n",
        "work_id = \"https://openalex.org/W1234567890\"\n",
        "if work_id in embedding_index:\n",
        "    paper_embedding = embeddings[embedding_index[work_id]]\n",
        "    print(f\"Embedding shape: {paper_embedding.shape}\")\n",
        "\n",
        "# 3. Get all papers for a researcher\n",
        "researcher_id = \"https://openalex.org/A5088154684\"  # Tahir Ekin\n",
        "if researcher_id in researcher_index:\n",
        "    work_ids = researcher_index[researcher_id]\n",
        "    researcher_papers = metadata_df[metadata_df['work_id'].isin(work_ids)]\n",
        "    print(f\"Researcher has {len(researcher_papers)} papers\")\n",
        "\n",
        "# 4. Get embeddings for a researcher's papers\n",
        "researcher_embeddings = datastore.get_researcher_embeddings(\n",
        "    researcher_id, embedding_index, researcher_index, embeddings\n",
        ")\n",
        "print(f\"Researcher embeddings shape: {researcher_embeddings.shape}\")\n",
        "\n",
        "# 5. Find papers with high recency weights\n",
        "recent_papers = metadata_df[metadata_df['recency_weight'] > 0.8]\n",
        "print(f\"Found {len(recent_papers)} recent papers\")\n",
        "\n",
        "# 6. Calculate weighted average embedding for a researcher\n",
        "if len(researcher_embeddings) > 0:\n",
        "    weights = researcher_papers['recency_weight'].values\n",
        "    weighted_avg = np.average(researcher_embeddings, axis=0, weights=weights)\n",
        "    print(f\"Weighted average embedding shape: {weighted_avg.shape}\")\n",
        "\"\"\"\n",
        "\n",
        "    print(code_examples)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the datastore\n",
        "    result = main()\n",
        "\n",
        "    if result:\n",
        "        # Show usage examples\n",
        "        example_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "fwAYFsa8kDAf",
        "outputId": "95d3455a-69e6-4c7d-b406-cb5e30a0a21c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2321526037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from sentence_transformers.backend import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mexport_dynamic_quantized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mexport_optimized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisable_datasets_caching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_datasets_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_torch_npu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .utils import (\n\u001b[1;32m     29\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .args_doc import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mClassAttrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mClassDocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/args_doc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m from .doc import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/regex/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/regex/regex.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;31m# Internals.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regex_core\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_regex_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthreading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRLock\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_RLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/regex/_regex_core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Flags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRegexFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntFlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mASCII\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0x80\u001b[0m          \u001b[0;31m# Assume ASCII locale.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBESTMATCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0x1000\u001b[0m    \u001b[0;31m# Best fuzzy match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/regex/_regex_core.py\u001b[0m in \u001b[0;36mRegexFlag\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Flags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRegexFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntFlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mASCII\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0x80\u001b[0m          \u001b[0;31m# Assume ASCII locale.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBESTMATCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0x1000\u001b[0m    \u001b[0;31m# Best fuzzy match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/enum.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnon_auto_store\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmore_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation Summary:\n",
        "Why: We chose a file-based Google Drive approach for zero setup complexity, allowing immediate MVP development without cloud infrastructure, billing, or authentication overhead.\n",
        "\n",
        "How: Stores paper metadata in Parquet format, 384-dimensional embeddings as NumPy arrays, and creates JSON indices mapping work_id→array_position and researcher_id→work_ids for fast lookups.\n",
        "\n",
        "Cons: Hits memory/performance walls around 10k-50k papers due to loading entire embedding arrays into RAM, lacks similarity search optimization, and has no concurrent access or query filtering capabilities.\n",
        "\n",
        "Other Issues: Google Drive has 15GB storage limits and slow transfer speeds, JSON parsing becomes expensive with large indices, no automatic backup/versioning, and the monolithic file structure makes partial updates impossible.\n",
        "\n",
        "Migration Path: Will need to move to proper vector database (Pinecone/Weaviate) or PostgreSQL with pgvector extension when scaling beyond prototype phase."
      ],
      "metadata": {
        "id": "yZ0ePCNQnN3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a new folder in drive called datastrore and store all these files inside there and print new paths.\n",
        "# ✅ Saved metadata: /content/drive/My Drive/researcher_profiles_metadata.parquet\n",
        "# ✅ Saved embeddings: /content/drive/My Drive/researcher_embeddings.npy\n",
        "# ✅ Saved embedding index: /content/drive/My Drive/embedding_index.json\n",
        "# ✅ Saved researcher index: /content/drive/My Drive/researcher_index.json\n",
        "# ✅ Saved datastore info: /content/drive/My Drive/datastore_info.json\n",
        "\n",
        "# Create the new folder if it doesn't exist\n",
        "import os\n",
        "new_folder_path = \"/content/drive/My Drive/datastore\"\n",
        "if not os.path.exists(new_folder_path):\n",
        "    os.makedirs(new_folder_path)\n",
        "    print(f\"Created new folder: {new_folder_path}\")\n",
        "\n",
        "# Define the old and new paths\n",
        "old_paths = [\n",
        "    \"/content/drive/My Drive/researcher_profiles_metadata.parquet\",\n",
        "    \"/content/drive/My Drive/researcher_embeddings.npy\",\n",
        "    \"/content/drive/My Drive/embedding_index.json\",\n",
        "    \"/content/drive/My Drive/researcher_index.json\",\n",
        "    \"/content/drive/My Drive/datastore_info.json\"\n",
        "]\n",
        "\n",
        "new_paths = []\n",
        "\n",
        "# Move each file and store the new path\n",
        "for old_path in old_paths:\n",
        "    filename = os.path.basename(old_path)\n",
        "    new_path = os.path.join(new_folder_path, filename)\n",
        "\n",
        "    # Check if the file exists before attempting to move\n",
        "    if os.path.exists(old_path):\n",
        "        os.rename(old_path, new_path)\n",
        "        print(f\"Moved '{old_path}' to '{new_path}'\")\n",
        "        new_paths.append(new_path)\n",
        "    else:\n",
        "        print(f\"File not found, cannot move: {old_path}\")\n",
        "        # Append the intended new path even if the file wasn't moved,\n",
        "        # so the list of new paths is complete based on the source list.\n",
        "        new_paths.append(new_path)\n",
        "\n",
        "\n",
        "# Print the new paths\n",
        "print(\"\\nNew paths of the moved files:\")\n",
        "for path in new_paths:\n",
        "  print (path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkzNdgj1pwuh",
        "outputId": "e224473c-c8f8-499b-d71c-940acda87f76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new folder: /content/drive/My Drive/datastore\n",
            "File not found, cannot move: /content/drive/My Drive/researcher_profiles_metadata.parquet\n",
            "File not found, cannot move: /content/drive/My Drive/researcher_embeddings.npy\n",
            "File not found, cannot move: /content/drive/My Drive/embedding_index.json\n",
            "File not found, cannot move: /content/drive/My Drive/researcher_index.json\n",
            "File not found, cannot move: /content/drive/My Drive/datastore_info.json\n",
            "\n",
            "New paths of the moved files:\n",
            "/content/drive/My Drive/datastore/researcher_profiles_metadata.parquet\n",
            "/content/drive/My Drive/datastore/researcher_embeddings.npy\n",
            "/content/drive/My Drive/datastore/embedding_index.json\n",
            "/content/drive/My Drive/datastore/researcher_index.json\n",
            "/content/drive/My Drive/datastore/datastore_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gtTJtcUzqsyW",
        "outputId": "0f8c9b9f-d519-4a00-fcd5-12e394289cd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
            "Collecting configobj (from fitz)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting configparser (from fitz)\n",
            "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.11/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from fitz) (5.3.2)\n",
            "Collecting nipype (from fitz)\n",
            "  Downloading nipype-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fitz) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fitz) (2.2.2)\n",
            "Collecting pyxnat (from fitz)\n",
            "  Downloading pyxnat-1.6.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fitz) (1.15.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2->fitz) (3.2.3)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (6.5.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (4.14.0)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (8.2.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.5)\n",
            "Collecting prov>=1.5.2 (from nipype->fitz)\n",
            "  Downloading prov-2.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (2.9.0.post0)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.20.1)\n",
            "Collecting traits>=6.2 (from nipype->fitz)\n",
            "  Downloading traits-7.0.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.18.0)\n",
            "Collecting acres (from nipype->fitz)\n",
            "  Downloading acres-0.5.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting etelemetry>=0.3.1 (from nipype->fitz)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting looseversion!=1.2 (from nipype->fitz)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting puremagic (from nipype->fitz)\n",
            "  Downloading puremagic-1.29-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fitz) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fitz) (2025.2)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (5.4.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (2.32.3)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.3.1->nipype->fitz)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (2025.6.15)\n",
            "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
            "Downloading nipype-1.10.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyxnat-1.6.3-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading prov-2.1.1-py3-none-any.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.9/425.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traits-7.0.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading acres-0.5.0-py3-none-any.whl (12 kB)\n",
            "Downloading puremagic-1.29-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: puremagic, looseversion, traits, rdflib, configparser, configobj, ci-info, acres, pyxnat, prov, etelemetry, nipype, fitz\n",
            "Successfully installed acres-0.5.0 ci-info-0.3.0 configobj-5.0.9 configparser-7.2.0 etelemetry-0.3.1 fitz-0.0.1.dev2 looseversion-1.3.0 nipype-1.10.0 prov-2.1.1 puremagic-1.29 pyxnat-1.6.3 rdflib-7.1.4 traits-7.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "backports"
                ]
              },
              "id": "fc0917ed4209438bbe85e426c74e2116"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "892859ef",
        "outputId": "b21e3d94-4489-4554-fdb4-38e81b03dd8b"
      },
      "source": [
        "# Uninstall fitz to ensure a clean install of PyMuPDF\n",
        "# !pip uninstall -y fitz\n",
        "\n",
        "# Install PyMuPDF, which is the recommended package\n",
        "!pip install PyMuPDF"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UyKZaBOrKMD",
        "outputId": "816d9987-476a-496f-fcbc-c8088ccbb652"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.55.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.55.0-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.55.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V1"
      ],
      "metadata": {
        "id": "GbFbofign5Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. IMPORT LIBRARIES AND DEFINE DATA STRUCTURE\n",
        "# ==============================================================================\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from google.colab import drive, userdata\n",
        "from transformers import pipeline\n",
        "import anthropic\n",
        "from dataclasses import dataclass, asdict\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class StructuredSolicitationObject:\n",
        "    \"\"\"\n",
        "    Structured object containing solicitation metadata and required skills.\n",
        "    \"\"\"\n",
        "    # Metadata\n",
        "    solicitation_id: str\n",
        "    title: str\n",
        "    abstract: str\n",
        "    processed_at: str\n",
        "    pdf_filename: str\n",
        "\n",
        "    # Skills from both paths\n",
        "    narrative_skills: List[str]  # From Claude API (Path A)\n",
        "    formal_topics: List[Dict]    # From OpenAlex classifier (Path B)\n",
        "\n",
        "    # Final combined checklist\n",
        "    required_skills_checklist: List[str]\n",
        "\n",
        "    # Processing details\n",
        "    text_length: int\n",
        "    processing_method: str = \"hybrid_deconstruction\"\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self, filepath: str):\n",
        "        \"\"\"Save to JSON file.\"\"\"\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PDF SOLICITATION PROCESSOR CLASS\n",
        "# ==============================================================================\n",
        "class PDFSolicitationProcessor:\n",
        "    \"\"\"\n",
        "    Processes PDF solicitations from a file path to extract required skills.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.claude_client = None\n",
        "        self.topic_classifier = None\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"Initialize Claude API client and OpenAlex topic classifier.\"\"\"\n",
        "        print(\"Setting up models...\")\n",
        "        try:\n",
        "            api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "            self.claude_client = anthropic.Anthropic(api_key=api_key)\n",
        "            print(\"✅ Claude API client initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Claude API setup failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            print(\"Loading OpenAlex topic classifier...\")\n",
        "            self.topic_classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=\"OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract\"\n",
        "            )\n",
        "            print(\"✅ OpenAlex topic classifier loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Topic classifier setup failed: {e}\")\n",
        "\n",
        "    def _extract_text_from_pdf(self, filepath: str) -> Tuple[str, str, str]:\n",
        "        \"\"\"Extracts text content from a PDF given a file path.\"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"The file was not found at: {filepath}\")\n",
        "\n",
        "        filename = os.path.basename(filepath)\n",
        "        print(f\"📄 Processing: {filename}\")\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(filepath)\n",
        "            full_text = \"\".join([page.get_text() for page in doc])\n",
        "            doc.close()\n",
        "\n",
        "            if not full_text.strip():\n",
        "                 raise ValueError(\"Extracted text is empty. The PDF might be an image.\")\n",
        "\n",
        "            title, abstract = self._extract_title_and_abstract(full_text, filename)\n",
        "            print(f\"✅ Extracted {len(full_text)} characters from PDF.\")\n",
        "            return filename, title, abstract\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting text from PDF '{filename}': {e}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_title_and_abstract(self, full_text: str, filename: str) -> Tuple[str, str]:\n",
        "        \"\"\"Extract title and abstract from full text using heuristics.\"\"\"\n",
        "        lines = [line.strip() for line in full_text.split('\\n') if line.strip()]\n",
        "        title = filename.replace('.pdf', '').replace('_', ' ').replace('-', ' ')\n",
        "        for line in lines[:15]: # Check more lines for title\n",
        "            if 20 < len(line) < 250 and not line.isupper(): # Avoid all-caps headers\n",
        "                title = line\n",
        "                break\n",
        "\n",
        "        abstract = \"\"\n",
        "        abstract_started = False\n",
        "        for line in lines:\n",
        "            line_lower = line.lower()\n",
        "            if not abstract_started and any(marker in line_lower for marker in ['abstract', 'summary', 'overview']):\n",
        "                abstract_started = True\n",
        "                if len(line) > len('abstract') + 10: abstract += line.split(maxsplit=1)[1]\n",
        "                continue\n",
        "            if abstract_started:\n",
        "                abstract += \" \" + line\n",
        "                if len(abstract) > 1500 or any(marker in line_lower for marker in ['introduction', 'background']):\n",
        "                    break\n",
        "        if not abstract: abstract = ' '.join(lines[:10]) # Fallback\n",
        "        return title.strip(), abstract.strip()[:2000] # Increased limit\n",
        "\n",
        "    def extract_narrative_skills_claude(self, text: str) -> List[str]:\n",
        "        \"\"\"Path A: Extract narrative skills using Claude API.\"\"\"\n",
        "        if not self.claude_client:\n",
        "            print(\"⚠️ Claude API not available, skipping narrative skills.\")\n",
        "            return []\n",
        "\n",
        "        prompt = f\"\"\"As an expert research program analyst, identify the 5-7 most critical and distinct areas of expertise required by this research solicitation. Focus on specific technical skills, domain knowledge, and methodological expertise.\n",
        "\n",
        "Solicitation text:\n",
        "---\n",
        "{text}\n",
        "---\n",
        "\n",
        "Provide your response as a numbered list of distinct expertise areas. Each item should be a concise phrase.\n",
        "\"\"\"\n",
        "        try:\n",
        "            print(\"🤖 Calling Claude API for narrative skills...\")\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=1000,\n",
        "                temperature=0.2,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            skills = self._parse_claude_response(response.content[0].text)\n",
        "            print(f\"✅ Extracted {len(skills)} narrative skills from Claude.\")\n",
        "            return skills\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Claude API call failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _parse_claude_response(self, response_text: str) -> List[str]:\n",
        "        \"\"\"Parse Claude's response to extract a list of skills.\"\"\"\n",
        "        skills = []\n",
        "        for line in response_text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if re.match(r'^\\d+\\.\\s*', line):\n",
        "                skill = re.sub(r'^\\d+\\.\\s*', '', line)\n",
        "                skills.append(skill.strip())\n",
        "        return skills[:7]\n",
        "\n",
        "    def extract_formal_topics_openalex(self, title: str, abstract: str) -> List[Dict]:\n",
        "      \"\"\"\n",
        "      Path B: Extract formal topics using OpenAlex classifier with corrected data structure handling.\n",
        "      \"\"\"\n",
        "      if not self.topic_classifier:\n",
        "          print(\"⚠️ Topic classifier not available, skipping formal topics.\")\n",
        "          return []\n",
        "\n",
        "      formatted_text = f\"<TITLE> {title}\\n<ABSTRACT> {abstract}\"\n",
        "      print(\"🔬 Running OpenAlex topic classification...\")\n",
        "\n",
        "      try:\n",
        "          # Get predictions from the model. The output is a simple list of dicts.\n",
        "          predictions = self.topic_classifier(formatted_text, top_k=10, truncation=True)\n",
        "          # print(f\"   [DEBUG] Raw output from OpenAlex model: {predictions}\") # You can remove this now\n",
        "\n",
        "          if not predictions:\n",
        "              print(\"   OpenAlex model returned no valid predictions.\")\n",
        "              return []\n",
        "\n",
        "          # --- CORRECTED LOOP ---\n",
        "          # We iterate directly over 'predictions', which is the list of dictionaries.\n",
        "          formal_topics = []\n",
        "          for topic in predictions:\n",
        "              # Check if the item is a dictionary with the keys we need\n",
        "              if isinstance(topic, dict) and 'label' in topic and 'score' in topic:\n",
        "                  # We can now lower the threshold since we see the scores are generally low\n",
        "                  if topic['score'] > 0.01: # Lowered threshold to include the results\n",
        "                      formal_topics.append({\n",
        "                          'topic': topic['label'],\n",
        "                          'score': round(topic['score'], 4)\n",
        "                      })\n",
        "              else:\n",
        "                  print(f\"   ⚠️ Skipping unexpected item in model predictions: {topic}\")\n",
        "\n",
        "          print(f\"✅ Extracted {len(formal_topics)} formal topics from OpenAlex.\")\n",
        "          return formal_topics\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"⚠️ An exception occurred during topic classification: {e}\")\n",
        "          return []\n",
        "\n",
        "    def fusion_logic(self, narrative_skills: List[str], formal_topics: List[Dict]) -> List[str]:\n",
        "        \"\"\"Combine narrative skills and formal topics, removing duplicates.\"\"\"\n",
        "        print(\"🔄 Applying fusion logic...\")\n",
        "        combined_skills = list(narrative_skills)\n",
        "        narrative_lower = ' '.join(narrative_skills).lower()\n",
        "\n",
        "        for topic in formal_topics:\n",
        "            topic_name = topic['topic'].split(': ', 1)[-1] # Remove ID like \"123: \"\n",
        "            is_duplicate = topic_name.lower() in narrative_lower\n",
        "            if not is_duplicate:\n",
        "                combined_skills.append(f\"Expertise in {topic_name}\")\n",
        "\n",
        "        print(f\"✅ Created final checklist with {len(combined_skills)} skills.\")\n",
        "        return combined_skills\n",
        "\n",
        "    def process_solicitation(self, pdf_filepath: str) -> Optional[StructuredSolicitationObject]:\n",
        "        \"\"\"Main processing pipeline for a PDF solicitation from a given path.\"\"\"\n",
        "        print(\"🚀 Starting PDF Solicitation Processing Pipeline\")\n",
        "        print(\"=\" * 60)\n",
        "        try:\n",
        "            filename, title, abstract = self._extract_text_from_pdf(pdf_filepath)\n",
        "\n",
        "            # Input for Claude can be a simple combination\n",
        "            claude_input_text = f\"Title: {title}. Abstract: {abstract}\"\n",
        "            print(f\"\\n📊 Text stats for analysis: {len(claude_input_text)} characters.\")\n",
        "\n",
        "            # Path A: Claude\n",
        "            narrative_skills = self.extract_narrative_skills_claude(claude_input_text)\n",
        "\n",
        "            # Path B: OpenAlex (uses corrected function call)\n",
        "            formal_topics = self.extract_formal_topics_openalex(title, abstract)\n",
        "\n",
        "            # Path C: Fusion\n",
        "            required_skills_checklist = self.fusion_logic(narrative_skills, formal_topics)\n",
        "\n",
        "            solicitation_obj = StructuredSolicitationObject(\n",
        "                solicitation_id=f\"SOL_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "                title=title, abstract=abstract,\n",
        "                processed_at=datetime.now().isoformat(),\n",
        "                pdf_filename=filename,\n",
        "                narrative_skills=narrative_skills,\n",
        "                formal_topics=formal_topics,\n",
        "                required_skills_checklist=required_skills_checklist,\n",
        "                text_length=len(claude_input_text))\n",
        "\n",
        "            output_filename = f\"{filename.replace('.pdf', '')}_analysis.json\"\n",
        "            solicitation_obj.to_json(output_filename)\n",
        "            print(f\"\\n✅ Processing complete! Saved to: {output_filename}\")\n",
        "            return solicitation_obj\n",
        "        except Exception as e:\n",
        "            print(f\"❌ A fatal error occurred during processing: {e}\")\n",
        "            return None\n",
        "\n",
        "    def display_results(self, solicitation_obj: Optional[StructuredSolicitationObject]):\n",
        "        \"\"\"Display processing results in a readable format.\"\"\"\n",
        "        if not solicitation_obj:\n",
        "            print(\"\\nNo results to display due to a processing error.\")\n",
        "            return\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"📋 SOLICITATION PROCESSING RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"🆔 ID: {solicitation_obj.solicitation_id}\")\n",
        "        print(f\"📄 File: {solicitation_obj.pdf_filename}\")\n",
        "        print(f\"📝 Title: {solicitation_obj.title}\")\n",
        "        print(f\"\\n🤖 Path A - Narrative Skills (Claude):\")\n",
        "        for i, skill in enumerate(solicitation_obj.narrative_skills, 1): print(f\"   {i}. {skill}\")\n",
        "        print(f\"\\n🔬 Path B - Formal Topics (OpenAlex):\")\n",
        "        for i, topic in enumerate(solicitation_obj.formal_topics, 1): print(f\"   {i}. {topic['topic']} (Score: {topic['score']:.3f})\")\n",
        "        print(f\"\\n✅ Final Hybrid Skills Checklist:\")\n",
        "        for i, skill in enumerate(solicitation_obj.required_skills_checklist, 1): print(f\"   {i}. {skill}\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    \"\"\"Main function to run the solicitation processing pipeline.\"\"\"\n",
        "    print(\"📄 PDF Solicitation Processor - Hybrid Deconstruction Engine\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # --- ⚠️ IMPORTANT ⚠️ ---\n",
        "    # UPDATE THIS PATH to the location of your PDF file in Google Drive.\n",
        "    # PDF_FILE_PATH = \"/content/drive/My Drive/datastore/NSF 24-569_ Mathematical Foundations of Artificial Intelligence (MFAI) _ NSF - National Science Foundation.pdf\"\n",
        "    PDF_FILE_PATH = \"/content/drive/MyDrive/datastore/NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\"\n",
        "\n",
        "    # ---\n",
        "\n",
        "    try:\n",
        "        processor = PDFSolicitationProcessor()\n",
        "        result_obj = processor.process_solicitation(PDF_FILE_PATH)\n",
        "        processor.display_results(result_obj)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred in the main function: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=False) # Force remount to ensure it's fresh\n",
        "        print(\"✅ Google Drive mounted successfully.\")\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"A critical error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3sdCJkhqvHO",
        "outputId": "0fa3b11f-7095-4529-f5b3-e1a86bf89730"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted successfully.\n",
            "📄 PDF Solicitation Processor - Hybrid Deconstruction Engine\n",
            "============================================================\n",
            "Setting up models...\n",
            "✅ Claude API client initialized\n",
            "Loading OpenAlex topic classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OpenAlex topic classifier loaded\n",
            "🚀 Starting PDF Solicitation Processing Pipeline\n",
            "============================================================\n",
            "📄 Processing: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\n",
            "✅ Extracted 57431 characters from PDF.\n",
            "\n",
            "📊 Text stats for analysis: 1641 characters.\n",
            "🤖 Calling Claude API for narrative skills...\n",
            "✅ Extracted 7 narrative skills from Claude.\n",
            "🔬 Running OpenAlex topic classification...\n",
            "✅ Extracted 7 formal topics from OpenAlex.\n",
            "🔄 Applying fusion logic...\n",
            "✅ Created final checklist with 14 skills.\n",
            "\n",
            "✅ Processing complete! Saved to: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation_analysis.json\n",
            "\n",
            "============================================================\n",
            "📋 SOLICITATION PROCESSING RESULTS\n",
            "============================================================\n",
            "🆔 ID: SOL_20250629_013351\n",
            "📄 File: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\n",
            "📝 Title: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences\n",
            "\n",
            "🤖 Path A - Narrative Skills (Claude):\n",
            "   1. Artificial Intelligence (AI) and Machine Learning\n",
            "   2. Geosciences (e.g., Earth sciences, atmospheric sciences, oceanography)\n",
            "   3. Interdisciplinary research and collaboration\n",
            "   4. Project management and team leadership\n",
            "   5. Software engineering and research software development\n",
            "   6. Data analysis and computational methods\n",
            "   7. Domain-specific knowledge related to the proposed AI applications in geosciences\n",
            "\n",
            "🔬 Path B - Formal Topics (OpenAlex):\n",
            "   1. 1986: Management and Reproducibility of Scientific Workflows (Score: 0.404)\n",
            "   2. 1937: Data Sharing and Stewardship in Science (Score: 0.068)\n",
            "   3. 3937: Challenges and Innovations in Bioinformatics Education (Score: 0.040)\n",
            "   4. 1948: Accelerating Materials Innovation through Informatics (Score: 0.039)\n",
            "   5. 4427: Hydrologic Data Management and Analysis (Score: 0.018)\n",
            "   6. 3650: Scientific Computing and Data Analysis with Python (Score: 0.015)\n",
            "   7. 1636: Artificial Intelligence in Medicine (Score: 0.011)\n",
            "\n",
            "✅ Final Hybrid Skills Checklist:\n",
            "   1. Artificial Intelligence (AI) and Machine Learning\n",
            "   2. Geosciences (e.g., Earth sciences, atmospheric sciences, oceanography)\n",
            "   3. Interdisciplinary research and collaboration\n",
            "   4. Project management and team leadership\n",
            "   5. Software engineering and research software development\n",
            "   6. Data analysis and computational methods\n",
            "   7. Domain-specific knowledge related to the proposed AI applications in geosciences\n",
            "   8. Expertise in Management and Reproducibility of Scientific Workflows\n",
            "   9. Expertise in Data Sharing and Stewardship in Science\n",
            "   10. Expertise in Challenges and Innovations in Bioinformatics Education\n",
            "   11. Expertise in Accelerating Materials Innovation through Informatics\n",
            "   12. Expertise in Hydrologic Data Management and Analysis\n",
            "   13. Expertise in Scientific Computing and Data Analysis with Python\n",
            "   14. Expertise in Artificial Intelligence in Medicine\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF_FILE_PATH = \"/content/drive/My Drive/datastore/NSF 24-569_ Mathematical Foundations of Artificial Intelligence (MFAI) _ NSF - National Science Foundation.pdf\"\n",
        "PDF_FILE_PATH = \"/content/drive/MyDrive/datastore/NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\"\n",
        "processor = PDFSolicitationProcessor()\n",
        "result_obj = processor.process_solicitation(PDF_FILE_PATH)\n",
        "processor.display_results(result_obj)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "# Convert the result_obj to a dictionary\n",
        "result_dict = result_obj.to_dict()\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "# Since result_obj has nested lists/dicts, pandas might struggle to flatten it directly.\n",
        "# We can create a DataFrame with one row, where each column is the value of the corresponding key.\n",
        "# For list/dict values, they will be stored as objects in the cell.\n",
        "df_result = pd.DataFrame([result_dict])\n",
        "\n",
        "# Alternatively, if you want to normalize or flatten specific nested structures\n",
        "# you would need to process the dictionary before creating the DataFrame.\n",
        "# For example, to flatten formal_topics into separate columns:\n",
        "# df_formal_topics = pd.DataFrame(result_obj.formal_topics)\n",
        "# Then merge or combine this with the main DataFrame if needed.\n",
        "# For this task, a single-row DataFrame storing complex objects is sufficient.\n",
        "\n",
        "print(\"\\nDataFrame created from result_obj:\")\n",
        "print(df_result.head())\n",
        "print(f\"\\nDataFrame shape: {df_result.shape}\")\n",
        "print(f\"DataFrame columns: {list(df_result.columns)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5waonpvwQPt",
        "outputId": "9a43646a-ac6d-49d7-e1d3-d8e9a445d4d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up models...\n",
            "✅ Claude API client initialized\n",
            "Loading OpenAlex topic classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OpenAlex topic classifier loaded\n",
            "🚀 Starting PDF Solicitation Processing Pipeline\n",
            "============================================================\n",
            "📄 Processing: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\n",
            "✅ Extracted 57431 characters from PDF.\n",
            "\n",
            "📊 Text stats for analysis: 1641 characters.\n",
            "🤖 Calling Claude API for narrative skills...\n",
            "✅ Extracted 7 narrative skills from Claude.\n",
            "🔬 Running OpenAlex topic classification...\n",
            "✅ Extracted 7 formal topics from OpenAlex.\n",
            "🔄 Applying fusion logic...\n",
            "✅ Created final checklist with 14 skills.\n",
            "\n",
            "✅ Processing complete! Saved to: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation_analysis.json\n",
            "\n",
            "============================================================\n",
            "📋 SOLICITATION PROCESSING RESULTS\n",
            "============================================================\n",
            "🆔 ID: SOL_20250629_013416\n",
            "📄 File: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\n",
            "📝 Title: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences\n",
            "\n",
            "🤖 Path A - Narrative Skills (Claude):\n",
            "   1. Artificial Intelligence (AI) and Machine Learning\n",
            "   2. Geosciences (e.g., Earth sciences, atmospheric sciences, oceanography)\n",
            "   3. Interdisciplinary research and collaboration\n",
            "   4. Project management and team leadership\n",
            "   5. Software engineering and research software development\n",
            "   6. Data analysis and computational methods\n",
            "   7. Domain-specific knowledge related to the proposed AI applications in geosciences\n",
            "\n",
            "🔬 Path B - Formal Topics (OpenAlex):\n",
            "   1. 1986: Management and Reproducibility of Scientific Workflows (Score: 0.404)\n",
            "   2. 1937: Data Sharing and Stewardship in Science (Score: 0.068)\n",
            "   3. 3937: Challenges and Innovations in Bioinformatics Education (Score: 0.040)\n",
            "   4. 1948: Accelerating Materials Innovation through Informatics (Score: 0.039)\n",
            "   5. 4427: Hydrologic Data Management and Analysis (Score: 0.018)\n",
            "   6. 3650: Scientific Computing and Data Analysis with Python (Score: 0.015)\n",
            "   7. 1636: Artificial Intelligence in Medicine (Score: 0.011)\n",
            "\n",
            "✅ Final Hybrid Skills Checklist:\n",
            "   1. Artificial Intelligence (AI) and Machine Learning\n",
            "   2. Geosciences (e.g., Earth sciences, atmospheric sciences, oceanography)\n",
            "   3. Interdisciplinary research and collaboration\n",
            "   4. Project management and team leadership\n",
            "   5. Software engineering and research software development\n",
            "   6. Data analysis and computational methods\n",
            "   7. Domain-specific knowledge related to the proposed AI applications in geosciences\n",
            "   8. Expertise in Management and Reproducibility of Scientific Workflows\n",
            "   9. Expertise in Data Sharing and Stewardship in Science\n",
            "   10. Expertise in Challenges and Innovations in Bioinformatics Education\n",
            "   11. Expertise in Accelerating Materials Innovation through Informatics\n",
            "   12. Expertise in Hydrologic Data Management and Analysis\n",
            "   13. Expertise in Scientific Computing and Data Analysis with Python\n",
            "   14. Expertise in Artificial Intelligence in Medicine\n",
            "\n",
            "============================================================\n",
            "\n",
            "DataFrame created from result_obj:\n",
            "       solicitation_id                                              title  \\\n",
            "0  SOL_20250629_013416  NSF 25-530: Collaborations in Artificial Intel...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  of Program Requirements Feedback Important Inf...   \n",
            "\n",
            "                 processed_at  \\\n",
            "0  2025-06-29T01:34:16.950364   \n",
            "\n",
            "                                        pdf_filename  \\\n",
            "0  NSF 25-530: Collaborations in Artificial Intel...   \n",
            "\n",
            "                                    narrative_skills  \\\n",
            "0  [Artificial Intelligence (AI) and Machine Lear...   \n",
            "\n",
            "                                       formal_topics  \\\n",
            "0  [{'topic': '1986: Management and Reproducibili...   \n",
            "\n",
            "                           required_skills_checklist  text_length  \\\n",
            "0  [Artificial Intelligence (AI) and Machine Lear...         1641   \n",
            "\n",
            "       processing_method  \n",
            "0  hybrid_deconstruction  \n",
            "\n",
            "DataFrame shape: (1, 10)\n",
            "DataFrame columns: ['solicitation_id', 'title', 'abstract', 'processed_at', 'pdf_filename', 'narrative_skills', 'formal_topics', 'required_skills_checklist', 'text_length', 'processing_method']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "RBw0TNWuxTq6",
        "outputId": "da719260-1789-4add-8880-7b5a583d6458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       solicitation_id                                              title  \\\n",
              "0  SOL_20250627_002354  NSF 24-569: Mathematical Foundations of Artifi...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  of Program Requirements Feedback 6/26/25, 7:00...   \n",
              "\n",
              "                 processed_at  \\\n",
              "0  2025-06-27T00:23:54.745337   \n",
              "\n",
              "                                        pdf_filename  \\\n",
              "0  NSF 24-569_ Mathematical Foundations of Artifi...   \n",
              "\n",
              "                                    narrative_skills  \\\n",
              "0  [Mathematical Foundations of Artificial Intell...   \n",
              "\n",
              "                                       formal_topics  \\\n",
              "0  [{'topic': '2002: Computational Complexity and...   \n",
              "\n",
              "                           required_skills_checklist  text_length  \\\n",
              "0  [Mathematical Foundations of Artificial Intell...         1619   \n",
              "\n",
              "       processing_method  \n",
              "0  hybrid_deconstruction  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b82d77a-6c1d-4aa3-9ab7-8ad7997f3445\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>solicitation_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>processed_at</th>\n",
              "      <th>pdf_filename</th>\n",
              "      <th>narrative_skills</th>\n",
              "      <th>formal_topics</th>\n",
              "      <th>required_skills_checklist</th>\n",
              "      <th>text_length</th>\n",
              "      <th>processing_method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SOL_20250627_002354</td>\n",
              "      <td>NSF 24-569: Mathematical Foundations of Artifi...</td>\n",
              "      <td>of Program Requirements Feedback 6/26/25, 7:00...</td>\n",
              "      <td>2025-06-27T00:23:54.745337</td>\n",
              "      <td>NSF 24-569_ Mathematical Foundations of Artifi...</td>\n",
              "      <td>[Mathematical Foundations of Artificial Intell...</td>\n",
              "      <td>[{'topic': '2002: Computational Complexity and...</td>\n",
              "      <td>[Mathematical Foundations of Artificial Intell...</td>\n",
              "      <td>1619</td>\n",
              "      <td>hybrid_deconstruction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b82d77a-6c1d-4aa3-9ab7-8ad7997f3445')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b82d77a-6c1d-4aa3-9ab7-8ad7997f3445 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b82d77a-6c1d-4aa3-9ab7-8ad7997f3445');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_9625bb4b-eee3-45c4-b045-a11e68001227\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_result')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9625bb4b-eee3-45c4-b045-a11e68001227 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_result');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_result",
              "summary": "{\n  \"name\": \"df_result\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"solicitation_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"SOL_20250627_002354\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NSF 24-569: Mathematical Foundations of Artificial\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"of Program Requirements Feedback 6/26/25, 7:00 PM NSF 24-569: Mathematical Foundations of Artificial Intelligence (MFAI) | NSF - National Science Foundation https://www.nsf.gov/funding/opportunities/mfai-mathematical-foundations-artificial-intelligence/nsf24-569/solicitation 1/17 Important Information And Revision Notes Any proposal submitted in response to this solicitation should be submitted in accordance with the NSF Proposal & Award Policies & Procedures Guide (PAPPG) that is in effect for the relevant due date to which the proposal is being submitted. The NSF PAPPG is regularly revised and it is the responsibility of the proposer to ensure that the proposal meets the requirements specified in this solicitation and the applicable version of the PAPPG. Submitting a proposal prior to a specified deadline does not negate this requirement. Summary Of Program Requirements General Information Program Title: Synopsis of Program: V. Proposal Preparation and Submission Instructions A. Proposal Preparation Instructions B. Budgetary Information C. Due Dates D. Research.gov/Grants.gov Requirements VI. NSF Proposal Processing and Review Procedures A. Merit Review Principles and Criteria B. Review and Selection Process VII. Award Administration Information A. Notification of the Award B. Award Conditions C. Reporting Requirements VIII. Agency Contacts IX. Other Information Mathematical Foundations of Artificial Intelligence (MFAI) Machine Learning and Artificial Intelligence (AI) are enabling extraordinary scientific breakthroughs in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2025-06-27T00:23:54.745337\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NSF 24-569_ Mathematical Foundations of Artificial Intelligence (MFAI) _ NSF - National Science Foundation.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"narrative_skills\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"formal_topics\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"required_skills_checklist\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1619,\n        \"max\": 1619,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1619\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processing_method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hybrid_deconstruction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skills"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-PLw621WS4M",
        "outputId": "f389a12d-d8a5-4159-b8df-2b050bdcbc92"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mathematical Foundations of Artificial Intelligence',\n",
              " 'Machine Learning Theory and Algorithms',\n",
              " 'Optimization and Computational Mathematics',\n",
              " 'Statistical Learning and Inference',\n",
              " 'Knowledge Representation and Reasoning',\n",
              " 'Computational Complexity and Algorithm Analysis',\n",
              " 'Applications of AI in Scientific Domains',\n",
              " 'Expertise in Computational Complexity and Algorithmic Information Theory',\n",
              " 'Expertise in Artificial Intelligence in Medicine',\n",
              " 'Expertise in Challenges and Innovations in Undergraduate Neuroscience Education',\n",
              " 'Expertise in Scientific Computing and Data Analysis with Python',\n",
              " 'Expertise in Innovations in Education Technology and Learning Systems',\n",
              " 'Expertise in Theoretical Framework of Cognitive Informatics and Computational Intelligence',\n",
              " 'Expertise in Management and Reproducibility of Scientific Workflows',\n",
              " 'Expertise in Artificial Intelligence and Technology Innovation',\n",
              " 'Expertise in Secure Classical Communication Using Noise and Laws',\n",
              " 'Expertise in Challenges and Innovations in Bioinformatics Education']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Ensure the previous cell ran successfully and result_obj exists\n",
        "if 'result_obj' in locals() and result_obj is not None:\n",
        "    # Convert the result_obj to a dictionary\n",
        "    result_dict = result_obj.to_dict()\n",
        "\n",
        "    # Create a DataFrame from the dictionary for inspection\n",
        "    df_result = pd.DataFrame([result_dict])\n",
        "\n",
        "    print(\"\\nDataFrame created from result_obj:\")\n",
        "    print(df_result.head())\n",
        "    print(f\"\\nDataFrame shape: {df_result.shape}\")\n",
        "    print(f\"DataFrame columns: {list(df_result.columns)}\")\n",
        "\n",
        "    # --- CRITICAL STEP ---\n",
        "    # Extract the skills checklist for the next phase of analysis\n",
        "    skills_for_analysis = result_obj.required_skills_checklist\n",
        "    solicitation_id_for_analysis = result_obj.solicitation_id\n",
        "\n",
        "    print(f\"\\n✅ Extracted {len(skills_for_analysis)} skills for affinity analysis.\")\n",
        "    print(\"Skills are now ready for the Skill Affinity Engine.\")\n",
        "else:\n",
        "    print(\"⚠️ 'result_obj' not found or is None. Please run the first cell successfully.\")\n",
        "    # Provide a default list to prevent the next cell from failing, or handle as needed\n",
        "    skills_for_analysis = []\n",
        "    solicitation_id_for_analysis = \"SOL_ERROR\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eop1zaiqXooh",
        "outputId": "f1d6a664-96fd-4810-b046-90fcf1e86da5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame created from result_obj:\n",
            "       solicitation_id                                              title  \\\n",
            "0  SOL_20250629_013416  NSF 25-530: Collaborations in Artificial Intel...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  of Program Requirements Feedback Important Inf...   \n",
            "\n",
            "                 processed_at  \\\n",
            "0  2025-06-29T01:34:16.950364   \n",
            "\n",
            "                                        pdf_filename  \\\n",
            "0  NSF 25-530: Collaborations in Artificial Intel...   \n",
            "\n",
            "                                    narrative_skills  \\\n",
            "0  [Artificial Intelligence (AI) and Machine Lear...   \n",
            "\n",
            "                                       formal_topics  \\\n",
            "0  [{'topic': '1986: Management and Reproducibili...   \n",
            "\n",
            "                           required_skills_checklist  text_length  \\\n",
            "0  [Artificial Intelligence (AI) and Machine Lear...         1641   \n",
            "\n",
            "       processing_method  \n",
            "0  hybrid_deconstruction  \n",
            "\n",
            "DataFrame shape: (1, 10)\n",
            "DataFrame columns: ['solicitation_id', 'title', 'abstract', 'processed_at', 'pdf_filename', 'narrative_skills', 'formal_topics', 'required_skills_checklist', 'text_length', 'processing_method']\n",
            "\n",
            "✅ Extracted 14 skills for affinity analysis.\n",
            "Skills are now ready for the Skill Affinity Engine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SkillAffinityEngine:\n",
        "    \"\"\"\n",
        "    Phase 1: Core Analysis - Calculates affinity between researchers and required skills.\n",
        "    \"\"\"\n",
        "    def __init__(self, datastore_path=\"/content/drive/My Drive/datastore/\"):\n",
        "        self.datastore_path = datastore_path\n",
        "        self.model = None\n",
        "        self.metadata_df = None\n",
        "        self.embeddings_array = None\n",
        "        self.embedding_index = None\n",
        "        self.researcher_index = None\n",
        "        self.datastore_info = None\n",
        "\n",
        "    def load_datastore(self):\n",
        "        \"\"\"Load the complete researcher profile datastore.\"\"\"\n",
        "        print(\"📂 Loading Researcher Profile Datastore...\")\n",
        "        try:\n",
        "            self.metadata_df = pd.read_parquet(f\"{self.datastore_path}researcher_profiles_metadata.parquet\")\n",
        "            self.embeddings_array = np.load(f\"{self.datastore_path}researcher_embeddings.npy\")\n",
        "            with open(f\"{self.datastore_path}embedding_index.json\", 'r') as f: self.embedding_index = json.load(f)\n",
        "            with open(f\"{self.datastore_path}researcher_index.json\", 'r') as f: self.researcher_index = json.load(f)\n",
        "            with open(f\"{self.datastore_path}datastore_info.json\", 'r') as f: self.datastore_info = json.load(f)\n",
        "            print(f\"🎯 Datastore ready: {self.metadata_df['researcher_id'].nunique()} researchers, {len(self.metadata_df)} papers\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to load datastore: {e}\")\n",
        "\n",
        "    def load_model(self, model_name='all-MiniLM-L6-v2'):\n",
        "        \"\"\"Load the sentence transformer model.\"\"\"\n",
        "        print(f\"🤖 Loading sentence transformer model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        print(f\"✅ Model loaded.\")\n",
        "\n",
        "    def embed_skills(self, skills_checklist):\n",
        "        \"\"\"Embed each skill phrase.\"\"\"\n",
        "        if self.model is None: self.load_model()\n",
        "        print(f\"🧠 Embedding {len(skills_checklist)} skills...\")\n",
        "        skill_embeddings = self.model.encode(skills_checklist, show_progress_bar=True)\n",
        "        print(f\"✅ Created skill embeddings: {skill_embeddings.shape}\")\n",
        "        return skill_embeddings\n",
        "\n",
        "    def get_researcher_data(self, researcher_id):\n",
        "        \"\"\"Get data for a specific researcher.\"\"\"\n",
        "        work_ids = self.researcher_index.get(researcher_id, [])\n",
        "        positions = [self.embedding_index[wid] for wid in work_ids if wid in self.embedding_index]\n",
        "        if not positions: return np.array([]), [], []\n",
        "        paper_embeddings = self.embeddings_array[positions]\n",
        "        paper_metadata = self.metadata_df[self.metadata_df['work_id'].isin(work_ids)]\n",
        "        weight_mapping = dict(zip(paper_metadata['work_id'], paper_metadata['recency_weight']))\n",
        "        recency_weights = np.array([weight_mapping.get(wid, 0.0) for wid in work_ids if wid in self.embedding_index])\n",
        "        return paper_embeddings, work_ids, recency_weights\n",
        "\n",
        "    def calculate_skill_affinity_score(self, paper_embeddings, skill_embedding, recency_weights):\n",
        "        \"\"\"Calculate affinity score for one researcher against one skill.\"\"\"\n",
        "        if len(paper_embeddings) == 0: return 0.0\n",
        "        cosine_sims = cosine_similarity(paper_embeddings, skill_embedding.reshape(1, -1)).flatten()\n",
        "        max_weighted_sim = np.max(cosine_sims * recency_weights)\n",
        "        return np.clip(max_weighted_sim * 100, 0, 100)\n",
        "\n",
        "    def create_affinity_matrix(self, skills_checklist, solicitation_id=None):\n",
        "        \"\"\"Create the complete affinity matrix.\"\"\"\n",
        "        print(\"🎯 Creating Affinity Matrix...\")\n",
        "        print(\"=\" * 50)\n",
        "        if self.metadata_df is None: self.load_datastore()\n",
        "\n",
        "        # --- FIX: Check for empty skills_checklist ---\n",
        "        if not skills_checklist:\n",
        "            raise ValueError(\"Skills checklist is empty. Cannot create affinity matrix.\")\n",
        "\n",
        "        skill_embeddings = self.embed_skills(skills_checklist)\n",
        "        unique_researchers = list(self.researcher_index.keys())\n",
        "        print(f\"📊 Processing {len(unique_researchers)} researchers × {len(skills_checklist)} skills\")\n",
        "        affinity_matrix = np.zeros((len(unique_researchers), len(skills_checklist)))\n",
        "\n",
        "        for i, researcher_id in enumerate(tqdm(unique_researchers, desc=\"Processing researchers\")):\n",
        "            paper_embeddings, _, recency_weights = self.get_researcher_data(researcher_id)\n",
        "            if len(paper_embeddings) == 0: continue\n",
        "            for j, skill_embedding in enumerate(skill_embeddings):\n",
        "                affinity_matrix[i, j] = self.calculate_skill_affinity_score(paper_embeddings, skill_embedding, recency_weights)\n",
        "\n",
        "        researcher_names = [self.metadata_df[self.metadata_df['researcher_id'] == rid].iloc[0]['researcher_name'] for rid in unique_researchers]\n",
        "        skill_columns = [f\"Skill_{i+1:02d}: {skill[:50]}\" for i, skill in enumerate(skills_checklist)]\n",
        "        affinity_df = pd.DataFrame(affinity_matrix, index=researcher_names, columns=skill_columns)\n",
        "        print(f\"✅ Affinity Matrix created: {affinity_df.shape}\")\n",
        "        return affinity_df, unique_researchers, skills_checklist\n",
        "\n",
        "    def analyze_affinity_matrix(self, affinity_df, skills_checklist):\n",
        "        \"\"\"Provide analysis on the affinity matrix.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n📊 AFFINITY MATRIX ANALYSIS\\n\" + \"=\"*60)\n",
        "        print(f\"📏 Matrix dimensions: {affinity_df.shape[0]} researchers × {affinity_df.shape[1]} skills\")\n",
        "        print(f\"📈 Score range: {affinity_df.values.min():.2f} - {affinity_df.values.max():.2f}\")\n",
        "        print(f\"📊 Mean affinity score: {affinity_df.values.mean():.2f}\")\n",
        "        researcher_avg_scores = affinity_df.mean(axis=1).sort_values(ascending=False)\n",
        "        print(f\"\\n🏆 Top 5 Researchers (by average affinity):\\n{researcher_avg_scores.head().to_string(float_format='%.2f')}\")\n",
        "        skill_avg_scores = affinity_df.mean(axis=0).sort_values()\n",
        "        print(f\"\\n🎯 Most Challenging Skills (lowest average affinity):\")\n",
        "        for skill_col, score in skill_avg_scores.head().items():\n",
        "            original_skill_index = int(skill_col.split('_')[1].split(':')[0]) - 1\n",
        "            print(f\"   - {skills_checklist[original_skill_index][:60]}...: {score:.2f}\")\n",
        "\n",
        "\n",
        "def main_affinity_analysis(skills_checklist, solicitation_id):\n",
        "    \"\"\"Main function to run the affinity analysis.\"\"\"\n",
        "    print(\"\\n🎯 SKILL AFFINITY ENGINE - PHASE 1 CORE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    engine = SkillAffinityEngine()\n",
        "\n",
        "    # Run the full pipeline\n",
        "    affinity_df, unique_researchers, skills_list = engine.create_affinity_matrix(\n",
        "        skills_checklist, solicitation_id\n",
        "    )\n",
        "    engine.analyze_affinity_matrix(affinity_df, skills_list)\n",
        "    return affinity_df\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MAIN EXECUTION FOR AFFINITY ANALYSIS\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the previous cells have run and skills_for_analysis is available\n",
        "    if 'skills_for_analysis' in locals() and skills_for_analysis:\n",
        "        print(\"Running Skill Affinity Engine with extracted skills...\")\n",
        "        try:\n",
        "            # Pass the extracted skills and ID to the analysis function\n",
        "            affinity_matrix = main_affinity_analysis(skills_for_analysis, solicitation_id_for_analysis)\n",
        "            print(f\"\\n📊 Sample of Affinity Matrix:\")\n",
        "            print(affinity_matrix.iloc[:5, :5])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ An error occurred during affinity analysis: {e}\")\n",
        "    else:\n",
        "        print(\"⚠️ Cannot run affinity analysis because the skills checklist is empty or not defined.\")\n",
        "        print(\"Please run the first two cells successfully to extract skills from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f608812de00149439800cd96bae8413b",
            "e250c7aa89e841a1917a48d3fc91f071",
            "d9dbbfb521cb4933b7a0725353ee87e8",
            "30fd9f446cd14d2b8fbd6723b86ab9e6",
            "c2fa4e85674d4d2e940016c5e2222ac6",
            "6264b4591aac436894e22e065b942078",
            "17e771b7406c420c946a34d46662d4fd",
            "0cbe2085e634483cbc33e084120f74f5",
            "d30ee13178a340ffa9219f4e804b73a8",
            "61be64e437984538b298e69c59a572f9",
            "0df02a4e1714404e8dc59fc637fed4a7"
          ]
        },
        "id": "qbMJcu05xUPA",
        "outputId": "56d5fc3e-af53-41ed-df0f-a9dbb858562a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Skill Affinity Engine with extracted skills...\n",
            "\n",
            "🎯 SKILL AFFINITY ENGINE - PHASE 1 CORE ANALYSIS\n",
            "============================================================\n",
            "🎯 Creating Affinity Matrix...\n",
            "==================================================\n",
            "📂 Loading Researcher Profile Datastore...\n",
            "🎯 Datastore ready: 51 researchers, 9067 papers\n",
            "🤖 Loading sentence transformer model: all-MiniLM-L6-v2\n",
            "✅ Model loaded.\n",
            "🧠 Embedding 14 skills...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f608812de00149439800cd96bae8413b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created skill embeddings: (14, 384)\n",
            "📊 Processing 51 researchers × 14 skills\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing researchers: 100%|██████████| 51/51 [00:01<00:00, 32.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Affinity Matrix created: (51, 14)\n",
            "\n",
            "============================================================\n",
            "📊 AFFINITY MATRIX ANALYSIS\n",
            "============================================================\n",
            "📏 Matrix dimensions: 51 researchers × 14 skills\n",
            "📈 Score range: 0.00 - 50.03\n",
            "📊 Mean affinity score: 12.47\n",
            "\n",
            "🏆 Top 5 Researchers (by average affinity):\n",
            "Subasish Das           34.05\n",
            "Brady T. West          26.18\n",
            "Robert McLean          24.95\n",
            "John P. Tiefenbacher   24.06\n",
            "Raymond P. Fisk        22.17\n",
            "\n",
            "🎯 Most Challenging Skills (lowest average affinity):\n",
            "   - Project management and team leadership...: 10.33\n",
            "   - Expertise in Artificial Intelligence in Medicine...: 10.68\n",
            "   - Expertise in Accelerating Materials Innovation through Infor...: 11.24\n",
            "   - Software engineering and research software development...: 11.41\n",
            "   - Geosciences (e.g., Earth sciences, atmospheric sciences, oce...: 11.73\n",
            "\n",
            "📊 Sample of Affinity Matrix:\n",
            "                    Skill_01: Artificial Intelligence (AI) and Machine Learning  \\\n",
            "Manfred Schartl                                              9.627572             \n",
            "Larry R. Price                                              10.747916             \n",
            "Michael A. Huston                                            2.539897             \n",
            "Marcus Felson                                                9.310672             \n",
            "Togay Ozbakkaloglu                                          23.967419             \n",
            "\n",
            "                    Skill_02: Geosciences (e.g., Earth sciences, atmospheric sci  \\\n",
            "Manfred Schartl                                             13.628834              \n",
            "Larry R. Price                                              12.510272              \n",
            "Michael A. Huston                                            4.938897              \n",
            "Marcus Felson                                                5.268790              \n",
            "Togay Ozbakkaloglu                                          14.937198              \n",
            "\n",
            "                    Skill_03: Interdisciplinary research and collaboration  \\\n",
            "Manfred Schartl                                             16.777699        \n",
            "Larry R. Price                                              21.375076        \n",
            "Michael A. Huston                                            8.040319        \n",
            "Marcus Felson                                               10.785990        \n",
            "Togay Ozbakkaloglu                                          28.334100        \n",
            "\n",
            "                    Skill_04: Project management and team leadership  \\\n",
            "Manfred Schartl                                             6.868222   \n",
            "Larry R. Price                                             11.255434   \n",
            "Michael A. Huston                                           8.249076   \n",
            "Marcus Felson                                               6.455185   \n",
            "Togay Ozbakkaloglu                                         24.104122   \n",
            "\n",
            "                    Skill_05: Software engineering and research software develop  \n",
            "Manfred Schartl                                              9.253222             \n",
            "Larry R. Price                                              15.163214             \n",
            "Michael A. Huston                                           13.301635             \n",
            "Marcus Felson                                                9.419647             \n",
            "Togay Ozbakkaloglu                                          18.554434             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "import anthropic\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DreamTeamAssembler:\n",
        "    \"\"\"\n",
        "    Phase 2: Dream Team Assembly & Strategic Output\n",
        "\n",
        "    Implements the greedy algorithm to select optimal research teams and generates\n",
        "    comprehensive strategic reports using AI analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.claude_client = None\n",
        "        self.setup_claude_api()\n",
        "\n",
        "    def setup_claude_api(self):\n",
        "        \"\"\"Initialize Claude API client for gap analysis.\"\"\"\n",
        "        try:\n",
        "            api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "            self.claude_client = anthropic.Anthropic(api_key=api_key)\n",
        "            print(\"✅ Claude API client initialized for gap analysis\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Claude API setup failed: {e}\")\n",
        "            print(\"Gap analysis will be limited without API access\")\n",
        "\n",
        "    def load_affinity_matrix(self, csv_path, metadata_path=None):\n",
        "        \"\"\"\n",
        "        Load the affinity matrix and associated metadata.\n",
        "\n",
        "        Args:\n",
        "            csv_path (str): Path to affinity matrix CSV\n",
        "            metadata_path (str): Optional path to metadata JSON\n",
        "\n",
        "        Returns:\n",
        "            tuple: (affinity_df, metadata)\n",
        "        \"\"\"\n",
        "        print(f\"📊 Loading affinity matrix from: {csv_path}\")\n",
        "\n",
        "        # Load the matrix\n",
        "        affinity_df = pd.read_csv(csv_path, index_col=0)\n",
        "        print(f\"✅ Loaded matrix: {affinity_df.shape[0]} researchers × {affinity_df.shape[1]} skills\")\n",
        "\n",
        "        # Load metadata if available\n",
        "        metadata = None\n",
        "        if metadata_path:\n",
        "            try:\n",
        "                with open(metadata_path, 'r') as f:\n",
        "                    metadata = json.load(f)\n",
        "                print(f\"✅ Loaded metadata\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not load metadata: {e}\")\n",
        "\n",
        "        return affinity_df, metadata\n",
        "\n",
        "    def calculate_team_coverage(self, affinity_df, team_indices):\n",
        "        \"\"\"\n",
        "        Calculate team coverage scores for all skills.\n",
        "\n",
        "        Args:\n",
        "            affinity_df (pd.DataFrame): Affinity matrix\n",
        "            team_indices (list): Indices of selected team members\n",
        "\n",
        "        Returns:\n",
        "            tuple: (skill_coverages, overall_coverage_score)\n",
        "        \"\"\"\n",
        "        if not team_indices:\n",
        "            return [], 0.0\n",
        "\n",
        "        # Get affinity scores for team members\n",
        "        team_affinities = affinity_df.iloc[team_indices]\n",
        "\n",
        "        # For each skill, take the maximum affinity among team members\n",
        "        skill_coverages = team_affinities.max(axis=0).values\n",
        "\n",
        "        # Overall team coverage score is the average\n",
        "        overall_coverage_score = np.mean(skill_coverages)\n",
        "\n",
        "        return skill_coverages, overall_coverage_score\n",
        "\n",
        "    def calculate_marginal_gain(self, affinity_df, current_team_indices, candidate_index):\n",
        "        \"\"\"\n",
        "        Calculate marginal gain of adding a candidate to the current team.\n",
        "\n",
        "        Args:\n",
        "            affinity_df (pd.DataFrame): Affinity matrix\n",
        "            current_team_indices (list): Current team member indices\n",
        "            candidate_index (int): Index of candidate researcher\n",
        "\n",
        "        Returns:\n",
        "            float: Marginal gain in coverage score\n",
        "        \"\"\"\n",
        "        # Current team coverage\n",
        "        _, current_coverage = self.calculate_team_coverage(affinity_df, current_team_indices)\n",
        "\n",
        "        # New team coverage with candidate added\n",
        "        new_team_indices = current_team_indices + [candidate_index]\n",
        "        _, new_coverage = self.calculate_team_coverage(affinity_df, new_team_indices)\n",
        "\n",
        "        # Marginal gain\n",
        "        marginal_gain = new_coverage - current_coverage\n",
        "\n",
        "        return marginal_gain\n",
        "\n",
        "    def dream_team_greedy_algorithm(self, affinity_df, min_team_size=2, max_team_size=4):\n",
        "        \"\"\"\n",
        "        Implement the Dream Team Greedy Algorithm.\n",
        "\n",
        "        Args:\n",
        "            affinity_df (pd.DataFrame): Affinity matrix\n",
        "            min_team_size (int): Minimum team size\n",
        "            max_team_size (int): Maximum team size\n",
        "\n",
        "        Returns:\n",
        "            tuple: (selected_team_indices, selection_history)\n",
        "        \"\"\"\n",
        "        print(\"🎯 Running Dream Team Greedy Algorithm...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        n_researchers = len(affinity_df)\n",
        "        selected_indices = []\n",
        "        selection_history = []\n",
        "\n",
        "        # Step 1: Select the best overall researcher (likely PI)\n",
        "        researcher_avg_scores = affinity_df.mean(axis=1)\n",
        "        best_researcher_idx = researcher_avg_scores.idxmax()\n",
        "        best_researcher_pos = affinity_df.index.get_loc(best_researcher_idx)\n",
        "\n",
        "        selected_indices.append(best_researcher_pos)\n",
        "        _, initial_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "\n",
        "        selection_history.append({\n",
        "            'step': 1,\n",
        "            'action': 'initial_selection',\n",
        "            'researcher_idx': best_researcher_pos,\n",
        "            'researcher_name': affinity_df.index[best_researcher_pos],\n",
        "            'reason': 'Highest average affinity score (likely PI)',\n",
        "            'team_coverage': initial_coverage,\n",
        "            'marginal_gain': initial_coverage\n",
        "        })\n",
        "\n",
        "        print(f\"🏆 Step 1 - PI Selection: {affinity_df.index[best_researcher_pos]}\")\n",
        "        print(f\"    Initial coverage: {initial_coverage:.2f}\")\n",
        "\n",
        "        # Steps 2-4: Iteratively add researchers with maximum marginal gain\n",
        "        for step in range(2, max_team_size + 1):\n",
        "            best_candidate_idx = None\n",
        "            best_marginal_gain = -1\n",
        "            candidate_gains = []\n",
        "\n",
        "            # Evaluate all remaining researchers\n",
        "            for candidate_idx in range(n_researchers):\n",
        "                if candidate_idx in selected_indices:\n",
        "                    continue  # Skip already selected researchers\n",
        "\n",
        "                marginal_gain = self.calculate_marginal_gain(\n",
        "                    affinity_df, selected_indices, candidate_idx\n",
        "                )\n",
        "                candidate_gains.append((candidate_idx, marginal_gain))\n",
        "\n",
        "                if marginal_gain > best_marginal_gain:\n",
        "                    best_marginal_gain = marginal_gain\n",
        "                    best_candidate_idx = candidate_idx\n",
        "\n",
        "            # Add the best candidate if marginal gain is positive and we haven't reached min size\n",
        "            # Or if marginal gain is significant enough and we're expanding beyond min size\n",
        "            should_add = False\n",
        "            if len(selected_indices) < min_team_size:\n",
        "                should_add = True  # Must reach minimum team size\n",
        "            elif best_marginal_gain > 0.5:  # Only add if significant improvement\n",
        "                should_add = True\n",
        "\n",
        "            if should_add and best_candidate_idx is not None:\n",
        "                selected_indices.append(best_candidate_idx)\n",
        "                _, new_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "\n",
        "                selection_history.append({\n",
        "                    'step': step,\n",
        "                    'action': 'add_member',\n",
        "                    'researcher_idx': best_candidate_idx,\n",
        "                    'researcher_name': affinity_df.index[best_candidate_idx],\n",
        "                    'reason': f'Maximum marginal gain (+{best_marginal_gain:.2f})',\n",
        "                    'team_coverage': new_coverage,\n",
        "                    'marginal_gain': best_marginal_gain\n",
        "                })\n",
        "\n",
        "                print(f\"✅ Step {step} - Added: {affinity_df.index[best_candidate_idx]}\")\n",
        "                print(f\"    Marginal gain: +{best_marginal_gain:.2f}, New coverage: {new_coverage:.2f}\")\n",
        "            else:\n",
        "                print(f\"🛑 Step {step} - Stopping: No significant marginal gain (best: +{best_marginal_gain:.2f})\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\n🎯 Final Dream Team: {len(selected_indices)} researchers\")\n",
        "        print(f\"📊 Final team coverage: {self.calculate_team_coverage(affinity_df, selected_indices)[1]:.2f}\")\n",
        "\n",
        "        return selected_indices, selection_history\n",
        "\n",
        "    def generate_coverage_report(self, affinity_df, team_indices, skills_list):\n",
        "        \"\"\"\n",
        "        Generate detailed coverage report for the selected team.\n",
        "\n",
        "        Args:\n",
        "            affinity_df (pd.DataFrame): Affinity matrix\n",
        "            team_indices (list): Selected team member indices\n",
        "            skills_list (list): List of skill descriptions\n",
        "\n",
        "        Returns:\n",
        "            dict: Comprehensive coverage report\n",
        "        \"\"\"\n",
        "        print(\"📋 Generating Coverage Report...\")\n",
        "\n",
        "        # Calculate coverage\n",
        "        skill_coverages, overall_coverage = self.calculate_team_coverage(affinity_df, team_indices)\n",
        "\n",
        "        # Team member details\n",
        "        team_members = []\n",
        "        for idx in team_indices:\n",
        "            researcher_name = affinity_df.index[idx]\n",
        "            researcher_scores = affinity_df.iloc[idx].values\n",
        "            avg_score = np.mean(researcher_scores)\n",
        "            max_score = np.max(researcher_scores)\n",
        "\n",
        "            # Find top skills for this researcher\n",
        "            top_skill_indices = np.argsort(researcher_scores)[-3:][::-1]  # Top 3\n",
        "            top_skills = [(skills_list[i], researcher_scores[i]) for i in top_skill_indices]\n",
        "\n",
        "            team_members.append({\n",
        "                'name': researcher_name,\n",
        "                'index': idx,\n",
        "                'avg_affinity': avg_score,\n",
        "                'max_affinity': max_score,\n",
        "                'top_skills': top_skills,\n",
        "                'all_scores': researcher_scores.tolist()\n",
        "            })\n",
        "\n",
        "        # Skill coverage analysis\n",
        "        skill_analysis = []\n",
        "        for i, (skill, coverage) in enumerate(zip(skills_list, skill_coverages)):\n",
        "            # Find which team member provides this coverage\n",
        "            team_scores_for_skill = [member['all_scores'][i] for member in team_members]\n",
        "            best_member_idx = np.argmax(team_scores_for_skill)\n",
        "            best_member = team_members[best_member_idx]\n",
        "\n",
        "            coverage_level = 'High' if coverage >= 70 else 'Medium' if coverage >= 40 else 'Low'\n",
        "\n",
        "            skill_analysis.append({\n",
        "                'skill': skill,\n",
        "                'coverage_score': coverage,\n",
        "                'coverage_level': coverage_level,\n",
        "                'primary_expert': best_member['name'],\n",
        "                'expert_score': team_scores_for_skill[best_member_idx]\n",
        "            })\n",
        "\n",
        "        # Coverage statistics\n",
        "        high_coverage_count = sum(1 for s in skill_analysis if s['coverage_level'] == 'High')\n",
        "        medium_coverage_count = sum(1 for s in skill_analysis if s['coverage_level'] == 'Medium')\n",
        "        low_coverage_count = sum(1 for s in skill_analysis if s['coverage_level'] == 'Low')\n",
        "\n",
        "        coverage_report = {\n",
        "            'team_size': len(team_members),\n",
        "            'overall_coverage_score': overall_coverage,\n",
        "            'team_members': team_members,\n",
        "            'skill_analysis': skill_analysis,\n",
        "            'coverage_statistics': {\n",
        "                'high_coverage_skills': high_coverage_count,\n",
        "                'medium_coverage_skills': medium_coverage_count,\n",
        "                'low_coverage_skills': low_coverage_count,\n",
        "                'coverage_distribution': {\n",
        "                    'high_pct': 100 * high_coverage_count / len(skills_list),\n",
        "                    'medium_pct': 100 * medium_coverage_count / len(skills_list),\n",
        "                    'low_pct': 100 * low_coverage_count / len(skills_list)\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(f\"✅ Coverage report generated\")\n",
        "        print(f\"📊 Overall coverage: {overall_coverage:.2f}\")\n",
        "        print(f\"🔥 High coverage skills: {high_coverage_count}/{len(skills_list)}\")\n",
        "\n",
        "        return coverage_report\n",
        "\n",
        "    def format_gap_analysis_prompt(self, coverage_report, skills_list, solicitation_data=None):\n",
        "        \"\"\"\n",
        "        Format the prompt for Claude API gap analysis.\n",
        "\n",
        "        Args:\n",
        "            coverage_report (dict): Team coverage analysis\n",
        "            skills_list (list): Required skills\n",
        "            solicitation_data (dict): Original solicitation information\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted prompt for Claude API\n",
        "        \"\"\"\n",
        "\n",
        "        # Team summary\n",
        "        team_summary = f\"PROPOSED RESEARCH TEAM ({coverage_report['team_size']} members):\\n\"\n",
        "        for i, member in enumerate(coverage_report['team_members']):\n",
        "            role = \"Principal Investigator (PI)\" if i == 0 else f\"Co-Investigator {i}\"\n",
        "            team_summary += f\"\\n{i+1}. {member['name']} - {role}\\n\"\n",
        "            team_summary += f\"   Average Affinity: {member['avg_affinity']:.2f}\\n\"\n",
        "            team_summary += f\"   Top Expertise Areas:\\n\"\n",
        "            for skill, score in member['top_skills']:\n",
        "                team_summary += f\"     • {skill}: {score:.1f}\\n\"\n",
        "\n",
        "        # Coverage analysis\n",
        "        coverage_summary = f\"\\nTEAM COVERAGE ANALYSIS:\\n\"\n",
        "        coverage_summary += f\"Overall Team Coverage Score: {coverage_report['overall_coverage_score']:.2f}/100\\n\\n\"\n",
        "\n",
        "        coverage_summary += \"HIGH COVERAGE SKILLS (≥70):\\n\"\n",
        "        for skill in coverage_report['skill_analysis']:\n",
        "            if skill['coverage_level'] == 'High':\n",
        "                coverage_summary += f\"• {skill['skill']}: {skill['coverage_score']:.1f} (Expert: {skill['primary_expert']})\\n\"\n",
        "\n",
        "        coverage_summary += \"\\nMEDIUM COVERAGE SKILLS (40-69):\\n\"\n",
        "        for skill in coverage_report['skill_analysis']:\n",
        "            if skill['coverage_level'] == 'Medium':\n",
        "                coverage_summary += f\"• {skill['skill']}: {skill['coverage_score']:.1f} (Expert: {skill['primary_expert']})\\n\"\n",
        "\n",
        "        coverage_summary += \"\\nLOW COVERAGE SKILLS (<40) - POTENTIAL GAPS:\\n\"\n",
        "        for skill in coverage_report['skill_analysis']:\n",
        "            if skill['coverage_level'] == 'Low':\n",
        "                coverage_summary += f\"• {skill['skill']}: {skill['coverage_score']:.1f} (Expert: {skill['primary_expert']})\\n\"\n",
        "\n",
        "        # Solicitation context\n",
        "        solicitation_context = \"\"\n",
        "        if solicitation_data:\n",
        "            solicitation_context = f\"\\nORIGINAL SOLICITATION CONTEXT:\\n\"\n",
        "            solicitation_context += f\"Title: {solicitation_data.get('title', 'N/A')}\\n\"\n",
        "            solicitation_context += f\"Abstract: {solicitation_data.get('abstract', 'N/A')[:500]}...\\n\"\n",
        "\n",
        "        # Main prompt\n",
        "        prompt = f\"\"\"As an expert NSF Program Manager and research strategy consultant, analyze this proposed research team for a competitive grant application.\n",
        "\n",
        "{team_summary}\n",
        "\n",
        "{coverage_summary}\n",
        "\n",
        "{solicitation_context}\n",
        "\n",
        "Please provide a comprehensive strategic analysis covering:\n",
        "\n",
        "1. **TEAM STRENGTHS**: What are the key strengths of this team composition? How do their expertise areas complement each other?\n",
        "\n",
        "2. **COVERAGE GAPS & RISKS**: Analyze the low-coverage skills. Are these critical gaps that could harm competitiveness? Which gaps are most concerning?\n",
        "\n",
        "3. **STRATEGIC RECOMMENDATIONS**:\n",
        "   - Should additional collaborators be recruited for specific gaps?\n",
        "   - How can the team leverage their strengths to compensate for weaknesses?\n",
        "   - What sections of the proposal should each member lead?\n",
        "\n",
        "4. **COMPETITIVE POSITIONING**: How competitive is this team compared to typical NSF applications? What makes them stand out?\n",
        "\n",
        "5. **PROPOSAL STRATEGY**: Provide 3-5 specific, actionable recommendations for structuring their proposal to maximize success.\n",
        "\n",
        "Format your response as a professional strategic report suitable for team planning meetings.\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_gap_analysis(self, coverage_report, skills_list, solicitation_data=None):\n",
        "        \"\"\"\n",
        "        Generate AI-powered gap analysis using Claude API.\n",
        "\n",
        "        Args:\n",
        "            coverage_report (dict): Team coverage analysis\n",
        "            skills_list (list): Required skills list\n",
        "            solicitation_data (dict): Optional solicitation data\n",
        "\n",
        "        Returns:\n",
        "            str: Strategic analysis from Claude API\n",
        "        \"\"\"\n",
        "        if not self.claude_client:\n",
        "            return self._generate_fallback_analysis(coverage_report)\n",
        "\n",
        "        print(\"🤖 Generating strategic analysis with Claude API...\")\n",
        "\n",
        "        try:\n",
        "            prompt = self.format_gap_analysis_prompt(coverage_report, skills_list, solicitation_data)\n",
        "\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=2000,\n",
        "                temperature=0.7,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            analysis = response.content[0].text\n",
        "            print(\"✅ Strategic analysis generated\")\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Claude API analysis failed: {e}\")\n",
        "            return self._generate_fallback_analysis(coverage_report)\n",
        "\n",
        "    def _generate_fallback_analysis(self, coverage_report):\n",
        "        \"\"\"Generate basic analysis when Claude API is unavailable.\"\"\"\n",
        "\n",
        "        analysis = \"STRATEGIC ANALYSIS (Basic Report)\\n\"\n",
        "        analysis += \"=\" * 50 + \"\\n\\n\"\n",
        "\n",
        "        analysis += f\"TEAM OVERVIEW:\\n\"\n",
        "        analysis += f\"Team Size: {coverage_report['team_size']} researchers\\n\"\n",
        "        analysis += f\"Overall Coverage: {coverage_report['overall_coverage_score']:.2f}/100\\n\\n\"\n",
        "\n",
        "        analysis += \"COVERAGE DISTRIBUTION:\\n\"\n",
        "        stats = coverage_report['coverage_statistics']\n",
        "        analysis += f\"High Coverage Skills: {stats['high_coverage_skills']} ({stats['coverage_distribution']['high_pct']:.1f}%)\\n\"\n",
        "        analysis += f\"Medium Coverage Skills: {stats['medium_coverage_skills']} ({stats['coverage_distribution']['medium_pct']:.1f}%)\\n\"\n",
        "        analysis += f\"Low Coverage Skills: {stats['low_coverage_skills']} ({stats['coverage_distribution']['low_pct']:.1f}%)\\n\\n\"\n",
        "\n",
        "        if stats['low_coverage_skills'] > 0:\n",
        "            analysis += \"ATTENTION NEEDED:\\n\"\n",
        "            analysis += f\"The team has {stats['low_coverage_skills']} skills with low coverage. Consider recruiting additional expertise or developing strategic partnerships.\\n\\n\"\n",
        "\n",
        "        analysis += \"RECOMMENDATIONS:\\n\"\n",
        "        analysis += \"• Review low-coverage skills for recruitment opportunities\\n\"\n",
        "        analysis += \"• Leverage high-coverage areas as competitive advantages\\n\"\n",
        "        analysis += \"• Consider collaborative arrangements for gap areas\\n\"\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def create_strategic_report(self, affinity_df, metadata=None, skills_list=None, solicitation_data=None):\n",
        "        \"\"\"\n",
        "        Main function to create comprehensive strategic report.\n",
        "\n",
        "        Args:\n",
        "            affinity_df (pd.DataFrame): Affinity matrix\n",
        "            metadata (dict): Optional metadata\n",
        "            skills_list (list): Required skills\n",
        "            solicitation_data (dict): Original solicitation data\n",
        "\n",
        "        Returns:\n",
        "            dict: Complete strategic report\n",
        "        \"\"\"\n",
        "        print(\"🚀 CREATING STRATEGIC REPORT\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Extract skills list if not provided\n",
        "        if skills_list is None and metadata:\n",
        "            skills_list = metadata.get('skills_checklist', [])\n",
        "\n",
        "        if skills_list is None:\n",
        "            # Extract from column names\n",
        "            skills_list = [col.split(': ', 1)[1] if ': ' in col else col for col in affinity_df.columns]\n",
        "\n",
        "        # Step 1: Run Dream Team Algorithm\n",
        "        team_indices, selection_history = self.dream_team_greedy_algorithm(affinity_df)\n",
        "\n",
        "        # Step 2: Generate Coverage Report\n",
        "        coverage_report = self.generate_coverage_report(affinity_df, team_indices, skills_list)\n",
        "\n",
        "        # Step 3: Generate Gap Analysis\n",
        "        strategic_analysis = self.generate_gap_analysis(coverage_report, skills_list, solicitation_data)\n",
        "\n",
        "        # Step 4: Compile Strategic Report\n",
        "        strategic_report = {\n",
        "            'report_metadata': {\n",
        "                'generated_at': datetime.now().isoformat(),\n",
        "                'solicitation_id': solicitation_data.get('solicitation_id') if solicitation_data else None,\n",
        "                'analysis_type': 'dream_team_strategic_report'\n",
        "            },\n",
        "            'dream_team': {\n",
        "                'team_indices': team_indices,\n",
        "                'selection_algorithm': 'greedy_marginal_gain',\n",
        "                'selection_history': selection_history\n",
        "            },\n",
        "            'coverage_analysis': coverage_report,\n",
        "            'strategic_analysis': strategic_analysis,\n",
        "            'skills_checklist': skills_list,\n",
        "            'solicitation_context': solicitation_data\n",
        "        }\n",
        "\n",
        "        print(\"✅ Strategic Report Generated!\")\n",
        "        return strategic_report\n",
        "\n",
        "    def save_strategic_report(self, strategic_report, output_path=None):\n",
        "        \"\"\"Save the strategic report to files.\"\"\"\n",
        "\n",
        "        if output_path is None:\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            output_path = f\"strategic_report_{timestamp}\"\n",
        "\n",
        "        # Save complete JSON report\n",
        "        json_path = f\"{output_path}.json\"\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(strategic_report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save human-readable text report\n",
        "        text_path = f\"{output_path}.txt\"\n",
        "        with open(text_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(self.format_text_report(strategic_report))\n",
        "\n",
        "        print(f\"💾 Strategic report saved:\")\n",
        "        print(f\"   📄 JSON: {json_path}\")\n",
        "        print(f\"   📝 Text: {text_path}\")\n",
        "\n",
        "        return json_path, text_path\n",
        "\n",
        "    def format_text_report(self, strategic_report):\n",
        "        \"\"\"Format strategic report as human-readable text.\"\"\"\n",
        "\n",
        "        report = \"NSF DREAM TEAM STRATEGIC REPORT\\n\"\n",
        "        report += \"=\" * 60 + \"\\n\\n\"\n",
        "\n",
        "        # Header\n",
        "        metadata = strategic_report['report_metadata']\n",
        "        report += f\"Generated: {metadata['generated_at']}\\n\"\n",
        "        if metadata.get('solicitation_id'):\n",
        "            report += f\"Solicitation ID: {metadata['solicitation_id']}\\n\"\n",
        "        report += \"\\n\"\n",
        "\n",
        "        # Dream Team\n",
        "        report += \"RECOMMENDED DREAM TEAM\\n\"\n",
        "        report += \"-\" * 25 + \"\\n\"\n",
        "\n",
        "        team_members = strategic_report['coverage_analysis']['team_members']\n",
        "        for i, member in enumerate(team_members):\n",
        "            role = \"Principal Investigator (PI)\" if i == 0 else f\"Co-Investigator {i}\"\n",
        "            report += f\"\\n{i+1}. {member['name']} - {role}\\n\"\n",
        "            report += f\"   Average Affinity Score: {member['avg_affinity']:.2f}\\n\"\n",
        "            report += f\"   Primary Expertise:\\n\"\n",
        "            for skill, score in member['top_skills'][:2]:  # Top 2 skills\n",
        "                report += f\"     • {skill}: {score:.1f}\\n\"\n",
        "\n",
        "        # Coverage Summary\n",
        "        coverage = strategic_report['coverage_analysis']\n",
        "        report += f\"\\nTEAM COVERAGE SUMMARY\\n\"\n",
        "        report += \"-\" * 22 + \"\\n\"\n",
        "        report += f\"Overall Coverage Score: {coverage['overall_coverage_score']:.2f}/100\\n\"\n",
        "\n",
        "        stats = coverage['coverage_statistics']\n",
        "        report += f\"High Coverage Skills: {stats['high_coverage_skills']} ({stats['coverage_distribution']['high_pct']:.1f}%)\\n\"\n",
        "        report += f\"Medium Coverage Skills: {stats['medium_coverage_skills']} ({stats['coverage_distribution']['medium_pct']:.1f}%)\\n\"\n",
        "        report += f\"Low Coverage Skills: {stats['low_coverage_skills']} ({stats['coverage_distribution']['low_pct']:.1f}%)\\n\"\n",
        "\n",
        "        # Strategic Analysis\n",
        "        report += f\"\\nSTRATEGIC ANALYSIS\\n\"\n",
        "        report += \"-\" * 18 + \"\\n\"\n",
        "        report += strategic_report['strategic_analysis']\n",
        "\n",
        "        return report\n",
        "\n",
        "    def display_summary(self, strategic_report):\n",
        "        \"\"\"Display a summary of the strategic report.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"📋 DREAM TEAM STRATEGIC REPORT SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Team overview\n",
        "        team_members = strategic_report['coverage_analysis']['team_members']\n",
        "        print(f\"🏆 Recommended Team Size: {len(team_members)}\")\n",
        "\n",
        "        for i, member in enumerate(team_members):\n",
        "            role = \"PI\" if i == 0 else f\"Co-I {i}\"\n",
        "            print(f\"   {i+1}. {member['name']} ({role}) - Avg Score: {member['avg_affinity']:.2f}\")\n",
        "\n",
        "        # Coverage stats\n",
        "        coverage = strategic_report['coverage_analysis']['coverage_statistics']\n",
        "        print(f\"\\n📊 Coverage Distribution:\")\n",
        "        print(f\"   🔥 High Coverage: {coverage['high_coverage_skills']} skills ({coverage['coverage_distribution']['high_pct']:.1f}%)\")\n",
        "        print(f\"   📊 Medium Coverage: {coverage['medium_coverage_skills']} skills ({coverage['coverage_distribution']['medium_pct']:.1f}%)\")\n",
        "        print(f\"   ⚠️  Low Coverage: {coverage['low_coverage_skills']} skills ({coverage['coverage_distribution']['low_pct']:.1f}%)\")\n",
        "\n",
        "        overall_score = strategic_report['coverage_analysis']['overall_coverage_score']\n",
        "        print(f\"\\n🎯 Overall Team Coverage Score: {overall_score:.2f}/100\")\n",
        "\n",
        "        if overall_score >= 70:\n",
        "            print(\"✅ EXCELLENT: Strong team with high coverage\")\n",
        "        elif overall_score >= 50:\n",
        "            print(\"✅ GOOD: Solid team with room for strategic partnerships\")\n",
        "        else:\n",
        "            print(\"⚠️ NEEDS WORK: Consider additional recruitment or collaborations\")\n",
        "\n",
        "def main_dream_team_analysis(affinity_csv_path, metadata_json_path=None, solicitation_json_path=None):\n",
        "    \"\"\"\n",
        "    Main function to run complete Dream Team analysis.\n",
        "\n",
        "    Args:\n",
        "        affinity_csv_path (str): Path to affinity matrix CSV\n",
        "        metadata_json_path (str): Path to affinity metadata JSON\n",
        "        solicitation_json_path (str): Path to original solicitation JSON\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"🎯 DREAM TEAM ASSEMBLER - PHASE 2 STRATEGIC OUTPUT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initialize assembler\n",
        "    assembler = DreamTeamAssembler()\n",
        "\n",
        "    # Load affinity matrix\n",
        "    affinity_df, affinity_metadata = assembler.load_affinity_matrix(affinity_csv_path, metadata_json_path)\n",
        "\n",
        "    # Load solicitation data if available\n",
        "    solicitation_data = None\n",
        "    if solicitation_json_path:\n",
        "        try:\n",
        "            with open(solicitation_json_path, 'r') as f:\n",
        "                solicitation_data = json.load(f)\n",
        "            print(f\"✅ Loaded solicitation data\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load solicitation data: {e}\")\n",
        "\n",
        "    # Extract skills list\n",
        "    skills_list = None\n",
        "    if affinity_metadata and 'skills_checklist' in affinity_metadata:\n",
        "        skills_list = affinity_metadata['skills_checklist']\n",
        "    elif solicitation_data and 'required_skills_checklist' in solicitation_data:\n",
        "        skills_list = solicitation_data['required_skills_checklist']\n",
        "\n",
        "    # Create strategic report\n",
        "    strategic_report = assembler.create_strategic_report(\n",
        "        affinity_df=affinity_df,\n",
        "        metadata=affinity_metadata,\n",
        "        skills_list=skills_list,\n",
        "        solicitation_data=solicitation_data\n",
        "    )\n",
        "\n",
        "    # Save report\n",
        "    json_path, text_path = assembler.save_strategic_report(strategic_report)\n",
        "\n",
        "    # Display summary\n",
        "    assembler.display_summary(strategic_report)\n",
        "\n",
        "    print(f\"\\n✅ Dream Team Analysis Complete!\")\n",
        "    print(f\"📄 Full report available at: {text_path}\")\n",
        "\n",
        "    return strategic_report\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage - update paths as needed\n",
        "    AFFINITY_CSV = \"/content/drive/MyDrive/datastore/affinity_matrix_SOL_EXAMPLE1.csv\"\n",
        "    AFFINITY_METADATA = \"/content/drive/MyDrive/datastore/affinity_matrix_SOL_EXAMPLE1_metadata1.json\"\n",
        "    SOLICITATION_JSON = \"NSF 24-569_ Mathematical Foundations of Artificial Intelligence (MFAI) _ NSF - National Science Foundation_analysis.json\"  # Optional\n",
        "\n",
        "    try:\n",
        "        report = main_dream_team_analysis(\n",
        "            affinity_csv_path=AFFINITY_CSV,\n",
        "            metadata_json_path=AFFINITY_METADATA,\n",
        "            solicitation_json_path=SOLICITATION_JSON\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        print(\"Please verify file paths and try again\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "tW3A72pyyts1",
        "outputId": "ad684d6a-34a8-42f0-e2e2-01076daea378"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 DREAM TEAM ASSEMBLER - PHASE 2 STRATEGIC OUTPUT\n",
            "============================================================\n",
            "✅ Claude API client initialized for gap analysis\n",
            "📊 Loading affinity matrix from: /content/drive/MyDrive/datastore/affinity_matrix_SOL_EXAMPLE1.csv\n",
            "✅ Loaded matrix: 51 researchers × 17 skills\n",
            "✅ Loaded metadata\n",
            "✅ Loaded solicitation data\n",
            "🚀 CREATING STRATEGIC REPORT\n",
            "============================================================\n",
            "🎯 Running Dream Team Greedy Algorithm...\n",
            "==================================================\n",
            "🏆 Step 1 - PI Selection: Subasish Das (A5053621729)\n",
            "    Initial coverage: 34.17\n",
            "✅ Step 2 - Added: Brady T. West (A5010637807)\n",
            "    Marginal gain: +3.95, New coverage: 38.12\n",
            "✅ Step 3 - Added: Russell Lang (A5112296295)\n",
            "    Marginal gain: +2.34, New coverage: 40.46\n",
            "✅ Step 4 - Added: Martin Burtscher (A5103125276)\n",
            "    Marginal gain: +0.75, New coverage: 41.22\n",
            "\n",
            "🎯 Final Dream Team: 4 researchers\n",
            "📊 Final team coverage: 41.22\n",
            "📋 Generating Coverage Report...\n",
            "✅ Coverage report generated\n",
            "📊 Overall coverage: 41.22\n",
            "🔥 High coverage skills: 0/17\n",
            "🤖 Generating strategic analysis with Claude API...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-268198437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         report = main_dream_team_analysis(\n\u001b[0m\u001b[1;32m    645\u001b[0m             \u001b[0maffinity_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAFFINITY_CSV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mmetadata_json_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAFFINITY_METADATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-23-268198437.py\u001b[0m in \u001b[0;36mmain_dream_team_analysis\u001b[0;34m(affinity_csv_path, metadata_json_path, solicitation_json_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;31m# Create strategic report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m     strategic_report = assembler.create_strategic_report(\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0maffinity_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maffinity_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maffinity_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-23-268198437.py\u001b[0m in \u001b[0;36mcreate_strategic_report\u001b[0;34m(self, affinity_df, metadata, skills_list, solicitation_data)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;31m# Step 3: Generate Gap Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mstrategic_analysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_gap_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoverage_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskills_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolicitation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# Step 4: Compile Strategic Report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-23-268198437.py\u001b[0m in \u001b[0;36mgenerate_gap_analysis\u001b[0;34m(self, coverage_report, skills_list, solicitation_data)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_gap_analysis_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoverage_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskills_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolicitation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             response = self.claude_client.messages.create(\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-3-sonnet-20240229\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    976\u001b[0m             )\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    979\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         )\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m   1038\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Researcher Profile Datastore: Built system to extract top 50 Texas State researchers + Tahir Ekin, generating embeddings for all papers using all-MiniLM-L6-v2 sentence transformer model with recency weights Wt = max(0, 1-(CurrentYear-PublicationYear)/10).\n",
        "\n",
        "Storage Structure: Created indexed datastore in Google Drive with researcher_profiles_metadata.parquet (paper metadata), researcher_embeddings.npy (384-dim vectors), embedding_index.json (work_id→position), researcher_index.json (researcher_id→work_ids).\n",
        "\n",
        "PDF Solicitation Processor: Hybrid system using PyMuPDF extraction + Claude API (narrative skills) + OpenAlex BERT classifier (formal topics), with fusion logic to create final Required Skills Checklist.\n",
        "\n",
        "Skill Affinity Engine: Implements mathematical formula SkillAffinityScore(R,Sk) = max(cosine_similarity(paper,skill) × recency_weight) × 100 for every researcher×skill combination.\n",
        "\n",
        "Affinity Matrix: 51 researchers × 17 skills matrix with percentage scores (0-100), saved as CSV with metadata JSON for tracking and analysis.\n",
        "Dream Team Greedy Algorithm: Selects optimal 2-4 researcher teams by iteratively choosing candidates with maximum marginal gain in team coverage scores.\n",
        "\n",
        "Coverage Analysis: Calculates Team Coverage Score (average of max affinity per skill), categorizes skills as High/Medium/Low coverage, identifies primary experts per skill area.\n",
        "\n",
        "Strategic Report Generation: Uses Claude API to analyze team composition, coverage gaps, competitive positioning, and proposal strategy recommendations.\n",
        "End-to-End Pipeline: PDF→Skills Extraction→Affinity Calculation→Team Selection→Strategic Analysis, all integrated with proper error handling and fallback methods.\n",
        "\n",
        "Key Files: Researcher datastore (5 files), solicitation JSON objects, affinity matrices with metadata, and comprehensive strategic reports (JSON + human-readable text formats)."
      ],
      "metadata": {
        "id": "mENmaWcP0xcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V2 with better file management\n"
      ],
      "metadata": {
        "id": "lQc-c8qela63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: PDF Processing & Run Initialization\n",
        "# ==============================================================================\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from google.colab import drive, userdata\n",
        "from transformers import pipeline\n",
        "import anthropic\n",
        "from dataclasses import dataclass, asdict\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# --- Only change this section for a new analysis ---\n",
        "DRIVE_MOUNT_PATH = '/content/drive'\n",
        "DATASTORE_PATH = \"/content/drive/MyDrive/datastore/\"\n",
        "# This is the ONLY line you need to change to process a new file.\n",
        "PDF_FILE_PATH = \"/content/drive/MyDrive/datastore/NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\"\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@dataclass\n",
        "class StructuredSolicitationObject:\n",
        "    \"\"\"\n",
        "    Structured object containing solicitation metadata and required skills.\n",
        "    \"\"\"\n",
        "    # Metadata\n",
        "    solicitation_id: str\n",
        "    title: str\n",
        "    abstract: str\n",
        "    processed_at: str\n",
        "    pdf_filename: str\n",
        "\n",
        "    # Skills from both paths\n",
        "    narrative_skills: List[str]  # From Claude API (Path A)\n",
        "    formal_topics: List[Dict]    # From OpenAlex classifier (Path B)\n",
        "\n",
        "    # Final combined checklist\n",
        "    required_skills_checklist: List[str]\n",
        "\n",
        "    # Processing details\n",
        "    text_length: int\n",
        "    processing_method: str = \"hybrid_deconstruction\"\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self, filepath: str):\n",
        "        \"\"\"Save to JSON file.\"\"\"\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PDF SOLICITATION PROCESSOR CLASS\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "class PDFSolicitationProcessor:\n",
        "    \"\"\"\n",
        "    Processes PDF solicitations from a file path to extract required skills.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.claude_client = None\n",
        "        self.topic_classifier = None\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"Initialize Claude API client and OpenAlex topic classifier.\"\"\"\n",
        "        print(\"Setting up models...\")\n",
        "        try:\n",
        "            api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "            self.claude_client = anthropic.Anthropic(api_key=api_key)\n",
        "            print(\"✅ Claude API client initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Claude API setup failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            print(\"Loading OpenAlex topic classifier...\")\n",
        "            self.topic_classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=\"OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract\"\n",
        "            )\n",
        "            print(\"✅ OpenAlex topic classifier loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Topic classifier setup failed: {e}\")\n",
        "\n",
        "    def _extract_text_from_pdf(self, filepath: str) -> Tuple[str, str, str]:\n",
        "        \"\"\"Extracts text content from a PDF given a file path.\"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"The file was not found at: {filepath}\")\n",
        "\n",
        "        filename = os.path.basename(filepath)\n",
        "        print(f\"📄 Processing: {filename}\")\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(filepath)\n",
        "            full_text = \"\".join([page.get_text() for page in doc])\n",
        "            doc.close()\n",
        "\n",
        "            if not full_text.strip():\n",
        "                 raise ValueError(\"Extracted text is empty. The PDF might be an image.\")\n",
        "\n",
        "            title, abstract = self._extract_title_and_abstract(full_text, filename)\n",
        "            print(f\"✅ Extracted {len(full_text)} characters from PDF.\")\n",
        "            return filename, title, abstract\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting text from PDF '{filename}': {e}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_title_and_abstract(self, full_text: str, filename: str) -> Tuple[str, str]:\n",
        "        \"\"\"Extract title and abstract from full text using heuristics.\"\"\"\n",
        "        lines = [line.strip() for line in full_text.split('\\n') if line.strip()]\n",
        "        title = filename.replace('.pdf', '').replace('_', ' ').replace('-', ' ')\n",
        "        for line in lines[:15]: # Check more lines for title\n",
        "            if 20 < len(line) < 250 and not line.isupper(): # Avoid all-caps headers\n",
        "                title = line\n",
        "                break\n",
        "\n",
        "        abstract = \"\"\n",
        "        abstract_started = False\n",
        "        for line in lines:\n",
        "            line_lower = line.lower()\n",
        "            if not abstract_started and any(marker in line_lower for marker in ['abstract', 'summary', 'overview']):\n",
        "                abstract_started = True\n",
        "                if len(line) > len('abstract') + 10: abstract += line.split(maxsplit=1)[1]\n",
        "                continue\n",
        "            if abstract_started:\n",
        "                abstract += \" \" + line\n",
        "                if len(abstract) > 1500 or any(marker in line_lower for marker in ['introduction', 'background']):\n",
        "                    break\n",
        "        if not abstract: abstract = ' '.join(lines[:10]) # Fallback\n",
        "        return title.strip(), abstract.strip()[:2000] # Increased limit\n",
        "\n",
        "    def extract_narrative_skills_claude(self, text: str) -> List[str]:\n",
        "        \"\"\"Path A: Extract narrative skills using Claude API.\"\"\"\n",
        "        if not self.claude_client:\n",
        "            print(\"⚠️ Claude API not available, skipping narrative skills.\")\n",
        "            return []\n",
        "\n",
        "        prompt = f\"\"\"As an expert research program analyst, identify the 5-7 most critical and distinct areas of expertise required by this research solicitation. Focus on specific technical skills, domain knowledge, and methodological expertise.\n",
        "\n",
        "Solicitation text:\n",
        "---\n",
        "{text}\n",
        "---\n",
        "\n",
        "Provide your response as a numbered list of distinct expertise areas. Each item should be a concise phrase.\n",
        "\"\"\"\n",
        "        try:\n",
        "            print(\"🤖 Calling Claude API for narrative skills...\")\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\",\n",
        "                max_tokens=1000,\n",
        "                temperature=0.2,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            skills = self._parse_claude_response(response.content[0].text)\n",
        "            print(f\"✅ Extracted {len(skills)} narrative skills from Claude.\")\n",
        "            return skills\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Claude API call failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _parse_claude_response(self, response_text: str) -> List[str]:\n",
        "        \"\"\"Parse Claude's response to extract a list of skills.\"\"\"\n",
        "        skills = []\n",
        "        for line in response_text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if re.match(r'^\\d+\\.\\s*', line):\n",
        "                skill = re.sub(r'^\\d+\\.\\s*', '', line)\n",
        "                skills.append(skill.strip())\n",
        "        return skills[:7]\n",
        "\n",
        "    def extract_formal_topics_openalex(self, title: str, abstract: str) -> List[Dict]:\n",
        "      \"\"\"\n",
        "      Path B: Extract formal topics using OpenAlex classifier with corrected data structure handling.\n",
        "      \"\"\"\n",
        "      if not self.topic_classifier:\n",
        "          print(\"⚠️ Topic classifier not available, skipping formal topics.\")\n",
        "          return []\n",
        "\n",
        "      formatted_text = f\"<TITLE> {title}\\n<ABSTRACT> {abstract}\"\n",
        "      print(\"🔬 Running OpenAlex topic classification...\")\n",
        "\n",
        "      try:\n",
        "          # Get predictions from the model. The output is a simple list of dicts.\n",
        "          predictions = self.topic_classifier(formatted_text, top_k=10, truncation=True)\n",
        "          # print(f\"   [DEBUG] Raw output from OpenAlex model: {predictions}\") # You can remove this now\n",
        "\n",
        "          if not predictions:\n",
        "              print(\"   OpenAlex model returned no valid predictions.\")\n",
        "              return []\n",
        "\n",
        "          # --- CORRECTED LOOP ---\n",
        "          # We iterate directly over 'predictions', which is the list of dictionaries.\n",
        "          formal_topics = []\n",
        "          for topic in predictions:\n",
        "              # Check if the item is a dictionary with the keys we need\n",
        "              if isinstance(topic, dict) and 'label' in topic and 'score' in topic:\n",
        "                  # We can now lower the threshold since we see the scores are generally low\n",
        "                  if topic['score'] > 0.01: # Lowered threshold to include the results\n",
        "                      formal_topics.append({\n",
        "                          'topic': topic['label'],\n",
        "                          'score': round(topic['score'], 4)\n",
        "                      })\n",
        "              else:\n",
        "                  print(f\"   ⚠️ Skipping unexpected item in model predictions: {topic}\")\n",
        "\n",
        "          print(f\"✅ Extracted {len(formal_topics)} formal topics from OpenAlex.\")\n",
        "          return formal_topics\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"⚠️ An exception occurred during topic classification: {e}\")\n",
        "          return []\n",
        "\n",
        "    def fusion_logic(self, narrative_skills: List[str], formal_topics: List[Dict]) -> List[str]:\n",
        "        \"\"\"Combine narrative skills and formal topics, removing duplicates.\"\"\"\n",
        "        print(\"🔄 Applying fusion logic...\")\n",
        "        combined_skills = list(narrative_skills)\n",
        "        narrative_lower = ' '.join(narrative_skills).lower()\n",
        "\n",
        "        for topic in formal_topics:\n",
        "            topic_name = topic['topic'].split(': ', 1)[-1] # Remove ID like \"123: \"\n",
        "            is_duplicate = topic_name.lower() in narrative_lower\n",
        "            if not is_duplicate:\n",
        "                combined_skills.append(f\"Expertise in {topic_name}\")\n",
        "\n",
        "        print(f\"✅ Created final checklist with {len(combined_skills)} skills.\")\n",
        "        return combined_skills\n",
        "\n",
        "    def process_solicitation(self, pdf_filepath: str) -> Optional[StructuredSolicitationObject]:\n",
        "        \"\"\"Main processing pipeline for a PDF solicitation from a given path.\"\"\"\n",
        "        print(\"🚀 Starting PDF Solicitation Processing Pipeline\")\n",
        "        print(\"=\" * 60)\n",
        "        try:\n",
        "            filename, title, abstract = self._extract_text_from_pdf(pdf_filepath)\n",
        "\n",
        "            # Input for Claude can be a simple combination\n",
        "            claude_input_text = f\"Title: {title}. Abstract: {abstract}\"\n",
        "            print(f\"\\n📊 Text stats for analysis: {len(claude_input_text)} characters.\")\n",
        "\n",
        "            # Path A: Claude\n",
        "            narrative_skills = self.extract_narrative_skills_claude(claude_input_text)\n",
        "\n",
        "            # Path B: OpenAlex (uses corrected function call)\n",
        "            formal_topics = self.extract_formal_topics_openalex(title, abstract)\n",
        "\n",
        "            # Path C: Fusion\n",
        "            required_skills_checklist = self.fusion_logic(narrative_skills, formal_topics)\n",
        "\n",
        "            solicitation_obj = StructuredSolicitationObject(\n",
        "                solicitation_id=f\"SOL_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "                title=title, abstract=abstract,\n",
        "                processed_at=datetime.now().isoformat(),\n",
        "                pdf_filename=filename,\n",
        "                narrative_skills=narrative_skills,\n",
        "                formal_topics=formal_topics,\n",
        "                required_skills_checklist=required_skills_checklist,\n",
        "                text_length=len(claude_input_text))\n",
        "\n",
        "            output_filename = f\"{filename.replace('.pdf', '')}_analysis.json\"\n",
        "            solicitation_obj.to_json(output_filename)\n",
        "            print(f\"\\n✅ Processing complete! Saved to: {output_filename}\")\n",
        "            return solicitation_obj\n",
        "        except Exception as e:\n",
        "            print(f\"❌ A fatal error occurred during processing: {e}\")\n",
        "            return None\n",
        "\n",
        "    def display_results(self, solicitation_obj: Optional[StructuredSolicitationObject]):\n",
        "        \"\"\"Display processing results in a readable format.\"\"\"\n",
        "        if not solicitation_obj:\n",
        "            print(\"\\nNo results to display due to a processing error.\")\n",
        "            return\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"📋 SOLICITATION PROCESSING RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"🆔 ID: {solicitation_obj.solicitation_id}\")\n",
        "        print(f\"📄 File: {solicitation_obj.pdf_filename}\")\n",
        "        print(f\"📝 Title: {solicitation_obj.title}\")\n",
        "        print(f\"\\n🤖 Path A - Narrative Skills (Claude):\")\n",
        "        for i, skill in enumerate(solicitation_obj.narrative_skills, 1): print(f\"   {i}. {skill}\")\n",
        "        print(f\"\\n🔬 Path B - Formal Topics (OpenAlex):\")\n",
        "        for i, topic in enumerate(solicitation_obj.formal_topics, 1): print(f\"   {i}. {topic['topic']} (Score: {topic['score']:.3f})\")\n",
        "        print(f\"\\n✅ Final Hybrid Skills Checklist:\")\n",
        "        for i, skill in enumerate(solicitation_obj.required_skills_checklist, 1): print(f\"   {i}. {skill}\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Main Execution for Cell 1\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings('ignore')\n",
        "    print(\"🚀 Starting Pipeline: Cell 1 - PDF Processing\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        if not os.path.exists(DRIVE_MOUNT_PATH):\n",
        "            drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
        "            print(\"✅ Google Drive mounted successfully.\")\n",
        "        else:\n",
        "            print(\"✅ Google Drive already mounted.\")\n",
        "\n",
        "        # --- 1. Generate a Unique Run ID from the PDF Filename ---\n",
        "        base_filename = os.path.basename(PDF_FILE_PATH)\n",
        "        sanitized_base = os.path.splitext(base_filename)[0]\n",
        "        # Make the filename safe for all systems\n",
        "        run_id = re.sub(r'[^a-zA-Z0-9_-]', '_', sanitized_base)\n",
        "        print(f\"🆔 Generated unique Run ID: {run_id}\")\n",
        "\n",
        "        # --- 2. Define Output Path ---\n",
        "        solicitation_output_path = os.path.join(DATASTORE_PATH, f\"{run_id}_solicitation_analysis.json\")\n",
        "        print(f\"💾 Defined solicitation output path:\\n   {solicitation_output_path}\")\n",
        "\n",
        "\n",
        "        # --- 3. Process the Solicitation ---\n",
        "        processor = PDFSolicitationProcessor()\n",
        "        result_obj = processor.process_solicitation(PDF_FILE_PATH) # This function will now just return the object\n",
        "\n",
        "        if result_obj:\n",
        "            # --- 4. Save the Output Manually ---\n",
        "            result_obj.to_json(solicitation_output_path)\n",
        "            print(f\"\\n✅ Analysis complete! Saved to: {solicitation_output_path}\")\n",
        "            processor.display_results(result_obj)\n",
        "            print(\"\\n✅ Cell 1 finished. Proceed to Cell 2.\")\n",
        "        else:\n",
        "            print(\"❌ Processing failed, no result object created. Cannot proceed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ A critical error occurred in Cell 1: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OodOGLF6lah2",
        "outputId": "2f9970ab-e383-4e87-e7d9-3c3b06f7149d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Pipeline: Cell 1 - PDF Processing\n",
            "============================================================\n",
            "✅ Google Drive already mounted.\n",
            "🆔 Generated unique Run ID: NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation\n",
            "💾 Defined solicitation output path:\n",
            "   /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_solicitation_analysis.json\n",
            "Setting up models...\n",
            "✅ Claude API client initialized\n",
            "Loading OpenAlex topic classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OpenAlex topic classifier loaded\n",
            "🚀 Starting PDF Solicitation Processing Pipeline\n",
            "============================================================\n",
            "📄 Processing: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\n",
            "✅ Extracted 57431 characters from PDF.\n",
            "\n",
            "📊 Text stats for analysis: 1641 characters.\n",
            "🤖 Calling Claude API for narrative skills...\n",
            "✅ Extracted 7 narrative skills from Claude.\n",
            "🔬 Running OpenAlex topic classification...\n",
            "✅ Extracted 7 formal topics from OpenAlex.\n",
            "🔄 Applying fusion logic...\n",
            "✅ Created final checklist with 14 skills.\n",
            "\n",
            "✅ Processing complete! Saved to: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation_analysis.json\n",
            "\n",
            "✅ Analysis complete! Saved to: /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_solicitation_analysis.json\n",
            "\n",
            "============================================================\n",
            "📋 SOLICITATION PROCESSING RESULTS\n",
            "============================================================\n",
            "🆔 ID: SOL_20250629_031908\n",
            "📄 File: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences (CAIG) | NSF - National Science Foundation.pdf\n",
            "📝 Title: NSF 25-530: Collaborations in Artificial Intelligence and Geosciences\n",
            "\n",
            "🤖 Path A - Narrative Skills (Claude):\n",
            "   1. Artificial Intelligence (AI) and Machine Learning\n",
            "   2. Geosciences (e.g., Earth sciences, atmospheric sciences, oceanography)\n",
            "   3. Interdisciplinary research and collaboration\n",
            "   4. Project management and team leadership\n",
            "   5. Software engineering and research software development\n",
            "   6. Data analysis and computational methods\n",
            "   7. Domain-specific knowledge related to the proposed AI applications in geosciences\n",
            "\n",
            "🔬 Path B - Formal Topics (OpenAlex):\n",
            "   1. 1986: Management and Reproducibility of Scientific Workflows (Score: 0.404)\n",
            "   2. 1937: Data Sharing and Stewardship in Science (Score: 0.068)\n",
            "   3. 3937: Challenges and Innovations in Bioinformatics Education (Score: 0.040)\n",
            "   4. 1948: Accelerating Materials Innovation through Informatics (Score: 0.039)\n",
            "   5. 4427: Hydrologic Data Management and Analysis (Score: 0.018)\n",
            "   6. 3650: Scientific Computing and Data Analysis with Python (Score: 0.015)\n",
            "   7. 1636: Artificial Intelligence in Medicine (Score: 0.011)\n",
            "\n",
            "✅ Final Hybrid Skills Checklist:\n",
            "   1. Artificial Intelligence (AI) and Machine Learning\n",
            "   2. Geosciences (e.g., Earth sciences, atmospheric sciences, oceanography)\n",
            "   3. Interdisciplinary research and collaboration\n",
            "   4. Project management and team leadership\n",
            "   5. Software engineering and research software development\n",
            "   6. Data analysis and computational methods\n",
            "   7. Domain-specific knowledge related to the proposed AI applications in geosciences\n",
            "   8. Expertise in Management and Reproducibility of Scientific Workflows\n",
            "   9. Expertise in Data Sharing and Stewardship in Science\n",
            "   10. Expertise in Challenges and Innovations in Bioinformatics Education\n",
            "   11. Expertise in Accelerating Materials Innovation through Informatics\n",
            "   12. Expertise in Hydrologic Data Management and Analysis\n",
            "   13. Expertise in Scientific Computing and Data Analysis with Python\n",
            "   14. Expertise in Artificial Intelligence in Medicine\n",
            "\n",
            "============================================================\n",
            "\n",
            "✅ Cell 1 finished. Proceed to Cell 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Skill Affinity Engine\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "class SkillAffinityEngine:\n",
        "    \"\"\"\n",
        "    Phase 1: Core Analysis - Calculates affinity between researchers and required skills.\n",
        "    \"\"\"\n",
        "    def __init__(self, datastore_path=\"/content/drive/My Drive/datastore/\"):\n",
        "        self.datastore_path = datastore_path\n",
        "        self.model = None\n",
        "        self.metadata_df = None\n",
        "        self.embeddings_array = None\n",
        "        self.embedding_index = None\n",
        "        self.researcher_index = None\n",
        "        self.datastore_info = None\n",
        "\n",
        "    def load_datastore(self):\n",
        "        \"\"\"Load the complete researcher profile datastore.\"\"\"\n",
        "        print(\"📂 Loading Researcher Profile Datastore...\")\n",
        "        try:\n",
        "            self.metadata_df = pd.read_parquet(os.path.join(self.datastore_path, \"researcher_profiles_metadata.parquet\"))\n",
        "            self.embeddings_array = np.load(os.path.join(self.datastore_path, \"researcher_embeddings.npy\"))\n",
        "            with open(os.path.join(self.datastore_path, \"embedding_index.json\"), 'r') as f:\n",
        "                self.embedding_index = json.load(f)\n",
        "            with open(os.path.join(self.datastore_path, \"researcher_index.json\"), 'r') as f:\n",
        "                self.researcher_index = json.load(f)\n",
        "            with open(os.path.join(self.datastore_path, \"datastore_info.json\"), 'r') as f:\n",
        "                self.datastore_info = json.load(f)\n",
        "            print(f\"🎯 Datastore ready: {self.metadata_df['researcher_id'].nunique()} researchers, {len(self.metadata_df)} papers\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to load datastore: {e}\")\n",
        "\n",
        "    def load_model(self, model_name='all-MiniLM-L6-v2'):\n",
        "        \"\"\"Load the sentence transformer model.\"\"\"\n",
        "        print(f\"🤖 Loading sentence transformer model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        print(\"✅ Model loaded.\")\n",
        "\n",
        "    def embed_skills(self, skills_checklist):\n",
        "        \"\"\"Embed each skill phrase.\"\"\"\n",
        "        if self.model is None:\n",
        "            self.load_model()\n",
        "        print(f\"🧠 Embedding {len(skills_checklist)} skills...\")\n",
        "        skill_embeddings = self.model.encode(skills_checklist, show_progress_bar=True)\n",
        "        print(f\"✅ Created skill embeddings: {skill_embeddings.shape}\")\n",
        "        return skill_embeddings\n",
        "\n",
        "    def get_researcher_data(self, researcher_id):\n",
        "        \"\"\"Get embeddings, work_ids, and recency weights for a specific researcher.\"\"\"\n",
        "        work_ids = self.researcher_index.get(researcher_id, [])\n",
        "        valid_work_ids = [wid for wid in work_ids if wid in self.embedding_index]\n",
        "        positions = [self.embedding_index[wid] for wid in valid_work_ids]\n",
        "        if not positions:\n",
        "            return np.array([]), [], []\n",
        "        paper_embeddings = self.embeddings_array[positions]\n",
        "        paper_metadata = self.metadata_df[self.metadata_df['work_id'].isin(valid_work_ids)]\n",
        "        weight_mapping = dict(zip(paper_metadata['work_id'], paper_metadata['recency_weight']))\n",
        "        recency_weights = np.array([weight_mapping.get(wid, 0.0) for wid in valid_work_ids])\n",
        "        return paper_embeddings, valid_work_ids, recency_weights\n",
        "\n",
        "    def calculate_skill_affinity_score(self, paper_embeddings, skill_embedding, recency_weights):\n",
        "        \"\"\"Calculate SkillAffinityScore for one researcher against one skill.\"\"\"\n",
        "        if len(paper_embeddings) == 0:\n",
        "            return 0.0\n",
        "        skill_embedding_2d = skill_embedding.reshape(1, -1)\n",
        "        cosine_sims = cosine_similarity(paper_embeddings, skill_embedding_2d).flatten()\n",
        "        weighted_sims = cosine_sims * recency_weights\n",
        "        max_weighted_sim = np.max(weighted_sims)\n",
        "        affinity_score = np.clip(max_weighted_sim * 100, 0, 100)\n",
        "        return round(affinity_score, 2)\n",
        "\n",
        "    def create_affinity_matrix(self, skills_checklist, solicitation_id=None):\n",
        "        \"\"\"Create the complete affinity matrix for all researchers and skills.\"\"\"\n",
        "        print(\"🎯 Creating Affinity Matrix...\")\n",
        "        print(\"=\" * 50)\n",
        "        if self.metadata_df is None:\n",
        "            self.load_datastore()\n",
        "        if not skills_checklist:\n",
        "            raise ValueError(\"Skills checklist is empty. Cannot create affinity matrix.\")\n",
        "        skill_embeddings = self.embed_skills(skills_checklist)\n",
        "        unique_researchers = list(self.researcher_index.keys())\n",
        "        print(f\"📊 Processing {len(unique_researchers)} researchers × {len(skills_checklist)} skills\")\n",
        "        affinity_matrix = np.zeros((len(unique_researchers), len(skills_checklist)))\n",
        "        for i, researcher_id in enumerate(tqdm(unique_researchers, desc=\"Processing researchers\")):\n",
        "            paper_embeddings, _, recency_weights = self.get_researcher_data(researcher_id)\n",
        "            if len(paper_embeddings) == 0:\n",
        "                continue\n",
        "            for j, skill_embedding in enumerate(skill_embeddings):\n",
        "                affinity_matrix[i, j] = self.calculate_skill_affinity_score(\n",
        "                    paper_embeddings, skill_embedding, recency_weights\n",
        "                )\n",
        "        researcher_names = [self.metadata_df[self.metadata_df['researcher_id'] == rid].iloc[0]['researcher_name'] for rid in unique_researchers]\n",
        "        skill_columns = [f\"Skill_{i+1:02d}: {skill[:50]}\" for i, skill in enumerate(skills_checklist)]\n",
        "        affinity_df = pd.DataFrame(affinity_matrix, index=researcher_names, columns=skill_columns)\n",
        "        print(f\"✅ Affinity Matrix created: {affinity_df.shape}\")\n",
        "        return affinity_df, unique_researchers, skills_checklist\n",
        "\n",
        "    def analyze_affinity_matrix(self, affinity_df, skills_checklist):\n",
        "        \"\"\"Provide analysis and insights on the affinity matrix.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n📊 AFFINITY MATRIX ANALYSIS\\n\" + \"=\"*60)\n",
        "        if affinity_df.empty or affinity_df.shape[1] == 0:\n",
        "            print(\"⚠️ Affinity matrix is empty. Skipping analysis.\")\n",
        "            return\n",
        "        print(f\"📏 Matrix dimensions: {affinity_df.shape[0]} researchers × {affinity_df.shape[1]} skills\")\n",
        "        print(f\"📈 Score range: {affinity_df.values.min():.2f} - {affinity_df.values.max():.2f}\")\n",
        "        print(f\"📊 Mean affinity score: {affinity_df.values.mean():.2f}\")\n",
        "        researcher_avg_scores = affinity_df.mean(axis=1).sort_values(ascending=False)\n",
        "        print(f\"\\n🏆 Top 5 Researchers (by average affinity):\\n{researcher_avg_scores.head().to_string(float_format='%.2f')}\")\n",
        "        skill_avg_scores = affinity_df.mean(axis=0).sort_values()\n",
        "        print(f\"\\n🎯 Most Challenging Skills (lowest average affinity):\")\n",
        "        for skill_col, score in skill_avg_scores.head().items():\n",
        "            try:\n",
        "                original_skill_index = int(re.search(r'Skill_(\\d+):', skill_col).group(1)) - 1\n",
        "                print(f\"   - {skills_checklist[original_skill_index][:60]}...: {score:.2f}\")\n",
        "            except (AttributeError, IndexError):\n",
        "                print(f\"   - {skill_col}: {score:.2f}\")\n",
        "\n",
        "\n",
        "    def save_affinity_matrix(self, affinity_df, csv_path, metadata_path, metadata):\n",
        "        \"\"\"\n",
        "        Save the affinity matrix and related data to specified paths.\n",
        "        This method is corrected to accept full paths instead of creating them internally.\n",
        "        \"\"\"\n",
        "        # Save main affinity matrix\n",
        "        affinity_df.to_csv(csv_path)\n",
        "        print(f\"💾 Affinity matrix saved to: {csv_path}\")\n",
        "\n",
        "        # Save metadata\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"📋 Metadata saved to: {metadata_path}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Main Execution for Cell 2\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings('ignore')\n",
        "    print(\"\\n🚀 Starting Pipeline: Cell 2 - Affinity Matrix Generation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # --- 1. Check for Inputs from Cell 1 ---\n",
        "    if 'result_obj' not in locals() or 'run_id' not in locals() or result_obj is None:\n",
        "        print(\"❌ ERROR: Input variables from Cell 1 are missing.\")\n",
        "        print(\"   Please run Cell 1 successfully before running this cell.\")\n",
        "    else:\n",
        "        print(f\"✅ Received inputs for Run ID: {run_id}\")\n",
        "        try:\n",
        "            # --- 2. Initialize Engine and Get Skills ---\n",
        "            engine = SkillAffinityEngine(datastore_path=DATASTORE_PATH)\n",
        "            skills_checklist = result_obj.required_skills_checklist\n",
        "            solicitation_id = result_obj.solicitation_id\n",
        "            print(f\"🎯 Analyzing {len(skills_checklist)} required skills.\")\n",
        "\n",
        "            # --- 3. Create Affinity Matrix ---\n",
        "            affinity_df, unique_researchers, skills_list = engine.create_affinity_matrix(\n",
        "                skills_checklist, solicitation_id\n",
        "            )\n",
        "\n",
        "            # --- 4. Define Output Paths Using the Run ID ---\n",
        "            affinity_csv_path = os.path.join(DATASTORE_PATH, f\"{run_id}_affinity_matrix.csv\")\n",
        "            affinity_metadata_path = os.path.join(DATASTORE_PATH, f\"{run_id}_affinity_metadata.json\")\n",
        "            print(f\"\\n💾 Defined affinity matrix output path:\\n   {affinity_csv_path}\")\n",
        "            print(f\"💾 Defined affinity metadata output path:\\n   {affinity_metadata_path}\")\n",
        "\n",
        "            # --- 5. Prepare and Save Results ---\n",
        "            metadata_payload = {\n",
        "                \"created_at\": pd.Timestamp.now().isoformat(),\n",
        "                \"solicitation_id\": solicitation_id,\n",
        "                \"run_id\": run_id,\n",
        "                \"matrix_shape\": affinity_df.shape,\n",
        "                \"num_researchers\": len(unique_researchers),\n",
        "                \"num_skills\": len(skills_list),\n",
        "                \"researcher_ids\": unique_researchers,\n",
        "                \"skills_checklist\": skills_list,\n",
        "                \"score_statistics\": {\n",
        "                    \"min\": float(affinity_df.values.min()),\n",
        "                    \"max\": float(affinity_df.values.max()),\n",
        "                    \"mean\": float(affinity_df.values.mean()),\n",
        "                    \"std\": float(affinity_df.values.std())\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Call the corrected save method\n",
        "            engine.save_affinity_matrix(affinity_df, affinity_csv_path, affinity_metadata_path, metadata_payload)\n",
        "\n",
        "            # --- 6. Analyze and Conclude ---\n",
        "            engine.analyze_affinity_matrix(affinity_df, skills_checklist)\n",
        "            print(\"\\n✅ Cell 2 finished. You may now proceed to Cell 3.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ An error occurred during affinity analysis in Cell 2: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827,
          "referenced_widgets": [
            "0e7d16e5d0ce48959e2b981129bf083a",
            "b37a5c038d584b199bec0886ccf913a4",
            "cf4eca5b08ea4f28a6db3e82ba655c62",
            "b5adde69c950473bacb94696c2ef87e6",
            "989acf8eb8d141a5b05a450a0f5c3857",
            "50226896f9864f36a6cacc4a41504c97",
            "0c3a67c070a547c1a5b0e5b1682365ef",
            "6641ff34b3d04af9be1f7f49046b4097",
            "7bd0121d80444127a0ffc8a54bba8b00",
            "1a58e44c63eb40ad8995ef499243f0e2",
            "1bb9e01dc16545d5bc0090820fe051ad"
          ]
        },
        "id": "qyrz6qQOmKVQ",
        "outputId": "a4518d71-df22-4f05-c87e-eb8e1bdd7b83"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Starting Pipeline: Cell 2 - Affinity Matrix Generation\n",
            "============================================================\n",
            "✅ Received inputs for Run ID: NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation\n",
            "🎯 Analyzing 14 required skills.\n",
            "🎯 Creating Affinity Matrix...\n",
            "==================================================\n",
            "📂 Loading Researcher Profile Datastore...\n",
            "🎯 Datastore ready: 51 researchers, 9067 papers\n",
            "🤖 Loading sentence transformer model: all-MiniLM-L6-v2\n",
            "✅ Model loaded.\n",
            "🧠 Embedding 14 skills...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e7d16e5d0ce48959e2b981129bf083a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created skill embeddings: (14, 384)\n",
            "📊 Processing 51 researchers × 14 skills\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing researchers: 100%|██████████| 51/51 [00:00<00:00, 72.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Affinity Matrix created: (51, 14)\n",
            "\n",
            "💾 Defined affinity matrix output path:\n",
            "   /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_affinity_matrix.csv\n",
            "💾 Defined affinity metadata output path:\n",
            "   /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_affinity_metadata.json\n",
            "💾 Affinity matrix saved to: /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_affinity_matrix.csv\n",
            "📋 Metadata saved to: /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_affinity_metadata.json\n",
            "\n",
            "============================================================\n",
            "📊 AFFINITY MATRIX ANALYSIS\n",
            "============================================================\n",
            "📏 Matrix dimensions: 51 researchers × 14 skills\n",
            "📈 Score range: 0.00 - 50.03\n",
            "📊 Mean affinity score: 12.47\n",
            "\n",
            "🏆 Top 5 Researchers (by average affinity):\n",
            "Subasish Das           34.05\n",
            "Brady T. West          26.18\n",
            "Robert McLean          24.95\n",
            "John P. Tiefenbacher   24.06\n",
            "Raymond P. Fisk        22.17\n",
            "\n",
            "🎯 Most Challenging Skills (lowest average affinity):\n",
            "   - Project management and team leadership...: 10.33\n",
            "   - Expertise in Artificial Intelligence in Medicine...: 10.68\n",
            "   - Expertise in Accelerating Materials Innovation through Infor...: 11.24\n",
            "   - Software engineering and research software development...: 11.41\n",
            "   - Geosciences (e.g., Earth sciences, atmospheric sciences, oce...: 11.73\n",
            "\n",
            "✅ Cell 2 finished. You may now proceed to Cell 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 3: Dream Team Assembler & Strategic Output\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "import anthropic\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "class DreamTeamAssembler:\n",
        "    \"\"\"\n",
        "    Phase 2: Dream Team Assembly & Strategic Output.\n",
        "    Generates optimal teams and creates comprehensive strategic reports.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.claude_client = None\n",
        "        self.setup_claude_api()\n",
        "\n",
        "    def setup_claude_api(self):\n",
        "        \"\"\"Initialize Claude API client.\"\"\"\n",
        "        try:\n",
        "            api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "            self.claude_client = anthropic.Anthropic(api_key=api_key)\n",
        "            print(\"✅ Claude API client initialized for gap analysis\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Claude API setup failed: {e}. Strategic analysis will be basic.\")\n",
        "\n",
        "    def load_affinity_matrix(self, csv_path, metadata_path=None):\n",
        "        \"\"\"Load the affinity matrix and associated metadata.\"\"\"\n",
        "        print(f\"📊 Loading affinity matrix from: {csv_path}\")\n",
        "        affinity_df = pd.read_csv(csv_path, index_col=0)\n",
        "        print(f\"✅ Loaded matrix: {affinity_df.shape[0]} researchers × {affinity_df.shape[1]} skills\")\n",
        "        metadata = None\n",
        "        if metadata_path and os.path.exists(metadata_path):\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "            print(\"✅ Loaded affinity metadata\")\n",
        "        return affinity_df, metadata\n",
        "\n",
        "    def calculate_team_coverage(self, affinity_df, team_indices):\n",
        "        \"\"\"Calculate team coverage scores for all skills.\"\"\"\n",
        "        if not team_indices:\n",
        "            return np.array([0.0] * affinity_df.shape[1]), 0.0\n",
        "        team_affinities = affinity_df.iloc[team_indices]\n",
        "        skill_coverages = team_affinities.max(axis=0).values\n",
        "        return skill_coverages, np.mean(skill_coverages)\n",
        "\n",
        "    def calculate_marginal_gain(self, affinity_df, current_team_indices, candidate_index):\n",
        "        \"\"\"Calculate the marginal gain of adding a candidate to the team.\"\"\"\n",
        "        _, current_coverage = self.calculate_team_coverage(affinity_df, current_team_indices)\n",
        "        _, new_coverage = self.calculate_team_coverage(affinity_df, current_team_indices + [candidate_index])\n",
        "        return new_coverage - current_coverage\n",
        "\n",
        "    def dream_team_greedy_algorithm(self, affinity_df, min_team_size=2, max_team_size=4):\n",
        "        \"\"\"Implement the greedy algorithm to select the best team.\"\"\"\n",
        "        print(\"🎯 Running Dream Team Greedy Algorithm...\")\n",
        "        print(\"=\" * 50)\n",
        "        n_researchers = len(affinity_df)\n",
        "        selected_indices = []\n",
        "        selection_history = []\n",
        "\n",
        "        # Step 1: Select the best overall researcher as PI\n",
        "        best_researcher_pos = affinity_df.mean(axis=1).idxmax()\n",
        "        best_researcher_loc = affinity_df.index.get_loc(best_researcher_pos)\n",
        "        selected_indices.append(best_researcher_loc)\n",
        "        _, initial_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "\n",
        "        selection_history.append({\n",
        "            'step': 1, 'action': 'Select PI',\n",
        "            'researcher_name': affinity_df.index[best_researcher_loc],\n",
        "            'reason': 'Highest average affinity score',\n",
        "            'team_coverage': initial_coverage\n",
        "        })\n",
        "        print(f\"🏆 Step 1 - PI Selection: {affinity_df.index[best_researcher_loc]} (Coverage: {initial_coverage:.2f})\")\n",
        "\n",
        "        # Step 2-N: Iteratively add members with the highest marginal gain\n",
        "        for step in range(2, max_team_size + 1):\n",
        "            gains = [(idx, self.calculate_marginal_gain(affinity_df, selected_indices, idx))\n",
        "                     for idx in range(n_researchers) if idx not in selected_indices]\n",
        "            if not gains: break\n",
        "\n",
        "            best_candidate_idx, best_marginal_gain = max(gains, key=lambda item: item[1])\n",
        "\n",
        "            if best_marginal_gain > 0.5 or len(selected_indices) < min_team_size:\n",
        "                selected_indices.append(best_candidate_idx)\n",
        "                _, new_coverage = self.calculate_team_coverage(affinity_df, selected_indices)\n",
        "                selection_history.append({\n",
        "                    'step': step, 'action': 'Add Member',\n",
        "                    'researcher_name': affinity_df.index[best_candidate_idx],\n",
        "                    'reason': f'Maximum marginal gain (+{best_marginal_gain:.2f})',\n",
        "                    'team_coverage': new_coverage\n",
        "                })\n",
        "                print(f\"✅ Step {step} - Added: {affinity_df.index[best_candidate_idx]} (New Coverage: {new_coverage:.2f})\")\n",
        "            else:\n",
        "                print(f\"🛑 Step {step} - Stopping: No significant marginal gain found (best was +{best_marginal_gain:.2f}).\")\n",
        "                break\n",
        "\n",
        "        final_coverage = self.calculate_team_coverage(affinity_df, selected_indices)[1]\n",
        "        print(f\"\\n🎯 Final Dream Team ({len(selected_indices)} members) with {final_coverage:.2f} coverage.\")\n",
        "        return selected_indices, selection_history\n",
        "\n",
        "    def generate_coverage_report(self, affinity_df, team_indices, skills_list):\n",
        "        \"\"\"Generate a detailed coverage report for the selected team.\"\"\"\n",
        "        skill_coverages, overall_coverage = self.calculate_team_coverage(affinity_df, team_indices)\n",
        "        team_members = []\n",
        "        for idx in team_indices:\n",
        "            scores = affinity_df.iloc[idx]\n",
        "            top_skills = [{'skill': skills_list[i], 'score': scores[i]} for i in scores.argsort()[-3:][::-1]]\n",
        "            team_members.append({'name': affinity_df.index[idx], 'avg_affinity': scores.mean(), 'top_skills': top_skills})\n",
        "\n",
        "        skill_analysis = []\n",
        "        for i, (skill, coverage) in enumerate(zip(skills_list, skill_coverages)):\n",
        "            team_scores = affinity_df.iloc[team_indices, i]\n",
        "            best_member_idx = team_scores.idxmax()\n",
        "            skill_analysis.append({\n",
        "                'skill': skill, 'coverage_score': coverage,\n",
        "                'level': 'High' if coverage >= 70 else 'Medium' if coverage >= 40 else 'Low',\n",
        "                'expert': best_member_idx, 'expert_score': team_scores.max()\n",
        "            })\n",
        "        return {'overall_coverage_score': overall_coverage, 'team_members': team_members, 'skill_analysis': skill_analysis}\n",
        "\n",
        "    def generate_strategic_analysis(self, coverage_report, skills_list, solicitation_data):\n",
        "        \"\"\"Generate AI-powered gap analysis using Claude API.\"\"\"\n",
        "        if not self.claude_client:\n",
        "            return \"Claude API not available. Basic analysis only: Review low-coverage skills and consider recruitment.\"\n",
        "        print(\"🤖 Generating strategic analysis with Claude API...\")\n",
        "        # Create a detailed prompt (shortened for brevity, full logic assumed)\n",
        "        prompt = f\"Analyze this research team's fit for the solicitation titled '{solicitation_data.get('title', 'N/A')}'.\\n\"\n",
        "        prompt += f\"Team has an overall coverage score of {coverage_report['overall_coverage_score']:.2f}.\\n\"\n",
        "        low_skills = [s['skill'] for s in coverage_report['skill_analysis'] if s['level'] == 'Low']\n",
        "        prompt += f\"Potential Gaps (Low Coverage): {', '.join(low_skills) if low_skills else 'None'}.\\n\"\n",
        "        prompt += \"Provide a strategic report covering strengths, weaknesses, and actionable recommendations for the proposal.\"\n",
        "        try:\n",
        "            response = self.claude_client.messages.create(\n",
        "                model=\"claude-3-sonnet-20240229\", max_tokens=2000, temperature=0.5,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            analysis = response.content[0].text\n",
        "            print(\"✅ Strategic analysis generated.\")\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            return f\"Claude API analysis failed: {e}. Fallback: Review low-coverage skills: {low_skills}.\"\n",
        "\n",
        "    def create_strategic_report(self, affinity_df, metadata, solicitation_data):\n",
        "        \"\"\"Main function to create a comprehensive strategic report.\"\"\"\n",
        "        print(\"\\n🚀 CREATING STRATEGIC REPORT\")\n",
        "        print(\"=\" * 60)\n",
        "        skills_list = metadata.get('skills_checklist', [col.split(': ', 1)[-1] for col in affinity_df.columns])\n",
        "        team_indices, history = self.dream_team_greedy_algorithm(affinity_df)\n",
        "        coverage_report = self.generate_coverage_report(affinity_df, team_indices, skills_list)\n",
        "        strategic_analysis = self.generate_strategic_analysis(coverage_report, skills_list, solicitation_data)\n",
        "\n",
        "        return {\n",
        "            'report_metadata': {'generated_at': datetime.now().isoformat(), 'solicitation_id': metadata.get('solicitation_id')},\n",
        "            'coverage_analysis': coverage_report, 'strategic_analysis': strategic_analysis,\n",
        "        }\n",
        "\n",
        "    def format_markdown_report(self, strategic_report):\n",
        "        \"\"\"Format the strategic report as a human-readable Markdown file.\"\"\"\n",
        "        report = f\"# NSF Dream Team Strategic Report\\n\\n\"\n",
        "        meta = strategic_report['report_metadata']\n",
        "        report += f\"**Generated:** {meta['generated_at']}\\n\"\n",
        "        report += f\"**Solicitation ID:** `{meta.get('solicitation_id', 'N/A')}`\\n\\n\"\n",
        "\n",
        "        # --- Team Summary Table ---\n",
        "        coverage = strategic_report['coverage_analysis']\n",
        "        report += f\"## 🏆 Recommended Dream Team\\n\\n\"\n",
        "        report += f\"**Overall Team Coverage Score:** **`{coverage['overall_coverage_score']:.2f} / 100`**\\n\\n\"\n",
        "        report += \"| Role | Researcher | Avg. Affinity | Top Expertise Areas |\\n\"\n",
        "        report += \"|:---|:---|:---:|:---|\\n\"\n",
        "        for i, member in enumerate(coverage['team_members']):\n",
        "            role = \"**Principal Investigator (PI)**\" if i == 0 else f\"Co-Investigator {i+1}\"\n",
        "            top_skills = \", \".join([s['skill'] for s in member['top_skills']])\n",
        "            report += f\"| {role} | {member['name']} | `{member['avg_affinity']:.2f}` | {top_skills} |\\n\"\n",
        "\n",
        "        # --- Coverage Analysis Table ---\n",
        "        report += f\"\\n## 📊 Skills Coverage Analysis\\n\\n\"\n",
        "        report += \"| Skill / Expertise Area | Coverage | Level | Primary Expert |\\n\"\n",
        "        report += \"|:---|:---:|:---|:---|\\n\"\n",
        "        for skill in sorted(coverage['skill_analysis'], key=lambda x: x['coverage_score']):\n",
        "            level_emoji = \"🟢\" if skill['level'] == 'High' else \"🟡\" if skill['level'] == 'Medium' else \"🔴\"\n",
        "            report += f\"| {skill['skill']} | `{skill['coverage_score']:.2f}` | {level_emoji} {skill['level']} | {skill['expert']} |\\n\"\n",
        "\n",
        "        # --- Strategic Analysis ---\n",
        "        report += f\"\\n## 🧠 AI-Powered Strategic Analysis\\n\\n\"\n",
        "        report += \"> \" + strategic_report['strategic_analysis'].replace('\\n', '\\n> ') + \"\\n\"\n",
        "        return report\n",
        "\n",
        "    def save_strategic_report(self, strategic_report, drive_base_path, local_base_path):\n",
        "        \"\"\"Saves the strategic report as JSON and Markdown to GDrive and local storage.\"\"\"\n",
        "        print(\"\\n💾 Saving strategic reports...\")\n",
        "\n",
        "        # --- Generate Markdown Content ---\n",
        "        md_content = self.format_markdown_report(strategic_report)\n",
        "\n",
        "        # --- Define Paths ---\n",
        "        json_path = f\"{drive_base_path}_strategic_report.json\"\n",
        "        drive_md_path = f\"{drive_base_path}_strategic_report.md\"\n",
        "        local_md_path = f\"{local_base_path}_strategic_report.md\"\n",
        "\n",
        "        # --- Save Files ---\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(strategic_report, f, indent=2)\n",
        "        print(f\"   📄 Saved full JSON report to Google Drive:\\n      {json_path}\")\n",
        "\n",
        "        with open(drive_md_path, 'w') as f:\n",
        "            f.write(md_content)\n",
        "        print(f\"   📝 Saved Markdown report to Google Drive:\\n      {drive_md_path}\")\n",
        "\n",
        "        with open(local_md_path, 'w') as f:\n",
        "            f.write(md_content)\n",
        "        print(f\"   💻 Saved Markdown report to Colab local storage:\\n      {local_md_path}\")\n",
        "\n",
        "    def display_summary(self, strategic_report):\n",
        "        \"\"\"Displays a summary of the strategic report in the console.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"📋 DREAM TEAM STRATEGIC REPORT SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        coverage = strategic_report['coverage_analysis']\n",
        "        print(f\"🏆 Recommended Team ({len(coverage['team_members'])} members) --> Overall Score: {coverage['overall_coverage_score']:.2f}/100\")\n",
        "        for i, member in enumerate(coverage['team_members']):\n",
        "            role = \"PI\" if i == 0 else f\"Co-I {i+1}\"\n",
        "            print(f\"   - **{member['name']}** ({role})\")\n",
        "\n",
        "        low_skills = [s for s in coverage['skill_analysis'] if s['level'] == 'Low']\n",
        "        if low_skills:\n",
        "            print(f\"\\n🔴 Identified {len(low_skills)} potential skill gaps (Low Coverage).\")\n",
        "            print(\"   Review the saved Markdown report for details.\")\n",
        "        else:\n",
        "            print(\"\\n🟢 Excellent coverage. No significant skill gaps were identified.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Main Execution for Cell 3\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings('ignore')\n",
        "    print(\"\\n🚀 Starting Pipeline: Cell 3 - Dream Team Assembly & Report Generation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # --- 1. Check for Inputs from Previous Cells ---\n",
        "    if 'affinity_csv_path' not in locals() or not os.path.exists(affinity_csv_path):\n",
        "        print(\"❌ ERROR: Input file from Cell 2 is missing.\")\n",
        "        print(\"   Please run Cells 1 and 2 successfully before running this cell.\")\n",
        "    else:\n",
        "        print(f\"✅ Received inputs for Run ID: {run_id}\")\n",
        "        try:\n",
        "            # --- 2. Initialize Assembler ---\n",
        "            assembler = DreamTeamAssembler()\n",
        "\n",
        "            # --- 3. Load Data Using Dynamic Paths ---\n",
        "            print(\"📊 Loading data using dynamic paths from previous cells...\")\n",
        "            affinity_df, affinity_metadata = assembler.load_affinity_matrix(affinity_csv_path, affinity_metadata_path)\n",
        "            with open(solicitation_output_path, 'r') as f:\n",
        "                solicitation_data = json.load(f)\n",
        "            print(\"✅ Loaded all necessary data.\")\n",
        "\n",
        "            # --- 4. Create Strategic Report ---\n",
        "            strategic_report = assembler.create_strategic_report(\n",
        "                affinity_df=affinity_df,\n",
        "                metadata=affinity_metadata,\n",
        "                solicitation_data=solicitation_data\n",
        "            )\n",
        "\n",
        "            # --- 5. Define Output Paths and Save Report ---\n",
        "            report_drive_base_path = os.path.join(DATASTORE_PATH, run_id)\n",
        "            report_local_base_path = f\"/content/{run_id}\" # For Colab's temporary storage\n",
        "\n",
        "            assembler.save_strategic_report(strategic_report, report_drive_base_path, report_local_base_path)\n",
        "\n",
        "            # --- 6. Display Final Summary in Console ---\n",
        "            assembler.display_summary(strategic_report)\n",
        "\n",
        "            print(f\"\\n\\n✅✅✅ Pipeline Complete! ✅✅✅\")\n",
        "            print(f\"Check the file browser on the left for the local report or your Drive folder.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ An error occurred during dream team analysis in Cell 3: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcj8pN6snJQl",
        "outputId": "f75a4c0c-f9fd-4d12-e007-82c01366559c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Starting Pipeline: Cell 3 - Dream Team Assembly & Report Generation\n",
            "============================================================\n",
            "✅ Received inputs for Run ID: NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation\n",
            "✅ Claude API client initialized for gap analysis\n",
            "📊 Loading data using dynamic paths from previous cells...\n",
            "📊 Loading affinity matrix from: /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_affinity_matrix.csv\n",
            "✅ Loaded matrix: 51 researchers × 14 skills\n",
            "✅ Loaded affinity metadata\n",
            "✅ Loaded all necessary data.\n",
            "\n",
            "🚀 CREATING STRATEGIC REPORT\n",
            "============================================================\n",
            "🎯 Running Dream Team Greedy Algorithm...\n",
            "==================================================\n",
            "🏆 Step 1 - PI Selection: Subasish Das (Coverage: 34.05)\n",
            "✅ Step 2 - Added: Brady T. West (New Coverage: 35.39)\n",
            "✅ Step 3 - Added: Martin Burtscher (New Coverage: 36.59)\n",
            "🛑 Step 4 - Stopping: No significant marginal gain found (best was +0.38).\n",
            "\n",
            "🎯 Final Dream Team (3 members) with 36.59 coverage.\n",
            "🤖 Generating strategic analysis with Claude API...\n",
            "✅ Strategic analysis generated.\n",
            "\n",
            "💾 Saving strategic reports...\n",
            "   📄 Saved full JSON report to Google Drive:\n",
            "      /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_strategic_report.json\n",
            "   📝 Saved Markdown report to Google Drive:\n",
            "      /content/drive/MyDrive/datastore/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_strategic_report.md\n",
            "   💻 Saved Markdown report to Colab local storage:\n",
            "      /content/NSF_25-530__Collaborations_in_Artificial_Intelligence_and_Geosciences__CAIG____NSF_-_National_Science_Foundation_strategic_report.md\n",
            "\n",
            "============================================================\n",
            "📋 DREAM TEAM STRATEGIC REPORT SUMMARY\n",
            "============================================================\n",
            "🏆 Recommended Team (3 members) --> Overall Score: 36.59/100\n",
            "   - **Subasish Das** (PI)\n",
            "   - **Brady T. West** (Co-I 2)\n",
            "   - **Martin Burtscher** (Co-I 3)\n",
            "\n",
            "🔴 Identified 9 potential skill gaps (Low Coverage).\n",
            "   Review the saved Markdown report for details.\n",
            "\n",
            "\n",
            "✅✅✅ Pipeline Complete! ✅✅✅\n",
            "Check the file browser on the left for the local report or your Drive folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAy9EHqNnkfQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}